{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBTI Parallel Classification Model with Neural BOW (I/E Axis)\n",
    "\n",
    "First, load libraries and useful functions from class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from w266_common import patched_numpy_io\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Utils and Helper libraries\n",
    "# import nltk\n",
    "from w266_common import utils, vocabulary\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Specifications for Binary Classification NBOW for MBTI\n",
    "\n",
    "In this baseline, the task is to predict the first MBTI axis (I vs. E) given a text string. We will model after the A2 assignment, with Architecture and Parameters defined below.\n",
    "\n",
    "### Pre-Processing:\n",
    "* Minimial pre-processing, only separating punctuation from text and lower-case all text\n",
    "* Assigning words to numerical indices based on a fixed Vocab size, defined by word frequency in training set\n",
    "* Pulled out first axis of all target labels, assigned to binary (E = 0, I = 1)\n",
    "\n",
    "### Architecture:\n",
    "* Encoder: Bag of Words \n",
    "* Decoder: Softmax\n",
    "* Classification: Binary (2 MBTI types - I or E)\n",
    "\n",
    "### Parameters\n",
    "* Batch Size: 25 \n",
    "* Text length: 100\n",
    "* Vocabulary size (V): ~328K - removed stopwords\n",
    "* Embedding Size: 50\n",
    "* Hidden Dimensions: 25\n",
    "\n",
    "### Training:\n",
    "* Epochs = 10 \n",
    "* 80% train, 20% test\n",
    "* Loss: Sparse Softmax Cross Entropy \n",
    "* Optimizers: Adagrad Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus & Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Personality</th>\n",
       "      <th>username</th>\n",
       "      <th>user_id</th>\n",
       "      <th>tweets</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>enfj</td>\n",
       "      <td>pr3achlikeagirl</td>\n",
       "      <td>2.310891e+09</td>\n",
       "      <td>['God is on the move!', 'Stop telling God what...</td>\n",
       "      <td>67</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1090079036...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enfj</td>\n",
       "      <td>ChurchTherapist</td>\n",
       "      <td>8.606921e+08</td>\n",
       "      <td>['@user It describes anxious attachment as opp...</td>\n",
       "      <td>1841</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1033441323...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>enfj</td>\n",
       "      <td>camperry21</td>\n",
       "      <td>2.653982e+07</td>\n",
       "      <td>[\"@user @user He ain't winning with the Clippe...</td>\n",
       "      <td>1240</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1078547897...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enfj</td>\n",
       "      <td>galaxiaskykloz</td>\n",
       "      <td>2.268542e+09</td>\n",
       "      <td>['does anyone remember poopreport dot com', '@...</td>\n",
       "      <td>2227</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1001856767...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enfj</td>\n",
       "      <td>sarahyoung_esq</td>\n",
       "      <td>1.090463e+18</td>\n",
       "      <td>['MeatPotatoesNBeans ', 'THE LITTLE BLEP TONGU...</td>\n",
       "      <td>76</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1090465009...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Personality         username       user_id  \\\n",
       "0        enfj  pr3achlikeagirl  2.310891e+09   \n",
       "1        enfj  ChurchTherapist  8.606921e+08   \n",
       "2        enfj       camperry21  2.653982e+07   \n",
       "3        enfj   galaxiaskykloz  2.268542e+09   \n",
       "4        enfj   sarahyoung_esq  1.090463e+18   \n",
       "\n",
       "                                              tweets  followers_count  \\\n",
       "0  ['God is on the move!', 'Stop telling God what...               67   \n",
       "1  ['@user It describes anxious attachment as opp...             1841   \n",
       "2  [\"@user @user He ain't winning with the Clippe...             1240   \n",
       "3  ['does anyone remember poopreport dot com', '@...             2227   \n",
       "4  ['MeatPotatoesNBeans ', 'THE LITTLE BLEP TONGU...               76   \n",
       "\n",
       "                                               image  \n",
       "0  http://pbs.twimg.com/profile_images/1090079036...  \n",
       "1  http://pbs.twimg.com/profile_images/1033441323...  \n",
       "2  http://pbs.twimg.com/profile_images/1078547897...  \n",
       "3  http://pbs.twimg.com/profile_images/1001856767...  \n",
       "4  http://pbs.twimg.com/profile_images/1090465009...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "df = pd.read_csv('../personalities_FINAL.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove nan values\n",
    "df = df.dropna(subset=['tweets'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize and clean sentence [\"Hello world.\"] into list of words [\"hello\",\"world\"]\n",
    "def clean_tokenize(sentence):\n",
    "    ignore_words = ['a', 'the', 'user', 'i','is']\n",
    "    sentence = re.sub(\"\\'\",\"\",sentence)\n",
    "    words = re.sub(\"[^\\w]|[0-9]\", \" \",  sentence).split() #removes all non-alphanumeric words, removes all numbers\n",
    "    words_cleaned = [w.lower() for w in words if w.lower() not in ignore_words]\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    #words_cleaned = ' '.join(word for word in words_cleaned)\n",
    "    \n",
    "    return words_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_tweets\"] = df[\"tweets\"].apply(clean_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['god',\n",
       " 'on',\n",
       " 'move',\n",
       " 'stop',\n",
       " 'telling',\n",
       " 'god',\n",
       " 'what',\n",
       " 'you',\n",
       " 'don',\n",
       " 't',\n",
       " 'have',\n",
       " 'and',\n",
       " 'instead',\n",
       " 'give',\n",
       " 'christ',\n",
       " 'what',\n",
       " 'you',\n",
       " 'do',\n",
       " 'have',\n",
       " 'and',\n",
       " 'let',\n",
       " 'him',\n",
       " 'take',\n",
       " 'it',\n",
       " 'bless',\n",
       " 'it',\n",
       " 'break',\n",
       " 'it',\n",
       " 'and',\n",
       " 'multiply',\n",
       " 'it',\n",
       " 'for',\n",
       " 'his',\n",
       " 'glory',\n",
       " 'in',\n",
       " 'hands',\n",
       " 'of',\n",
       " 'jesus',\n",
       " 'all',\n",
       " 'things',\n",
       " 'become',\n",
       " 'possible',\n",
       " 'dr',\n",
       " 'david',\n",
       " 'busic',\n",
       " 'watching',\n",
       " 'people',\n",
       " 'love',\n",
       " 'receive',\n",
       " 'their',\n",
       " 'district',\n",
       " 'licenses',\n",
       " 'and',\n",
       " 'ordination',\n",
       " 'very',\n",
       " 'emotional',\n",
       " 'experience',\n",
       " 'for',\n",
       " 'me',\n",
       " 'love',\n",
       " 'seeing',\n",
       " 'and',\n",
       " 'hearing',\n",
       " 'how',\n",
       " 'far',\n",
       " 'god',\n",
       " 'has',\n",
       " 'brought',\n",
       " 'them',\n",
       " 'and',\n",
       " 'love',\n",
       " 'their',\n",
       " 'obedience',\n",
       " 'to',\n",
       " 'god',\n",
       " 'as',\n",
       " 'they',\n",
       " 'follow',\n",
       " 'his',\n",
       " 'call',\n",
       " 'upon',\n",
       " 'their',\n",
       " 'lives',\n",
       " 'pghcotndistrictassembly',\n",
       " 'not',\n",
       " 'our',\n",
       " 'own',\n",
       " 'righteousness',\n",
       " 'but',\n",
       " 'christ',\n",
       " 'within',\n",
       " 'living',\n",
       " 'and',\n",
       " 'reigning',\n",
       " 'and',\n",
       " 'saving',\n",
       " 'from',\n",
       " 'sin',\n",
       " 'holinessuntothelord',\n",
       " 'calleduntoholiness',\n",
       " 'pittsburg',\n",
       " 'district',\n",
       " 'cotn',\n",
       " 'teen',\n",
       " 'camp',\n",
       " 'pittproud',\n",
       " 'pitt',\n",
       " 'life',\n",
       " 'cotn',\n",
       " 'pghteencamp',\n",
       " 'for',\n",
       " 'some',\n",
       " 'reason',\n",
       " 'this',\n",
       " 'video',\n",
       " 'was',\n",
       " 'marked',\n",
       " 'as',\n",
       " 'sensitive',\n",
       " 'content',\n",
       " 'in',\n",
       " 'my',\n",
       " 'newsfeed',\n",
       " 'wut',\n",
       " 'solid',\n",
       " 'rock',\n",
       " 'foundation',\n",
       " 'not',\n",
       " 'biblical',\n",
       " 'worldview',\n",
       " 'it',\n",
       " 'christ',\n",
       " 'alone',\n",
       " 'on',\n",
       " 'christ',\n",
       " 'solid',\n",
       " 'rock',\n",
       " 'stand',\n",
       " 'all',\n",
       " 'other',\n",
       " 'ground',\n",
       " 'sinking',\n",
       " 'sand',\n",
       " 'biblical',\n",
       " 'worldview',\n",
       " 'naught',\n",
       " 'but',\n",
       " 'part',\n",
       " 'of',\n",
       " 'building',\n",
       " 'upon',\n",
       " 'foundation',\n",
       " 'necessary',\n",
       " 'but',\n",
       " 'not',\n",
       " 'basis',\n",
       " 'of',\n",
       " 'faith',\n",
       " 'now',\n",
       " 'haven',\n",
       " 't',\n",
       " 'done',\n",
       " 'word',\n",
       " 'study',\n",
       " 'yet',\n",
       " 'but',\n",
       " 'are',\n",
       " 'we',\n",
       " 'certain',\n",
       " 'it',\n",
       " 'was',\n",
       " 'menstruation',\n",
       " 'had',\n",
       " 'heard',\n",
       " 'it',\n",
       " 'could',\n",
       " 'have',\n",
       " 'been',\n",
       " 'something',\n",
       " 'like',\n",
       " 'hemophilia',\n",
       " 'or',\n",
       " 'another',\n",
       " 'blood',\n",
       " 'disease',\n",
       " 'that',\n",
       " 'she',\n",
       " 'was',\n",
       " 'plagued',\n",
       " 'with',\n",
       " 'also',\n",
       " 'read',\n",
       " 'an',\n",
       " 'article',\n",
       " 'later',\n",
       " 'on',\n",
       " 'ritual',\n",
       " 'roof',\n",
       " 'baths',\n",
       " 'but',\n",
       " 'when',\n",
       " 'found',\n",
       " 'it',\n",
       " 'in',\n",
       " 'my',\n",
       " 'fb',\n",
       " 'history',\n",
       " 'link',\n",
       " 'was',\n",
       " 'broken',\n",
       " 'david',\n",
       " 'was',\n",
       " 'on',\n",
       " 'his',\n",
       " 'roof',\n",
       " 'spying',\n",
       " 'on',\n",
       " 'woman',\n",
       " 'who',\n",
       " 'was',\n",
       " 'probably',\n",
       " 'fresh',\n",
       " 'off',\n",
       " 'her',\n",
       " 'cycle',\n",
       " 'and',\n",
       " 'was',\n",
       " 'performing',\n",
       " 'her',\n",
       " 'required',\n",
       " 'ceremonial',\n",
       " 'cleansing',\n",
       " 'and',\n",
       " 'def',\n",
       " 'not',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'seduce',\n",
       " 'anyone',\n",
       " 'it',\n",
       " 'was',\n",
       " 'big',\n",
       " 'discussion',\n",
       " 'after',\n",
       " 'friend',\n",
       " 'came',\n",
       " 'home',\n",
       " 'from',\n",
       " 'missions',\n",
       " 'trip',\n",
       " 'to',\n",
       " 'israel',\n",
       " 'she',\n",
       " 'was',\n",
       " 'able',\n",
       " 'to',\n",
       " 'see',\n",
       " 'some',\n",
       " 'roof',\n",
       " 'baths',\n",
       " 'she',\n",
       " 'also',\n",
       " 'explained',\n",
       " 'that',\n",
       " 'she',\n",
       " 'had',\n",
       " 'been',\n",
       " 'told',\n",
       " 'they',\n",
       " 'were',\n",
       " 'for',\n",
       " 'ritual',\n",
       " 'cleansing',\n",
       " 'so',\n",
       " 'like',\n",
       " 'for',\n",
       " 'after',\n",
       " 'woman',\n",
       " 's',\n",
       " 'cycle',\n",
       " 'or',\n",
       " 'another',\n",
       " 'issue',\n",
       " 'requiring',\n",
       " 'purification',\n",
       " 'david',\n",
       " 's',\n",
       " 'palace',\n",
       " 'however',\n",
       " 'allowed',\n",
       " 'for',\n",
       " 'his',\n",
       " 'roof',\n",
       " 'to',\n",
       " 'be',\n",
       " 'above',\n",
       " 'hers',\n",
       " 'and',\n",
       " 'therefore',\n",
       " 'he',\n",
       " 'was',\n",
       " 'looking',\n",
       " 'down',\n",
       " 'onto',\n",
       " 'private',\n",
       " 'roof',\n",
       " 'and',\n",
       " 'watched',\n",
       " 'woman',\n",
       " 'bath',\n",
       " 'in',\n",
       " 'what',\n",
       " 'would',\n",
       " 'have',\n",
       " 'normally',\n",
       " 'been',\n",
       " 'considered',\n",
       " 'private',\n",
       " 'setting',\n",
       " 'there',\n",
       " 'were',\n",
       " 'baths',\n",
       " 'on',\n",
       " 'roofs',\n",
       " 'they',\n",
       " 'were',\n",
       " 'designed',\n",
       " 'that',\n",
       " 'way',\n",
       " 'to',\n",
       " 'fill',\n",
       " 'with',\n",
       " 'water',\n",
       " 'and',\n",
       " 'let',\n",
       " 'sun',\n",
       " 'heat',\n",
       " 'them',\n",
       " 'as',\n",
       " 'opposed',\n",
       " 'to',\n",
       " 'boiling',\n",
       " 'buckets',\n",
       " 'and',\n",
       " 'then',\n",
       " 'filling',\n",
       " 'bath',\n",
       " 'however',\n",
       " 'roof',\n",
       " 'was',\n",
       " 'considered',\n",
       " 'private',\n",
       " 'part',\n",
       " 'of',\n",
       " 'home',\n",
       " 'due',\n",
       " 'to',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'edges',\n",
       " 'were',\n",
       " 'raised',\n",
       " 'for',\n",
       " 'privacy',\n",
       " 'belief',\n",
       " 'in',\n",
       " 'infallibility',\n",
       " 'of',\n",
       " 'scripture',\n",
       " 'not',\n",
       " 'synonymous',\n",
       " 'with',\n",
       " 'belief',\n",
       " 'in',\n",
       " 'infallibility',\n",
       " 'of',\n",
       " 'one',\n",
       " 's',\n",
       " 'interpretation',\n",
       " 'of',\n",
       " 'it',\n",
       " 'scripture',\n",
       " 's',\n",
       " 'reliability',\n",
       " 'has',\n",
       " 'nothing',\n",
       " 'to',\n",
       " 'do',\n",
       " 'with',\n",
       " 'someone',\n",
       " 's',\n",
       " 'limited',\n",
       " 'understanding',\n",
       " 'of',\n",
       " 'it',\n",
       " 'dig',\n",
       " 'into',\n",
       " 'word',\n",
       " 'for',\n",
       " 'yourself',\n",
       " 'knowtheword',\n",
       " 'god',\n",
       " 's',\n",
       " 'will',\n",
       " 'not',\n",
       " 'for',\n",
       " 'us',\n",
       " 'to',\n",
       " 'follow',\n",
       " 'his',\n",
       " 'call',\n",
       " 'at',\n",
       " 'expense',\n",
       " 'of',\n",
       " 'what',\n",
       " 'scripture',\n",
       " 'said',\n",
       " 'that',\n",
       " 'not',\n",
       " 'truly',\n",
       " 'walking',\n",
       " 'with',\n",
       " 'him',\n",
       " 'no',\n",
       " 'instead',\n",
       " 'we',\n",
       " 'are',\n",
       " 'to',\n",
       " 'seek',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'scripture',\n",
       " 'what',\n",
       " 'it',\n",
       " 'says',\n",
       " 'and',\n",
       " 'why',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'our',\n",
       " 'calls',\n",
       " 'better',\n",
       " 'you',\n",
       " 'must',\n",
       " 'know',\n",
       " 'truth',\n",
       " 'for',\n",
       " 'it',\n",
       " 'to',\n",
       " 'set',\n",
       " 'you',\n",
       " 'free',\n",
       " 'am',\n",
       " 'an',\n",
       " 'enfj',\n",
       " 'and',\n",
       " 'this',\n",
       " 'has',\n",
       " 'been',\n",
       " 'me',\n",
       " 'recently',\n",
       " 'once',\n",
       " 'had',\n",
       " 'boyfriend',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'that',\n",
       " 'he',\n",
       " 'would',\n",
       " 'take',\n",
       " 'his',\n",
       " 'hands',\n",
       " 'and',\n",
       " 'squeeze',\n",
       " 'my',\n",
       " 'neck',\n",
       " 'til',\n",
       " 'it',\n",
       " 'snapped',\n",
       " 'he',\n",
       " 'claimed',\n",
       " 'he',\n",
       " 'was',\n",
       " 'joking',\n",
       " 'to',\n",
       " 'which',\n",
       " 'responded',\n",
       " 'you',\n",
       " 're',\n",
       " 'not',\n",
       " 'funny',\n",
       " 'don',\n",
       " 't',\n",
       " 'bother',\n",
       " 'talking',\n",
       " 'to',\n",
       " 'me',\n",
       " 'again',\n",
       " 'people',\n",
       " 'who',\n",
       " 'think',\n",
       " 'abusive',\n",
       " 'language',\n",
       " 'funny',\n",
       " 'are',\n",
       " 'totally',\n",
       " 'messed',\n",
       " 'up',\n",
       " 'body',\n",
       " 'which',\n",
       " 'stunts',\n",
       " 'growth',\n",
       " 'and',\n",
       " 'use',\n",
       " 'of',\n",
       " 'half',\n",
       " 'of',\n",
       " 'its',\n",
       " 'parts',\n",
       " 'becomes',\n",
       " 'deformed',\n",
       " 'and',\n",
       " 'crippled',\n",
       " 'likewise',\n",
       " 'church',\n",
       " 'which',\n",
       " 'limits',\n",
       " 'growth',\n",
       " 'and',\n",
       " 'utility',\n",
       " 'of',\n",
       " 'half',\n",
       " 'its',\n",
       " 'members',\n",
       " 'crippled',\n",
       " 'itself',\n",
       " 'and',\n",
       " 'becomes',\n",
       " 'ineffective',\n",
       " 'halfthechurch',\n",
       " 'womeninministry',\n",
       " 'ifwearethebody',\n",
       " 'church',\n",
       " 'needs',\n",
       " 'more',\n",
       " 'radical',\n",
       " 'prophetic',\n",
       " 'voices',\n",
       " 'making',\n",
       " 'people',\n",
       " 'uncomfortable',\n",
       " 'rev',\n",
       " 'brian',\n",
       " 'powell',\n",
       " 'junia',\n",
       " 'feminine',\n",
       " 'noun',\n",
       " 'as',\n",
       " 'you',\n",
       " 'can',\n",
       " 'see',\n",
       " 'also',\n",
       " 'it',\n",
       " 'says',\n",
       " 'she',\n",
       " 'was',\n",
       " 'notable',\n",
       " 'apostles',\n",
       " 'not',\n",
       " 'known',\n",
       " 'by',\n",
       " 'them',\n",
       " 'word',\n",
       " 'studies',\n",
       " 'are',\n",
       " 'wonderful',\n",
       " 'things',\n",
       " 'to',\n",
       " 'actually',\n",
       " 'take',\n",
       " 'time',\n",
       " 'to',\n",
       " 'do',\n",
       " 'just',\n",
       " 'saying',\n",
       " 'starting',\n",
       " 'in',\n",
       " 'minute',\n",
       " 'will',\n",
       " 'not',\n",
       " 'misrepresent',\n",
       " 'bible',\n",
       " 'love',\n",
       " 'bible',\n",
       " 'whole',\n",
       " 'bible',\n",
       " 'refuse',\n",
       " 'to',\n",
       " 'take',\n",
       " 'one',\n",
       " 'verse',\n",
       " 'and',\n",
       " 'use',\n",
       " 'it',\n",
       " 'to',\n",
       " 'limit',\n",
       " 'gods',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'use',\n",
       " 'his',\n",
       " 'creation',\n",
       " 'but',\n",
       " 'see',\n",
       " 'that',\n",
       " 'am',\n",
       " 'throwing',\n",
       " 'pearls',\n",
       " 'to',\n",
       " 'swine',\n",
       " 'so',\n",
       " 'to',\n",
       " 'continue',\n",
       " 'pointless',\n",
       " 'paul',\n",
       " 'says',\n",
       " 'in',\n",
       " 'tim',\n",
       " 'and',\n",
       " 'cor',\n",
       " 'that',\n",
       " 'women',\n",
       " 'cannot',\n",
       " 'lead',\n",
       " 'but',\n",
       " 'in',\n",
       " 'other',\n",
       " 'letters',\n",
       " 'he',\n",
       " 'acknowledges',\n",
       " 'and',\n",
       " 'affirms',\n",
       " 'women',\n",
       " 'leadership',\n",
       " 'therefore',\n",
       " 'there',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'be',\n",
       " 'contradiction',\n",
       " 'which',\n",
       " 'cultural',\n",
       " 'context',\n",
       " 'helps',\n",
       " 'lay',\n",
       " 'to',\n",
       " 'rest',\n",
       " 'why',\n",
       " 'else',\n",
       " 'would',\n",
       " 'paul',\n",
       " 'not',\n",
       " 'allow',\n",
       " 'in',\n",
       " 'one',\n",
       " 'area',\n",
       " 'but',\n",
       " 'affirm',\n",
       " 'in',\n",
       " 'another',\n",
       " 'you',\n",
       " 'can',\n",
       " 'write',\n",
       " 'them',\n",
       " 'off',\n",
       " 'all',\n",
       " 'you',\n",
       " 'want',\n",
       " 'but',\n",
       " 'that',\n",
       " 'does',\n",
       " 'not',\n",
       " 'change',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'god',\n",
       " 'called',\n",
       " 'and',\n",
       " 'equipped',\n",
       " 'each',\n",
       " 'of',\n",
       " 'these',\n",
       " 'women',\n",
       " 'to',\n",
       " 'serve',\n",
       " 'him',\n",
       " 'in',\n",
       " 'authoritative',\n",
       " 'spiritual',\n",
       " 'roles',\n",
       " 'mariam',\n",
       " 'was',\n",
       " 'spiritual',\n",
       " 'authority',\n",
       " 'over',\n",
       " 'israel',\n",
       " 'isaiahs',\n",
       " 'wife',\n",
       " 'was',\n",
       " 'prophetess',\n",
       " 'and',\n",
       " 'was',\n",
       " 'spiritual',\n",
       " 'leader',\n",
       " 'in',\n",
       " 'israel',\n",
       " 'other',\n",
       " 'leaders',\n",
       " 'include',\n",
       " 'but',\n",
       " 'are',\n",
       " 'not',\n",
       " 'limited',\n",
       " 'to',\n",
       " 'esther',\n",
       " 'due',\n",
       " 'to',\n",
       " 'becoming',\n",
       " 'queen',\n",
       " 'would',\n",
       " 'have',\n",
       " 'helped',\n",
       " 'set',\n",
       " 'spiritual',\n",
       " 'tone',\n",
       " 'for',\n",
       " 'people',\n",
       " 'in',\n",
       " 'exile',\n",
       " 'lydia',\n",
       " 'mary',\n",
       " 'huldah',\n",
       " 'also',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'barak',\n",
       " 'didnt',\n",
       " 'listen',\n",
       " 'to',\n",
       " 'deborahs',\n",
       " 'authority',\n",
       " 'and',\n",
       " 'trust',\n",
       " 'what',\n",
       " 'she',\n",
       " 'said',\n",
       " 'as',\n",
       " 'gods',\n",
       " 'command',\n",
       " 'he',\n",
       " 'did',\n",
       " 'not',\n",
       " 'get',\n",
       " 'glory',\n",
       " 'in',\n",
       " 'battle',\n",
       " 'but',\n",
       " 'rather',\n",
       " 'lost',\n",
       " 'that',\n",
       " 'glory',\n",
       " 'to',\n",
       " 'hand',\n",
       " 'of',\n",
       " 'woman',\n",
       " 'priscilla',\n",
       " 'was',\n",
       " 'main',\n",
       " 'authority',\n",
       " 'in',\n",
       " 'teaching',\n",
       " 'and',\n",
       " 'preparation',\n",
       " 'of',\n",
       " 'apollos',\n",
       " 'authority',\n",
       " 'over',\n",
       " 'man',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'she',\n",
       " 'mentioned',\n",
       " 'first',\n",
       " 'indicative',\n",
       " 'of',\n",
       " 'this',\n",
       " 'fact',\n",
       " 'deborah',\n",
       " 'was',\n",
       " 'judge',\n",
       " 'placed',\n",
       " 'in',\n",
       " 'authority',\n",
       " 'over',\n",
       " 'whole',\n",
       " 'of',\n",
       " 'israel',\n",
       " 'you',\n",
       " 'know',\n",
       " 'dont',\n",
       " 'think',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'told',\n",
       " 'you',\n",
       " 'but',\n",
       " 'your',\n",
       " 'articles',\n",
       " 'have',\n",
       " 'been',\n",
       " 'an',\n",
       " 'amazing',\n",
       " 'help',\n",
       " 'to',\n",
       " 'me',\n",
       " 'in',\n",
       " 'processing',\n",
       " 'these',\n",
       " 'passages',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'for',\n",
       " 'taking',\n",
       " 'time',\n",
       " 'to',\n",
       " 'help',\n",
       " 'others',\n",
       " 'understand',\n",
       " 'lt',\n",
       " 'if',\n",
       " 'god',\n",
       " 'can',\n",
       " 'speak',\n",
       " 'with',\n",
       " 'authority',\n",
       " 'through',\n",
       " 'mouth',\n",
       " 'of',\n",
       " 'female',\n",
       " 'donkey',\n",
       " 'you',\n",
       " 'can',\n",
       " 'bet',\n",
       " 'your',\n",
       " 'bottom',\n",
       " 'dollar',\n",
       " 'that',\n",
       " 'he',\n",
       " 'can',\n",
       " 'use',\n",
       " 'that',\n",
       " 'woman',\n",
       " 'you',\n",
       " 'claim',\n",
       " 'cannot',\n",
       " 'be',\n",
       " 'pastor',\n",
       " 'goduseswomen',\n",
       " 'womenarecalledtoo',\n",
       " 'womeninministry',\n",
       " 'only',\n",
       " 'way',\n",
       " 'to',\n",
       " 'accurately',\n",
       " 'interpret',\n",
       " 'those',\n",
       " 'passages',\n",
       " 'in',\n",
       " 'light',\n",
       " 'of',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'they',\n",
       " 'do',\n",
       " 'contradict',\n",
       " 'massive',\n",
       " 'portion',\n",
       " 'of',\n",
       " 'scripture',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'toxic',\n",
       " 'cultures',\n",
       " 'in',\n",
       " 'corinth',\n",
       " 'and',\n",
       " 'ephesian',\n",
       " 'churches',\n",
       " 'because',\n",
       " 'those',\n",
       " 'commands',\n",
       " 'were',\n",
       " 'not',\n",
       " 'echoed',\n",
       " 'in',\n",
       " 'letters',\n",
       " 'to',\n",
       " 'other',\n",
       " 'churches',\n",
       " 'what',\n",
       " 'about',\n",
       " 'whole',\n",
       " 'context',\n",
       " 'of',\n",
       " 'scripture',\n",
       " 'tho',\n",
       " 'you',\n",
       " 'say',\n",
       " 'context',\n",
       " 'meaning',\n",
       " 'chapter',\n",
       " 'context',\n",
       " 'whole',\n",
       " 'of',\n",
       " 'scripture',\n",
       " 'what',\n",
       " 'about',\n",
       " 'deborah',\n",
       " 'phoebe',\n",
       " 'junia',\n",
       " 'priscilla',\n",
       " 'just',\n",
       " 'to',\n",
       " 'name',\n",
       " 'few',\n",
       " 'countless',\n",
       " 'others',\n",
       " 'were',\n",
       " 'mentioned',\n",
       " 'and',\n",
       " 'affirmed',\n",
       " 'as',\n",
       " 'women',\n",
       " 'with',\n",
       " 'spiritual',\n",
       " 'authority',\n",
       " 'over',\n",
       " 'both',\n",
       " 'sexes',\n",
       " 'when',\n",
       " 'you',\n",
       " 'discover',\n",
       " 'that',\n",
       " 'creed',\n",
       " 'bratton',\n",
       " 'played',\n",
       " 'by',\n",
       " 'actor',\n",
       " 'creed',\n",
       " 'bratton',\n",
       " 'and',\n",
       " 'like',\n",
       " 'creed',\n",
       " 'bratton',\n",
       " 'creed',\n",
       " 'bratton',\n",
       " 'also',\n",
       " 'played',\n",
       " 'in',\n",
       " 'grass',\n",
       " 'roots',\n",
       " 'wellplayedtheoffice',\n",
       " 'creedthoughts',\n",
       " 'theoffice',\n",
       " 'when',\n",
       " 'your',\n",
       " 'baby',\n",
       " 'girl',\n",
       " 'decides',\n",
       " 'to',\n",
       " 'hold',\n",
       " 'you',\n",
       " 'all',\n",
       " 'day',\n",
       " 'on',\n",
       " 'your',\n",
       " 'birthday',\n",
       " 'bestbirthdaygiftever',\n",
       " 'lovemybabies',\n",
       " 'blessedmama',\n",
       " 'those',\n",
       " 'times',\n",
       " 'when',\n",
       " 'youre',\n",
       " 'so',\n",
       " 'used',\n",
       " 'to',\n",
       " 'staying',\n",
       " 'up',\n",
       " 'past',\n",
       " ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"clean_tweets\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize and clean sentence [\"Hello world.\"] into list of words [\"hello\",\"world\"]\n",
    "def clean(sentence):\n",
    "    ignore_words = ['a']\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() #nltk.word_tokenize(sentence)\n",
    "    words_cleaned = [w.lower() for w in words if w not in ignore_words]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words_cleaned = [w for w in words_cleaned if not w in stop_words]\n",
    "    words_string = ''.join(words_cleaned)\n",
    "    return words_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split posts per users into separate sentences\n",
    "#post = []\n",
    "#utype = []\n",
    "#user = []\n",
    "\n",
    "#for index, row in df.iterrows():\n",
    "#    posts = row['posts'].split('|||')\n",
    "#    posts_clean = []\n",
    "##    for sentence in posts:\n",
    "#       posts_clean.append(clean(sentence))\n",
    "#    post.extend(posts_clean)\n",
    "#     post.extend(posts)\n",
    "#    utype.extend([row['type'] for i in range(len(posts))])\n",
    "#    user.extend([index for i in range(len(posts))])\n",
    "    \n",
    "#short_posts = pd.DataFrame({\"user\": user,\"type\": utype,\"post\": post})\n",
    "#print(short_posts.shape)\n",
    "#short_posts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBIT posts ['yeess', 'am', 'an', 'intj', 'days', 'you', 'should', 'be', 'glad', 'do', 'something', 'today', 'that', 'your', 'future', 'self', 'will', 'be', 'proud', 'of', 'more', 'days', 'couldn', 't', 'be', 'prouder', 'and', 'best', 'yet', 'to', 'come', 'cutest', 'barista', 'ever', 'such', 'role', 'model', 'that', 'look', 'up', 'to', 'march', 'has', 'been', 'so', 'good', 'to', 'me', 'days', 'left', 'last', 'weekly', 'submission', 'more', 'days', 'to', 'go', 'morning', 'sunshine', 'm', 'full', 'of', 'positivity', 'these', 'days', 'couldnt', 'be', 'prouder', 'it', 'was', 'pleasant', 'visit', 'enjoyed', 'it', 'so', 'much', 'in', 'middle', 'of', 'portfolio', 'management', 'class', 'u', 'd', 'days', 'left', 'so', 'blessed', 'with', 'my', 'family', 'amp', 'friends', 'ily', 'alll', 'years', 'of', 'fabulous', 'so', 'proud', 'of', 'you', 'finally', 'what', 'goes', 'around', 'comes', 'around', 'last', 'message', 'to', 'more', 'to', 'go', 'true', 'story', 'always', 'when', 'we', 'change', 'way', 'we', 'look', 'at', 'things', 'things', 'we', 'look', 'at', 'change', 'breathe', 'nyou', 're', 'strong', 'nyou', 've', 'got', 'this', 'none', 'day', 'at', 'time', 'day', 'out', 'of', 'of', 'my', 'life', 'in', 'b', 'amp', 'w', 'nrules', 'no', 'humans', 'no', 'explanation', 'nchallenge', 'someone', 'new', 'everyday', 'nchallenged', 'by', 'ni', 'challenge', 'happy', 'birthday', 'best', 'friend', 'day', 'out', 'of', 'of', 'my', 'life', 'in', 'b', 'amp', 'w', 'nrules', 'no', 'humans', 'no', 'explanation', 'nchallenge', 'someone', 'new', 'everyday', 'nchallenged', 'by', 'ni', 'challenge', 'day', 'out', 'of', 'of', 'my', 'life', 'in', 'b', 'amp', 'w', 'nrules', 'no', 'humans', 'no', 'explanation', 'nchallenge', 'someone', 'new', 'everyday', 'nchallenged', 'by', 'ni', 'challenge', 'day', 'out', 'of', 'of', 'my', 'life', 'in', 'b', 'amp', 'w', 'nrules', 'no', 'humans', 'no', 'explanation', 'nchallenge', 'someone', 'new', 'everyday', 'nchallenged', 'by', 'ni', 'challenge', 'day', 'out', 'of', 'of', 'my', 'life', 'in', 'b', 'amp', 'w', 'nrules', 'no', 'humans', 'no', 'explanation', 'nchallenge', 'someone', 'new', 'everyday', 'nchallenged', 'by', 'ni', 'challenge', 'day', 'out', 'of', 'of', 'my', 'life', 'in', 'b', 'amp', 'w', 'nrules', 'no', 'humans', 'no', 'explanation', 'nchallenge', 'someone', 'new', 'everyday', 'nchallenged', 'by', 'ni', 'challenge', 'h', 'e', 'l', 'p', 'that', 'just', 'made', 'my', 'day', 'yeess', 'be', 'kind', 'to', 'yourself', 'today', 'forgive', 'past', 'take', 'responsibility', 'and', 'move', 'forward', 'how', 'we', 'survive', 'boring', 'class', 'finally', 'sugar', 'free', 'syrup', 'at', 'so', 'proud', 'of', 'my', 'team', 'best', 'team', 'ever', 'monday', 'same', 'mood', 'thankk', 'you', 'ms', 'samiraa', 'thank', 'you', 'for', 'shining', 'your', 'light', 'on', 'us', 'can', 't', 'wait', 'it', 's', 'been', 'six', 'years', 'mytwitteranniversary', 'can', 't', 'wait', 'proud', 'to', 'be', 'part', 'of', 'this', 'future', 'ours', 'can', 't', 'wait', 'congratttts', 'ladies', 'الملك_ينتصر_لقياده_المراءه', 'finally', 'someone', 'said', 'that', 'struggle', 'real', 'that', 'was', 'touching', 'elegance', 'does', 'not', 'consist', 'in', 'putting', 'on', 'new', 'dress', 'coco', 'chanel', 'njoin', 'us', 'to', 'find', 'out', 'what', 'does', 'that', 'mean', 'an', 'amazing', 'idea', 'احفظها_تدوم', 'you', 'are', 'not', 'what', 'you', 'own', 'aacsb', 'so', 'proud', 'to', 'be', 'part', 'of', 'this', 'were', 'back', 'nsee', 'u', 'tmw', 'n', 'راح_نفلسكم', 'first', 'step', 'in', 'solving', 'any', 'problem', 'recognizing', 'there', 'one', 'راح_نفلسكم', 'we', 'deserve', 'better', 'im', 'in', 'راح_نفلسكم', 'before', 'you', 'pray', 'believe', 'nbefore', 'you', 'speak', 'listen', 'nbefore', 'you', 'spend', 'earn', 'nbefore', 'you', 'write', 'think', 'nbefore', 'you', 'quit', 'try', 'nbefore', 'you', 'die', 'live', 'he', 'killed', 'of', 'yours', 'before', 'he', 'got', 'killed', 'pray', 'for', 'ur', 'selves', 'saudivision', 'nhope', 'for', 'better', 'future', 'ksa', 'so', 'proud', 'breastcancer', 'we', 'are', 'not', 'muslimsarenotterrorist', 'mecca_live', 'proud', 'to', 'be', 'muslim', 'stats', 'for', 'day', 'have', 'arrived', 'new', 'follower', 'and', 'no', 'unfollowers', 'via', 'stats', 'for', 'day', 'have', 'arrived', 'new', 'unfollower', 'stats', 'via', 'dear', 'nbe', 'eye', 'of', 'world', 'by', 'having', 'mecca_live', 'on', 'july', 'th', 'it', 's', 'topic', 'that', 'concerns', 'billion', 'people', 'in', 'this', 'planet', 'new', 'day', 'new', 'tweets', 'new', 'stats', 'followers', 'unfollower', 'via', 'good', 'old', 'stats', 'for', 'day', 'have', 'arrived', 'new', 'follower', 'and', 'no', 'unfollowers', 'via', 'new', 'day', 'new', 'tweets', 'new', 'stats', 'new', 'unfollower', 'via', 'good', 'old', 'new', 'day', 'new', 'tweets', 'new', 'stats', 'new', 'unfollower', 'via', 'good', 'old', 'number', 'crunching', 'for', 'past', 'day', 'new', 'unfollower', 'stats', 'via', 'number', 'crunching', 'for', 'past', 'day', 'new', 'followers', 'and', 'no', 'unfollowers', 'stats', 'via', 'new', 'unfollower', 'in', 'last', 'day', 'via', 'daily', 'unfollower', 'crowdfire', 'doesnt', 'miss', 'trick', 'via', 'stats', 'for', 'day', 'have', 'arrived', 'new', 'follower', 'and', 'no', 'unfollowers', 'via', 'new', 'day', 'new', 'tweets', 'new', 'stats', 'followers', 'unfollowers', 'via', 'good', 'old', 'mbcthexfactor', 'hamzaaaaa', 'you', 'deserve', 'it', 'tweep', 'followed', 'thank', 'you', 'and', 'nobody', 'unfollowed', 'me', 'in', 'past', 'day', 'thank', 'you', 'follower', 'unfollowers', 'didnt', 'know', 'was', 'this', 'awesome', 'get', 'your', 'daily', 'stats', 'via', 'amazing', 'followers', 'in', 'last', 'day', 'and', 'there', 'will', 'be', 'more', 'tomorrow', 'growing', 'with', 'ليش_لا', 'great', 'episode', 'stats', 'for', 'day', 'have', 'arrived', 'new', 'follower', 'and', 'no', 'unfollowers', 'via', 'follower', 'unfollowers', 'didnt', 'know', 'was', 'this', 'awesome', 'get', 'your', 'daily', 'stats', 'via', 'stats', 'for', 'day', 'have', 'arrived', 'new', 'follower', 'and', 'no', 'unfollowers', 'via', 'follower', 'unfollowers', 'didnt', 'know', 'was', 'this', 'awesome', 'get', 'your', 'daily', 'stats', 'via', 'daily', 'followers', 'unfollowers', 'justunfollow', 'doesnt', 'miss', 'trick', 'via', 'support', 'celiac', 'disease', 'awareness', 'celiac', 'السيلياك', 'project', 'support', 'celiac', 'disease', 'awareness', 'celiac', 'السيلياك', 'project', 'you', 'dont', 'choose', 'your', 'family', 'they', 'are', 'allahs', 'gift', 'to', 'you', 'as', 'you', 'are', 'to', 'them', 'n', 'nthank', 'you', 'mobarak', 'love', 'u', 'bro', 'thanks', 'chanel', 'my', 'new', 'love', 'love', 'my', 'girls', 'u', 'make', 'my', 'day', 'my', 'birthday', 'love', 'u', 'love', 'u', 'surprise', 'birthday', 'friends', 'good', 'morning', 'sunshine', 'first', 'day', 'healthy', 'breakfast', 'school', 'ipa', 'maybe', 'one', 'day', 'youll', 'understand', 'why', 'lethergo', 'lets', 'work', 'till', 'morning', 'n', 'busy_week', 'caffeine', 'tea', 'with', 'mint', 'tea', 'nlets', 'do', 'report', 'sorry', 'but', 'this', 'not', 'in', 'saudi', 'arabia', 'and', 'has', 'nothing', 'to', 'islam', 'theyre', 'singing', 'contract', 'for', 'carton', 'film', 'in', 'uae', 'love', 'u', 'get', 'little', 'bit', 'bigger', 'but', 'then', 'ill', 'admitim', 'just', 'same', 'as', 'was', 'now', 'dont', 'you', 'understand', 'that', 'im', 'never', 'changing', 'who', 'am', 'hbd', 'sweetie', 'may', 'all', 'ur', 'dreams', 'come', 'true', 'we', 'miss', 'u', 'gurl', 'take', 'this', 'quiz', 'cutttttteee', 'love', 'u', 'need', 'u', 'people', 'followed', 'me', 'and', 'people', 'unfollowed', 'me', 'automatically', 'checked', 'by', 'miss', 'u', 'more', 'go', 'to', 'bed', 'waiting', 'abu', 'klb', 'w', 'n', 'buy', 'blackberry', 'soon', 'miss', 'u', 'people', 'followed', 'me', 'and', 'people', 'unfollowed', 'me', 'automatically', 'checked', 'by', 'am', 'proud', 'of', 'myself', 'god', 'blessed', 'me', 'people', 'followed', 'me', 'and', 'people', 'unfollowed', 'me', 'automatically', 'checked', 'by', 'lol', 'for', 'u', 'no', 'p', 'twitition', 'we', 'ask', 'you', 'colorado_free_alturki', 'and', 'return', 'him', 'to', 'his', 'children', 'hickforco', 'barackobama', 'future', 'p', 'boring', 'm', 'bored', 'thank', 'u', 'lets', 'continue', 'studying', 'nerd', 'love', 'ya', 'bank', 'place', 'where', 'they', 'lend', 'you', 'an', 'umbrella', 'in', 'fair', 'weather', 'and', 'ask', 'for', 'it', 'back', 'when', 'it', 'begins', 'to', 'rain', '_', 'robert', 'frost', 'quote', 'u', 'too', 'dont', 'feel', 'like', 'want', 'to', 'start', 'but', 'will', 'now', 'no', 'havent', 'started', 'yet', 'ok', 'im', 'waiting', 'how', 'my', 'voice', 'when', 'was', 'sleeping', 'loll', 'did', 'talk', 'right', 'where', 'tweet', 'you', 'will', 'write', 'about', 'me', 'having', 'breakfast', 'with', 'ms', 'kate', 'nd', 'girls', 'im', 'going', 'to', 'cry', 'finalllllllllly', 'done', 'with', 'words', 'cool', 'coffee', 'cupcakes', 'lets', 'start', 'working', 'for', 'long', 'night', 'yay', 'repost', 'an', 'amazing', 'day', 'its', 'have', 'time', 'right', 'tomorrow', 'have', 'reading', 'midterm', 'nd', 'havent', 'finished', 'yet', 'wish', 'me', 'luck', 'please', 'people', 'followed', 'me', 'and', 'one', 'person', 'unfollowed', 'me', 'automatically', 'checked', 'by', 'elyzee', 'medical', 'center', 'offering', 'jobs', 'for', 'graduate', 'girls', 'in', 'ipa', 'sabb', 'bank', 'offering', 'jobs', 'for', 'graduate', 'girls', 'in', 'ipa', 'repost', 'gt', 'love', 'u', 'my', 'girls', 'it', 'has', 'been', 'my', 'best', 'day', 'ever', 'xoxo', 'aww', 'love', 'uu', 'was', 'soo', 'happy', 'too', 'need', 'my', 'medicine', 'which', 'of', 'course', 'ice', 'cream', 'dont', 'make', 'me', 'cry', 'today', 'not', 'my', 'day', 'im', 'sad', 'its', 'not', 'fun', 'to', 'absent', 'for', 'weeks', 'now', 'have', 'lots', 'of', 'shits', 'to', 'do', 'work', 'shit', 'absent', 'not', 'joy', 'if', 'u', 'r', 'absent', 'for', 'weeks', 'lots', 'of', 'shits', 'to', 'do', 'work', 'shit', 'absent', 'oh', 'shit', 'stop', 'looking', 'for', 'someone', 'whos', 'already', 'forgotten', 'you', 'happy', 'birthday', 'u', 'semo', 'happy', 'person', 'hope', 'ur', 'dreams', 'come', 'true', 'want', 'my', 'face', 'back', 'love', 'love', 'love', 'icecream', 'in', 'love', 'with', 'these', 'cute', 'beautiful', 'things', 'from', 'im', 'depressed', 'people', 'followed', 'me', 'and', 'people', 'unfollowed', 'me', 'automatically', 'checked', 'by', 'lets', 'be', 'boring', 'im', 'depressed', 'nothingfeelsbetterthan', 'watching', 'good', 'movie', 'alone', 'first', 'class', 'with', 'ms', 'luda', 'like', 'mornings', 'sunshine', 'our', 'survey', 'of', 'fashion', 'mornings', 'sunshine', 'sleep', 'now', 'sleeping', 'love', 'my', 'bed', 'lots', 'of', 'things', 'to', 'do', 'it', 'amazing', 'am', 'proud', 'of', 'you', 'miss', 'u', 'ifidontreplyitsbecause', 'im', 'sleeping', 'im', 'eating', 'there', 'nothing', 'to', 'say', 'enjoy', 'ur', 'weekend', 'xd', 'all', 'days', 'honey', 'hello', 'midterms', 'this', 'world', 'book', 'and', 'those', 'who', 'do', 'not', 'travel', 'read', 'only', 'page', 'take', 'my', 'stuff', 'without', 'asking', 'me', 'waystomakememad', 'really', 'need', 'some', 'space', 'for', 'myself', 'adore', 'this', 'pink', 'pen', 'thank', 'u', 'my', 'sweet', 'reemalahmari', 'pasta', 'delicous', 'day', 'of', 'cooking', 'p', 'eatmbc', 'great', 'movie', 'ever', 'wanna', 'try', 'this', 'experience', 'if', 'u', 'dont', 'tell', 'me', 'whats', 'wrong', 'how', 'cannot', 'make', 'it', 'right', 'wanna', 'forget', 'mn', 'jdd', 'lazm', 'taklen', 'youre', 'scary', 'when', 'you', 'hungry', 'nightmares', 'every', 'day', 'just', 'posted', 'photo', 'be', 'cool', 'bad', 'headache', 'for', 'long', 'week', 'black', 'book', 'from', 'inside', 'and', 'outside', 'book', 'my', 'birthday', 'soon', 'ymkn', 'd', 'y', 'enii', 'face', 'w', 'ke', 'why', 'bibty', 'best', 'coffee', 'ever', 'coffee', 'jordan', 'my', 'daily', 'stats', 'new', 'follower', 'via', 'people', 'followed', 'me', 'and', 'one', 'person', 'unfollowed', 'me', 'automatically', 'checked', 'by', 'my', 'daily', 'stats', 'new', 'follower', 'via', 'my', 'daily', 'stats', 'new', 'unfollower', 'via', 'remi', 'new', 'year', 'chocolate', 'newyear', 'reemalahmari', 'happy', 'new', 'year', 'xoxo', 'welcome', 'cutttte', 'love', 'it', 'nony', 'its', 'yours', 'babe', 'finally', 'love', 'vanilla', 'vanilla', 'love', 'love', 'love', 'my', 'favorite', 'flowers', 'casablanca', 'love', 'them', 'really', 'need', 'break', 'my', 'daily', 'stats', 'new', 'follower', 'via', 'ohh', 'babe', 'o', 'im', 'still', 'in', 'bed', 'missing', 'thank', 'u', 'babe', 'girls', 're', 'invite', 'me', 'on', 'my', 'bbm', 'feel', 'free', 'an', 'amazing', 'day', 'in', 'love', 'wirh', 'this', 'photo', 'people', 'followed', 'me', 'and', 'people', 'unfollowed', 'me', 'automatically', 'checked', 'by', 'my', 'daily', 'stats', 'new', 'followers', 'via', 'you', 'deserve', 'more', 'than', 'that', 'babe', '_', 'my', 'daily', 'stats', 'new', 'unfollower', 'via', 'delicious', '_', 'summer', 'my', 'new', 'books', 'hope', 'to', 'finsh', 'them', 'weeks', 'my', 'daily', 'stats', 'new', 'unfollowers', 'via', 'my', 'daily', 'stats', 'new', 'followers', 'new', 'unfollowers', 'via', 'gotta', 'tell', 'you', 'can', 'feel', 'love', 'you', 'know', 'said', 'its', 'true', 'can', 'feel', 'love', 'can', 'u', 'feel', 'it', 'too', 'my', 'daily', 'stats', 'new', 'follower', 'via', 'my', 'daily', 'stats', 'new', 'followers', 'new', 'unfollowers', 'via', 'my', 'daily', 'stats', 'new', 'followers', 'new', 'unfollowers', 'via', 'my', 'daily', 'stats', 'new', 'followers', 'new', 'unfollowers', 'via', 'my', 'daily', 'stats', 'new', 'followers', 'new', 'unfollowers', 'via', 'turn', 'music', 'louder', 'so', 'wont', 'hear', 'my', 'thoughts', 'but', 'its', 'stupid', 'because', 'lyrics', 'remind', 'me', 'of', 'what', 'im', 'trying', 'to', 'forget', 'gm', 'every', 'one', 'my', 'daily', 'stats', 'new', 'followers', 'new', 'unfollowers', 'via', 'ill', 'be', 'using', 'new', 'unfollow', 'tracker', 'from', 'to', 'track', 'my', 'unfollowers', 'final', 'exams', 'feel', 'bad', 'dont', 'know', 'why', 'boring', 'my', 'final', 'presentation', 'on', 'mon', 'but', 'no', 'matter', 'what', 'youll', 'never', 'see', 'me', 'cty', 'pitbull', 'youre', 'amazzzing', 'lt', 'pitbull', 'jk', 'love', 'u', 'egyption', 'ppl', 'sick', 'prt', 'money', 'freedom', 'honest', 'prt', 'funny', 'specail', 'pure', 'lol', 'its', 'ok', 'sweety', 'if', 'only', 'من_أجمل_الافلام_الي_شفتها', 'p', 'c', 'love', 'you', 'من_أجمل_الافلام_الي_شفتها', 'hunger', 'game', 'من_أجمل_الافلام_الي_شفتها', 'last', 'friday', 'before', 'break', 'd', 'it', 'doesnt', 'like', 'weekend', 'boring', 'wednesday', 'wanna', 'sleep', 'n', 'lonnnnng', 'nap', 'wanna', 'sleep', 'to', 'bed', 'milk', 'shake', 'at', 'midnight', 'nothing', 'better', 'dear', 'bed', 'dont', 'wanna', 'leave', 'u', 'girls', 'youre', 'gonna', 'find', 'yourselves', 'somewhere', 'somehow', 'and', 'here', 'day', 'starts', 'felling', 'so', 'bad', 'so', 'excited', 'finally', 'became', 'officially', 'student', 'at', 'ipa', 'ipa', 'felling', 'so', 'bad', 'get', 'well', 'soon', 'mum', 'my', 'photo', 'my', 'photo', 'by', 'galaxy', 's', 'just', 'boring', 'amp', 'wanna', 'my', 'mom', 'im', 'people', 'felling', 'not', 'so', 'well', 'thanks', 'for', 'party', 'nd', 'all', 'gifts', 'thanks', 'my', 'mom', 'amp', 'my', 'sisters', 'cant', 'wait', 'back', 'my', 'p', 'c', 'in', 'jeddah', 'my', 'p', 'c', 'tired', 'sick', 'wanna', 'my', 'bed', 'free', 'hug', 'plz', 'best', 'day', 'ever', 'love', 'u', 'girls', '_', 'tomorrow', 'graduation', 'day', 'class', 'of', 'seniors', 'may', 'wish', 'i_wish', 'may', 'my', 'graduation', 'day', 'feel', 'bad', 'wanna', 'to', 'cry', 'just', 'wanna', 'to', 'sleep', 'until', 'tomorrow', 'dear', 'old', 'days', 'miss', 'you', 't', 'bb', 'wanna', 'to', 'sleep', 'ff', 'my', 'sweet', 'just', 'wanna', 'to', 'cry', 'waiting', 'takki', 'takki', 'hbd', 'nouf', 'bebty', 'hope', 'all', 'your', 'dreams', 'come', 'true', 'feel', 'bad', 'from', 'today', 'no', 'rice', 'no', 'pizza', 'no', 'pasta', 'no', 'cola', 'no', 'coffee', 'no', 'sweets', 'god', 'help', 'me', 'wanna', 'stop', 'thinking', 'really', 'hate', 'that', 'moment', 'when', 'u', 'have', 'to', 'choice', 'your', 'future', 'thats', 'hard', 'also', 'with', 'some', 'ppl', 'who', 'laugh', 'they', 'think', 'thats', 'they', 'so', 'funny', 'dear', 'physic', 'hate', 'u', 'feel', 'bad', 'wanna', 'to', 'sleep', 'and', 'sleep', 'and', 'sleep', 'until', 'sleep', 'hate', 'me', 'love', 'my', 'mom', 'feel', 'sad', 'really', 'love', 'my', 'twitter', 'love', 'twitter', 'words', 'dont', 'have', 'power', 'to', 'hurt', 'u', 'unless', 'person', 'who', 'said', 'them', 'means', 'lot', 'to', 'u', 'hate', 'my', 'eyes', 'after', 'crying', 'this', 'love', 'love', 'hurts', 'happy', 'birth', 'day', 'my', 'baby', 'rakan', 'ue', 'ec', 'lt', 'p', 'sweet', 'honey', 'ue', 'lt', 'lt', 'good', 'morning', 'for', 'all', 'nhave', 'nice', 'day', 'lt', 'feel', 'really', 'bored', 'lonely', 'dont', 'know', 'what', 'want', 'n', 'nfeel', 'bad']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data: 80% train, 20% test\n",
    "post_train, post_test, label_train, label_test = train_test_split(np.array(df['clean_tweets']), \n",
    "                                                    np.array(df['Personality']), \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=88)\n",
    "\n",
    "\n",
    "print(\"MBIT posts\", post_train[2])\n",
    "print('')\n",
    "#print(\"MBTI Labels: \",label_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68981"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a vocabulary (V size is defaulted to full text) for train corpus\n",
    "\n",
    "vocab_train = []\n",
    "for i in range(len(post_train)):\n",
    "    for word in post_train[i]:\n",
    "        vocab_train.append(word)\n",
    "\n",
    "\n",
    "vocab_mbti = vocabulary.Vocabulary((w for w in vocab_train))\n",
    "#need to lower case all words if they are words\n",
    "vocab_mbti.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (68,981 words) written to 'vocab.csv'\n"
     ]
    }
   ],
   "source": [
    "vocab_mbti.write_flat_file('vocab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to']\n"
     ]
    }
   ],
   "source": [
    "print (vocab_mbti.ids_to_words([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 28, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "print (vocab_mbti.words_to_ids(['got','what','and','the']))\n",
    "#print (vocab_mbti.ids_to_words([202, 147565, 317206, 159348])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[38,\n",
       " 47,\n",
       " 928,\n",
       " 221,\n",
       " 4,\n",
       " 221,\n",
       " 4,\n",
       " 11,\n",
       " 403,\n",
       " 1589,\n",
       " 930,\n",
       " 16,\n",
       " 2259,\n",
       " 259,\n",
       " 13,\n",
       " 759,\n",
       " 71,\n",
       " 73,\n",
       " 1117,\n",
       " 7756,\n",
       " 515,\n",
       " 2013]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_mbti.words_to_ids(post_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'an'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_train[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and canonicalize train and test sets\n",
    "x_train = []\n",
    "for i in range(len(post_train)):\n",
    "    x_train.append(vocab_mbti.words_to_ids(post_train[i]))\n",
    "\n",
    "x_test = []\n",
    "for i in range(len(post_test)):\n",
    "    x_test.append(vocab_mbti.words_to_ids(post_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  ['was', 'one', 'of', 'hankering', 'friends', 'am', 'an', 'enfp', 'and', 'm', 'very', 'glad', 'to', 'have', 'you', 'in', 'family', 'you', 'lost', 'me', 'at', 'calf', 'fry', 'best', 'feeling', 'when', 'you', 'find', 'your', 'people', 'you', 'must', 'like', 'ted', 'cruz', 'if', 'you', 'support', 'global', 'warming', 'paterack', 'mccauly', 'thank', 'you', 'for', 'giving', 'us', 'wonderful', 'blend', 'of', 'suspense', 'and', 'comedy', 'everyone', 'needs', 'to', 'see', 'us', 'was', 'big', 'nick', 'collison', 'fan', 'because', 'was', 'white', 'll', 'wear', 's', 'oscar', 'jacket', 'to', 'prom', 'if', 'someone', 'accompanies', 'me', 'with', 'dress', 'offer', 'open', 'to', 'all', 'old', 'town', 'road', 'some', 'of', 'y', 'all', 'say', 'cried', 'when', 'x', 'died', 'that', 's', 'cool', 'but', 'where', 'were', 'you', 'when', 'we', 'lost', 'steve', 'irwin', 'that', 's', 'what', 'thought', 'world', 'would', 'be', 'so', 'different', 'if', 'everyone', 'sang', 'in', 'shower', 'it', 's', 'hard', 'not', 'to', 'stare', 'when', 'you', 're', 'making', 'eye', 'contact', 'me', 'attempting', 'to', 'hold', 'in', 'my', 'cough', 'while', 'in', 'class', 'poot', 'sometimes', 'guys', 'wanna', 'be', 'little', 'spoon', 'too', 'm', 'fool', 'for', 'dippin', 'dots', 'somebody', 'lost', 'their', 'safe', 'drivers', 'check', 'rent', 'my', 'favorite', 'christmas', 'movie', 'why', 'everyone', 'posing', 'so', 'much', 'about', 'christmas', 'eve', 'today', 'also', 'day', 'aristocats', 'came', 'out', 'these', 'postmates', 'ads', 'are', 'next', 'level', 'uncomfortable', 'did', 'alf', 'have', 'tourette', 's', 'syndrome', 'being', 'child', 'am', 'my', 'favorite', 'part', 'of', 'this', 'season', 'of', 'survivor', 'jeff', 's', 'narration', 'of', 'contestant', 's', 'balls', 'survivordavidvsgoliath', 'wanna', 'sprite', 'cranberry', 'update', 'at', 'random', 'times', 'my', 'sister', 'says', 'lightbulb', 'like', 'gru', 'this', 'purely', 'anti', 'oklahoman', 'propaganda', 'at', 'least', 'you', 'aren', 't', 'jets', 'right', 'now', 'absolutely', 'celebrate', 'talk', 'like', 'pirate', 'day', 'there', 'has', 'never', 'been', 'feeling', 'comparable', 'to', 'watching', 'elphaba', 'fly', 'at', 'end', 'of', 'act', 'am', 'livid', 'bb', 'd', 'like', 'arts', 'students', 'to', 'be', 'considered', 'for', 'things', 'ya', 'know', 'band', 'doesn', 't', 'get', 'banners', 'for', 'seniors', 'or', 'homecoming', 'huh', 'we', 're', 'in', 'this', 'together', 'hardest', 'part', 'about', 'existing', 'rn', 'battling', 'allergies', 'house', 'with', 'clock', 'in', 'its', 'walls', 'looks', 'like', 'horrible', 'movie', 'but', 'will', 'spend', 'money', 'to', 'watch', 'it', 'because', 'of', 'jack', 'black', 'm', 'praying', 'for', 'you', 'my', 'man', 'it', 's', 'cool', 'to', 'know', 'that', 'blind', 'characters', 'who', 'are', 'prophetic', 'have', 'been', 'in', 'literature', 'for', 'longest', 'time', 'millerapeng', 'feel', 'like', 'readers', 'have', 'natural', 'sympathy', 'for', 'character', 'who', 'marked', 'even', 'if', 'intent', 'to', 'show', 'character', 's', 'ways', 'millerapeng', 'since', 'there', 'never', 'new', 'story', 'wonder', 'if', 'there', 'are', 'any', 'tropes', 'or', 'ideas', 'that', 'authors', 'would', 'consider', 'overused', 'are', 'there', 'themes', 'and', 'stories', 'authors', 'are', 'tired', 'of', 'writing', 'seeing', 'always', 'assumed', 'rain', 'in', 'literature', 'was', 'warning', 'reader', 'that', 'something', 'bad', 'was', 'in', 'their', 'future', 'it', 's', 'interesting', 'to', 'now', 'know', 'other', 'symbolic', 'meanings', 'of', 'rain', 'millerapeng', 'still', 'remember', 'how', 'much', 'more', 'interesting', 'lord', 'of', 'flies', 'became', 'when', 'explained', 'simon', 's', 'christ', 'like', 'characteristics', 'have', 'always', 'connected', 'communion', 'with', 'church', 'never', 'thought', 'about', 'how', 'simple', 'act', 'of', 'sharing', 'meal', 'could', 'be', 'so', 'personal', 'millerapeng', 'imagine', 'finding', 'love', 'of', 'your', 'life', 'just', 'to', 'discover', 'they', 'sleep', 'in', 'socks', 'america', 's', 'sweetheart', 'this', 'still', 'relevant', 'am', 'only', 'one', 'with', 'no', 'desire', 'at', 'all', 'to', 'see', 'christopher', 'robin', 'this', 'needs', 'to', 'be', 'tee', 'shirt', 'when', 'luis', 'fonsi', 'said', 'despacito', 'bro', 'felt', 'that', 'this', 'holy', 'image', 'best', 'veto', 'speech', 'in', 'bb', 'history', 'best', 'member', 'of', 'fab', 'never', 'cared', 'for', 'stories', 'until', 'you', 'entered', 'mine', 'your', 'eyes', 'from', 'rent', 'never', 'fails', 'to', 'make', 'me', 'emotional', 'just', 'want', 'to', 'be', 'so', 'bad', 'lebron', 'and', 'kawhi', 'bouta', 'link', 'up', 'please', 'send', 'help', 'can', 't', 'stop', 'listening', 'to', 'godspell', 'if', 'it', 's', 'kanye', 's', 'birthday', 'does', 'that', 'make', 'today', 'christmas', 'my', 'one', 'complaint', 'about', 'reasons', 'why', 'that', 'everyone', 'too', 'attractive', 'you', 're', 'princeton', 'and', 'm', 'kate', 'monster', 'not', 'many', 'will', 'get', 'reference', 'but', 've', 'been', 'waiting', 'to', 'hear', 'those', 'words', 'my', 'whole', 'life', 'mission', 'accomplished', 'play', 'viola', 'rodolfo', 's', 'couldn', 't', 'handle', 'our', 'power', 'if', 'you', 'know', 'me', 'this', 'me', 'keep', 'your', 'two', 'cents', 'take', 'your', 'own', 'advice', 'don', 't', 'gotta', 'worry', 'about', 'it', 'zo', 'learned', 'my', 'lesson', 'just', 'glad', 'she', 'isn', 't', 'my', 'cousin', 'ya', 'know', 'baja', 'blast', 'back', 'on', 'shelves', 'god', 'good', 'pinstripes', 'are', 'all', 'that', 'they', 'see', 'psa', 'nmy', 'stage', 'makeup', 'makes', 'me', 'feel', 'like', 'adam', 'lambert', 'love', 'theater', 'this', 'isn', 't', 'church', 'you', 'don', 't', 'need', 'to', 'stand', 'suzie', 'kessler', 'dangit', 'shrek', 'musical', 'got', 'me', 'in', 'feels', 'before', 'prom', 'it', 's', 'been', 'too', 'long', 'since', 've', 'seen', 'scott', 'pilgrim', 'where', 'can', 'watch', 'it', 'would', 'highly', 'recommend', 'not', 'trying', 'them', 'would', 'do', 'nothing', 'for', 'klondike', 'bar', 'would', 'actually', 'endure', 'pain', 'to', 'secure', 'never', 'have', 'to', 'eat', 'one', 'again', 'can', 'not', 'express', 'how', 'disgusted', 'am', 'with', 'product', 'had', 'one', 'for', 'first', 'time', 'tonight', 'and', 'it', 'ruined', 'my', 'entire', 'weekend', 'my', 'redeemer', 'lives', 'people', 'ask', 'god', 'for', 'forgiveness', 'on', 'daily', 'but', 'for', 'some', 'reason', 'think', 'm', 'not', 'candidate', 'to', 'receive', 'same', 'grace', 'we', 'are', 'all', 'equal', 'we', 'all', 'fall', 'we', 'all', 'get', 'second', 'chance', 'there', 'no', 'higher', 'honor', 'then', 'to', 'be', 'compared', 'to', 'my', 'true', 'grandfather', 'll', 'se', 'him', 'at', 'thanksgiving', 'only', 'thing', 'want', 'in', 'life', 'to', 'go', 'see', 'movie', 'isle', 'of', 'dogs', 'this', 'must', 'be', 'settled', 'nwhat', 'lil', 'chairs', 'best', 'song', 'didn', 't', 'text', 'it', 'but', 'did', 'say', 'it', 'would', 'like', 'to', 'give', 'congratulations', 'for', 'excellent', 'try', 'however', 'it', 'midnight', 'so', 'will', 'not', 'be', 'joining', 'best', 'cheer', 'team', 'in', 'state', 'miss', 'steve', 'irwin', 'so', 'much', 'rn', 'this', 'stolen', 'content', 'do', 'not', 'control', 'that', 'speed', 'at', 'which', 'lobsters', 'die', 'stop', 'expecting', 'perfection', 'by', 'definition', 'it', 's', 'impossible', 'not', 'much', 'has', 'changed', 'mr', 'matheson', 'probably', 'biggest', 'role', 'model', 'in', 'my', 'life', 'rn', 'regret', 'sleepin', 'on', 'logic', 'gary', 'now', 'know', 'was', 'wrong', 'messed', 'up', 'and', 'now', 'you', 're', 'gone', 'you', 'know', 'our', 'nation', 'in', 'quite', 'spot', 'when', 'ya', 'boy', 'riff', 'raff', 'has', 'to', 'lay', 'out', 'some', 'words', 'ya', 'know', 'if', 'rapture', 'happened', 'wouldnt', 'have', 'to', 'finish', 'my', 'summer', 'assignment', 'are', 'we', 'all', 'gonna', 'sleep', 'on', 'lil', 'chair', 'all', 'of', 'sudden', 'better', 'stay', 'woketh', 'im', 'jake', 'paulers', 'this', 'how', 'you', 'know', 'something', 'good', 'just', 'happened', 'bb', 'people', 'are', 'with', 'you', 'dab', 'on', 'and', 'haters', 'looks', 'like', 'people', 'say', 'otherwise', 'correct', 'spelling', 'when', 'forrest', 'reading', 'his', 'sons', 'letter', 'at', 'jennys', 'grave', 'instead', 'of', 'complaining', 'how', 'hard', 'game', 'give', 'it', 'play', 'gotta', 'give', 'some', 'props', 'to', 'beibs', 'for', 'turning', 'his', 'life', 'around', 'youre', 'immature', 'if', 'you', 'dont', 'think', 'ross', 'plug', 'real', 'tragedy', 'somehow', 'every', 'girl', 'you', 'date', 'says', 'they', 'have', 'trust', 'issues', 'but', 'end', 'up', 'hopping', 'back', 'into', 'their', 'previous', 'toxic', 'relationship', 'idk', 'about', 'you', 'but', 'im', 'tryna', 'get', 'someone', 'to', 'send', 'me', 'some', 'mojos', 'mm', 'fish', 'dinners', 'not', 'much', 'has', 'changed', 'guess', 'im', 'big', 'brother', 'fan', 'now', 'bb', 'am', 'only', 'person', 'that', 'cant', 'spend', 'more', 'then', 'one', 'day', 'alone', 'at', 'least', 'he', 'got', 'his', 'ring', 'which', 'group', 'you', 'got', 'if', 'she', 'doesnt', 'sing', 'accompaniment', 'to', 'fruit', 'salad', 'with', 'you', 'find', 'someone', 'better', 'grab', 'your', 'snacks', 'my', 'dudes', 'at', 'it', 'again', 'when', 'mrs', 'patty', 'wont', 'curve', 'your', 'to', 'an', 'aint', 'gonna', 'lie', 'has', 'flow', 'and', 'message', 'hes', 'got', 'my', 'support', 'gotta', 'give', 'my', 'boi', 'shoutout', 'happy', 'you', 'can', 't', 'sit', 'with', 'us', 'my', 'boy', 'just', 'got', 'dan', 'schneiderd', 'changing', 'game', 'with', 'her', 'revolutionary', 'complex', 'tweets', 'gotta', 'see', 'this', 'sesame', 'street', 'for', 'sure', 'my', 'new', 'favorite', 'show', 'eventually', 'king', 'pays', 'his', 'dues', 'to', 'god', 'n', 'now', 'remember', 'why', 'have', 'tweet', 'notifications', 'on', 'can', 'someone', 'say', 'mvp', 'was', 'right', 'tho', 'its', 'nerf', 'or', 'nothing', 'shout', 'out', 'to', 'donald', 'glover', 'for', 'getting', 'golden', 'globe', 'theyre', 'on', 'to', 'me', 'everyone', 'better', 'choose', 'correctly', 'tomorrow', 'me', 'in', 'minutes', 'thewalkingdead', 'thus', 'true', 'meme', 'lord', 'emerges', 'juntos', 'now', 'my', 'jam', 'im', 'ready', 'to', 'grind', 'wraneslyfe', 'yes', 'can', 'see', 'her', 'almost', 'perfectly', 'in', 'this', 'cracked', 'darkness', 'omg', 'im', 'so', 'surprised', 'thx', 'bb', 'just', 'gonna', 'leave', 'this', 'here', 'my', 'and', 'my', 'boi', 'with', 'our', 'favs', 'mcdunmutedokc', 'sweeps', 'my', 'life', 'growingupinoklahoma', 'knowing', 'state', 'song', 'by', 'heart', 'love', 'trying', 'to', 'be', 'real', 'with', 'someone', 'and', 'then', 'they', 'end', 'up', 'not', 'being', 'able', 'to', 'comprehend', 'it', 'aint', 'nothing', 'like', 'good', 'cringe', 'after', 'watching', 'musical', 'ly', 'theres', 'not', 'storm', 'in', 'cush', 'unless', 'everyone', 'puts', 'it', 'online', 'even', 'doc', 'apart', 'of', 'nation', 'vn', 'r', 'p', 'republican', 'party', 'look', 'out', 'im', 'bouta', 'drop', 'viewafromthe', 'id', 'rather', 'watch', 'agricultural', 'films', 'for', 'next', 'five', 'weeks', 'votebajablast', 'to', 'keep', 'it', 'around', 'after', 'summer', 'which', 'flavor', 'will', 'you', 'choose', 'vote', 'now', 'kobe', 'isnt', 'even', 'and', 'hes', 'set', 'for', 'life', 'when', 'memes', 'are', 'dank', 'don', 't', 'want', 'any', 'drugs', 'fitnessgram', 'pacer', 'test', 'multistage', 'aerobic', 'capacity', 'test', 'that', 'progressively', 'gets', 'more', 'difficult', 'as', 'it', 'continues', 'love', 's', 'new', 'hair', 'as', 'keep', 're', 'watching', 'this', 'still', 'cant', 'believe', 'it', 'happened', 'well', 'lets', 'just', 'say', 'not', 'all', 'of', 'our', 'inside', 'jokes', 'are', 'appropriate', 'mustache', 'cents', 'jew', 'im', 'ethan', 'bradberry', 'fav', 'song', 'ive', 'come', 'to', 'look', 'for', 'america', 'deal', 'with', 'it', 'pools', 'of', 'sorrow', 'nwaves', 'of', 'joy', 'nare', 'drifting', 'through', 'my', 'open', 'mind', 'its', 'not', 'easy', 'being', 'meme', 'finally', 'somebody', 'knows', 'me', 'when', 'see', 'someone', 'tryna', 'repost', 'my', 'dank', 'memes', 'your', 'homework', 'due', 'monday', 'he', 'need', 'some', 'milk', 'if', 'you', 'dont', 'have', 'anything', 'to', 'do', 'come', 'to', 'field', 'house', 'at', 'happy', 'birthday', 'youngins', 'thanks', 'to', 'everyone', 'who', 'ive', 'ever', 'had', 'pleasure', 'to', 'call', 'friend', 'when', 'your', 'gma', 'gives', 'you', 'that', 'bday', 'hook', 'up', 'my', 'locker', 'looks', 'like', 'bad', 'game', 'of', 'jenga', 'where', 'did', 'go', 'wrong', 'just', 'because', 'someone', 'else', 'good', 'doesnt', 'mean', 'youre', 'not', 'civics', 'thought', 'he', 'taught', 'government', 'this', 'cinema', 'masterpiece', 'cringe', 'm', 'using', 'my', 'calculator', 'miss', 'columbia', 'was', 'on', 'new', 'season', 'of', 'punkd', 'with', 'steve', 'harvey', 'dont', 'care', 'if', 'its', 'cowboy', 'or', 'sooner', 'if', 'see', 'one', 'more', 'annoying', 'bedlam', 'meme', 'will', 'eat', 'broken', 'glass', 'just', 'think', 'of', 'sweet', 'meat', 'to', 'relief', 'you', 'when', 'mrs', 'fairbanks', 'roast', 'you', 'for', 'pronouncing', 'word', 'wrong', 'at', 'least', 'used', 'holy', 'spirit', 'to', 'cross', 'that', 'young', 'man', 'just', 'love', 'getting', 'swatted', 'walks', 'into', 'petco', 'smell', 'wildlife', 'tyler', 'norman', 'lit', 'af', 'halloween', 'getting', 'spooky', 'me', 'cant', 'tell', 'if', 'like', 'penguin', 'or', 'not', 'gotham', 'still', 'better', 'than', 'snapchat', 'update', 'we', 'all', 'have', 'difficulties', 'just', 'deal', 'with', 'it', 'dr', 'k', 'r', 'p', 'you', 'will', 'be', 'missed', 'cause', 'of', 'death', 'getting', 'roasted', 'by', 'fashion', 'police', 'when', 'you', 'bout', 'to', 'have', 'bad', 'day', 'but', 'then', 'single', 'ladies', 'on', 'radio', 'that', 'in', 'hd', 'can', 'you', 'spot', 'differences', 'when', 'your', 'both', 'your', 'parents', 'follow', 'you', 'on', 'twitter', 'much', 'sad', 'begging', 'to', 'great', 'friendship', 'its', 'in', 'adventure', 'time', 'everyone', 'pls', 'retweet', 'so', 'you', 'can', 'save', 'life', 'just', 'got', 'followers', 'watch', 'out', 'im', 'almost', 'caught', 'up', 'literally', 'going', 'to', 'see', 'this', 'just', 'for', 'jack', 'black', 'fiesty', 'my', 'favorite', 'part', 'of', 'eating', 'at', 'feeling', 'of', 'regret', 'when', 'youre', 'done', 'angry', 'at', 'everything', 'when', 'dj', 'plays', 'iggy', 'mrgiveyogirlbacks', 'video', 'missing', 'track', 'withdrawls', 'thehoodclipss', 'video', 'best', 'one', 'im', 'dead', 'they', 'will', 'think', 'its', 'fire', 'curtislepores', 'video', 'riskay', 'can', 'chaperone', 'that', 'momquotes', 'just', 'restarted', 'my', 'twitter', 'gonnatrend', 'lol', 'u', 'was', 'going', 'through', 'my', 'old', 'notebook', 'and', 'found', 'notes', 'from', 'youtube', 'videos', 'me', 'and', 'hunterh', 's', 'photo']\n",
      "Canonicalized Text:  [21, 42, 7, 55982, 199, 38, 47, 821, 4, 70, 122, 494, 3, 18, 5, 9, 262, 5, 476, 15, 36, 15246, 7836, 138, 361, 44, 5, 187, 25, 41, 5, 240, 26, 3740, 5006, 37, 5, 389, 2384, 8063, 55983, 55984, 112, 5, 12, 638, 118, 883, 5821, 7, 3757, 4, 2273, 170, 369, 3, 79, 118, 21, 235, 2744, 55985, 648, 76, 21, 255, 323, 1091, 24, 4743, 3773, 3, 6782, 37, 135, 24715, 15, 20, 1099, 1417, 511, 3, 32, 245, 1157, 1575, 73, 7, 487, 32, 121, 1809, 44, 414, 1126, 10, 24, 385, 22, 144, 113, 5, 44, 30, 476, 2368, 24554, 10, 24, 28, 219, 148, 68, 23, 16, 237, 37, 170, 4363, 9, 2253, 8, 24, 216, 19, 3, 3904, 44, 5, 158, 287, 1008, 2587, 15, 5002, 3, 794, 9, 11, 3963, 205, 9, 567, 38399, 299, 286, 441, 23, 211, 11292, 77, 70, 2633, 12, 55986, 7771, 1376, 476, 80, 828, 5680, 346, 4641, 11, 327, 762, 324, 82, 170, 11012, 16, 81, 39, 762, 2958, 87, 98, 94, 39110, 469, 48, 119, 15751, 3896, 14, 225, 596, 2580, 69, 55987, 18, 24008, 24, 2146, 102, 506, 38, 11, 327, 116, 7, 13, 466, 7, 5014, 4483, 24, 11539, 7, 16595, 24, 3584, 55988, 441, 16164, 16139, 1612, 36, 1116, 334, 11, 796, 388, 22763, 26, 39111, 13, 6999, 1152, 55989, 3709, 36, 368, 5, 775, 35, 5834, 96, 62, 537, 2339, 272, 26, 7908, 94, 60, 71, 109, 91, 361, 13388, 3, 312, 55990, 2109, 36, 296, 7, 799, 38, 12166, 3041, 155, 26, 2117, 921, 3, 23, 1911, 12, 117, 430, 53, 1895, 421, 35, 50, 12890, 12, 11797, 46, 9784, 2275, 30, 158, 9, 13, 395, 2858, 116, 39, 3476, 914, 9558, 6539, 273, 20, 4637, 9, 65, 2916, 313, 26, 1447, 324, 22, 45, 978, 236, 3, 195, 8, 76, 7, 2714, 380, 70, 1905, 12, 5, 11, 176, 8, 24, 385, 3, 53, 10, 2019, 652, 54, 14, 9360, 18, 91, 9, 4320, 12, 4632, 61, 25205, 106, 26, 2657, 18, 1296, 4305, 12, 569, 54, 7379, 97, 37, 3188, 3, 213, 569, 24, 658, 25205, 298, 60, 109, 63, 243, 604, 37, 60, 14, 161, 14048, 46, 674, 10, 2591, 68, 1335, 18379, 14, 60, 6040, 4, 609, 2591, 14, 636, 7, 257, 595, 123, 5332, 1306, 9, 4320, 21, 3815, 1543, 10, 141, 156, 21, 9, 80, 482, 8, 24, 465, 3, 62, 53, 110, 14815, 5047, 7, 1306, 25205, 126, 320, 49, 81, 59, 465, 1009, 7, 6693, 1144, 44, 3195, 5864, 24, 1675, 26, 7227, 18, 123, 2370, 10895, 20, 550, 109, 219, 39, 49, 1097, 799, 7, 886, 3071, 139, 23, 16, 565, 25205, 720, 1174, 43, 7, 25, 103, 27, 3, 3162, 29, 348, 9, 4385, 477, 24, 5082, 13, 126, 2989, 38, 99, 42, 20, 55, 2068, 36, 32, 3, 79, 7044, 4741, 13, 369, 3, 23, 23209, 1187, 44, 16560, 55991, 153, 55992, 1279, 608, 10, 13, 891, 1391, 138, 14995, 1285, 9, 3041, 518, 138, 1512, 7, 8345, 109, 5098, 12, 609, 339, 5, 4618, 508, 25, 556, 52, 4641, 109, 5408, 3, 92, 15, 867, 27, 75, 3, 23, 16, 156, 7683, 4, 3794, 12985, 981, 51, 202, 580, 203, 33, 35, 220, 747, 3, 33568, 37, 8, 24, 6319, 24, 456, 149, 10, 92, 87, 762, 11, 42, 6543, 39, 1085, 82, 10, 170, 77, 3259, 5, 158, 25206, 4, 70, 3717, 2869, 19, 146, 45, 50, 2047, 22, 192, 91, 692, 3, 383, 159, 307, 11, 341, 103, 1993, 6019, 333, 17104, 55993, 24, 1259, 35, 1297, 95, 398, 37, 5, 53, 15, 13, 15, 212, 25, 185, 7413, 145, 25, 198, 1054, 101, 35, 619, 1149, 39, 8, 29073, 605, 11, 2771, 27, 494, 72, 485, 35, 11, 2782, 430, 53, 18968, 6124, 120, 17, 7761, 136, 57, 25207, 14, 32, 10, 29, 79, 5507, 1028, 1457, 1329, 193, 15, 106, 26, 2733, 22380, 43, 5000, 13, 485, 35, 550, 5, 101, 35, 83, 3, 741, 39112, 55994, 22852, 5353, 2680, 104, 15, 9, 582, 177, 6782, 8, 24, 91, 77, 234, 298, 192, 363, 2635, 22082, 144, 33, 195, 8, 68, 1687, 1408, 19, 201, 64, 68, 34, 283, 12, 37397, 1766, 68, 196, 7191, 714, 3, 5785, 109, 18, 3, 535, 42, 197, 33, 19, 2081, 49, 7438, 38, 20, 1788, 89, 42, 12, 133, 61, 373, 4, 8, 3243, 11, 651, 712, 11, 17766, 764, 41, 435, 136, 12, 3839, 17, 350, 22, 12, 73, 428, 66, 70, 19, 2412, 3, 2610, 137, 2050, 30, 14, 32, 2215, 30, 32, 662, 30, 32, 50, 512, 939, 60, 55, 1464, 1636, 114, 3, 23, 2534, 3, 11, 221, 5823, 323, 1395, 100, 36, 3482, 99, 140, 75, 9, 103, 3, 84, 79, 324, 13808, 7, 1361, 13, 240, 23, 8550, 1291, 1714, 9998, 138, 295, 382, 35, 1471, 8, 22, 69, 121, 8, 68, 26, 3, 194, 1509, 12, 1769, 271, 963, 8, 2656, 16, 45, 19, 23, 4885, 138, 4248, 509, 9, 704, 406, 2368, 24554, 16, 81, 914, 13, 4519, 899, 34, 19, 632, 10, 3277, 36, 167, 55995, 568, 220, 2685, 3987, 56, 2272, 8, 24, 1922, 19, 81, 71, 839, 1286, 55996, 302, 865, 1238, 1616, 9, 11, 103, 914, 2739, 55997, 17, 1414, 4958, 62, 53, 21, 305, 2728, 51, 4, 62, 5, 158, 710, 5, 53, 95, 3020, 9, 950, 1195, 44, 430, 538, 55998, 55999, 71, 3, 2577, 48, 73, 307, 430, 53, 37, 25208, 544, 999, 18, 3, 1095, 11, 657, 4899, 14, 30, 32, 247, 348, 17, 1714, 2919, 32, 7, 3983, 150, 442, 56000, 58, 5538, 30415, 13, 49, 5, 53, 141, 57, 27, 544, 3041, 41, 14, 20, 5, 20452, 17, 4, 3526, 313, 26, 41, 121, 2069, 1265, 4309, 44, 31370, 337, 67, 3720, 1994, 36, 56001, 5517, 358, 7, 2877, 49, 216, 248, 194, 8, 333, 619, 194, 73, 9832, 3, 56002, 12, 1920, 67, 103, 242, 265, 7994, 37, 5, 78, 66, 7039, 8778, 209, 4725, 1244, 166, 266, 5, 923, 388, 29, 18, 560, 707, 22, 296, 51, 15155, 120, 132, 80, 2637, 1738, 706, 601, 39, 5, 22, 58, 3630, 50, 135, 3, 580, 15, 73, 56003, 15993, 2051, 15365, 19, 81, 71, 839, 462, 58, 235, 898, 648, 62, 3041, 38, 99, 174, 10, 152, 978, 59, 114, 42, 94, 481, 36, 368, 31, 104, 67, 2142, 167, 454, 5, 104, 37, 72, 402, 1517, 56004, 3, 3126, 3697, 20, 5, 187, 135, 150, 2598, 25, 5905, 11, 3674, 36, 8, 197, 44, 4371, 12601, 589, 8709, 25, 3, 47, 1572, 247, 740, 71, 3421, 4, 949, 527, 104, 11, 389, 619, 194, 11, 6162, 4509, 168, 5, 33, 35, 1035, 20, 118, 11, 538, 27, 104, 3490, 56005, 1501, 248, 20, 74, 9283, 1996, 705, 619, 79, 13, 9359, 1545, 12, 200, 11, 63, 327, 213, 1836, 811, 3777, 67, 15842, 3, 136, 6, 62, 320, 82, 18, 351, 5522, 17, 33, 135, 121, 8337, 21, 96, 579, 65, 56006, 46, 283, 3305, 48, 3, 1647, 25762, 12, 222, 1795, 5091, 842, 17, 3, 15, 170, 150, 715, 3153, 437, 15, 9, 627, 3148, 724, 221, 1684, 1009, 21953, 56007, 62, 11, 3064, 58, 549, 3, 7453, 56008, 129, 33, 79, 74, 434, 1958, 9, 13, 8536, 3404, 288, 58, 16, 1365, 3435, 3041, 27, 247, 490, 13, 108, 11, 4, 11, 6162, 20, 95, 5356, 56009, 24390, 11, 103, 56010, 1011, 704, 295, 56, 318, 43, 201, 3, 23, 209, 20, 135, 4, 114, 29, 296, 51, 19, 102, 499, 3, 6457, 8, 1572, 283, 26, 57, 4504, 163, 312, 2680, 8454, 703, 19, 2802, 9, 56011, 955, 170, 2585, 8, 767, 97, 4559, 1796, 7, 3020, 56012, 682, 780, 1555, 513, 151, 48, 58, 12985, 1232, 56013, 603, 848, 195, 13662, 2743, 12, 225, 1118, 671, 56014, 3, 212, 8, 242, 163, 657, 167, 6132, 45, 5, 715, 576, 62, 10707, 522, 97, 4, 527, 530, 12, 103, 44, 3260, 14, 16564, 101, 35, 75, 161, 2078, 39113, 30416, 1096, 39114, 26976, 6248, 1096, 10, 25004, 410, 59, 1135, 40, 8, 4036, 43, 24, 63, 531, 40, 212, 158, 312, 13, 126, 152, 230, 8, 544, 115, 539, 27, 121, 19, 32, 7, 95, 688, 1985, 14, 3910, 12685, 7413, 8289, 58, 16099, 56015, 1962, 295, 300, 188, 3, 151, 12, 477, 602, 20, 8, 14113, 7, 10778, 56016, 7, 1076, 3728, 15116, 191, 11, 511, 267, 65, 19, 564, 102, 1684, 447, 1376, 607, 15, 44, 79, 135, 3630, 6638, 11, 16564, 3260, 25, 2791, 590, 1482, 31, 83, 73, 2473, 37, 5, 78, 18, 246, 3, 34, 188, 3, 2139, 273, 36, 168, 456, 56017, 189, 3, 170, 54, 300, 160, 89, 2543, 3, 325, 264, 44, 25, 18615, 849, 5, 10, 4379, 6131, 51, 11, 14008, 313, 26, 156, 248, 7, 39115, 144, 69, 84, 305, 27, 76, 135, 315, 57, 402, 231, 265, 19, 23367, 219, 31, 1170, 583, 13, 4531, 5404, 4504, 70, 426, 11, 12096, 406, 7611, 21, 17, 63, 466, 7, 33399, 20, 2368, 8419, 78, 291, 37, 65, 8025, 46, 4145, 37, 79, 42, 59, 1566, 39116, 1684, 45, 535, 1129, 2145, 27, 66, 7, 614, 1721, 3, 4099, 5, 44, 4371, 29735, 5908, 5, 12, 30351, 330, 305, 36, 368, 239, 891, 1062, 3, 1801, 10, 615, 176, 27, 43, 222, 56018, 4676, 132, 25209, 2558, 10440, 5102, 13067, 2516, 1390, 2993, 222, 6005, 15, 152, 206, 37, 26, 8724, 46, 19, 7013, 126, 150, 111, 4324, 1612, 30, 32, 18, 11841, 27, 602, 20, 8, 1581, 546, 682, 780, 5, 45, 23, 1072, 378, 7, 650, 222, 7089, 56, 903, 1537, 44, 5, 2686, 3, 18, 156, 94, 22, 114, 750, 1798, 17, 2435, 10, 9, 4588, 33, 5, 1195, 3262, 44, 25, 244, 25, 637, 343, 5, 17, 214, 81, 404, 5707, 3, 131, 2014, 65, 9, 2355, 61, 170, 1224, 1036, 16, 5, 33, 737, 103, 27, 104, 449, 195, 48, 58, 434, 1470, 51, 278, 93, 3, 79, 13, 27, 12, 2714, 380, 56019, 11, 327, 116, 7, 968, 36, 361, 7, 2739, 44, 265, 226, 972, 36, 233, 44, 5123, 1716, 13058, 56020, 258, 1288, 1937, 56021, 56022, 258, 138, 42, 58, 353, 29, 45, 66, 65, 656, 56023, 258, 56024, 33, 32554, 10, 56025, 27, 37618, 11, 214, 56026, 85, 124, 21, 93, 191, 11, 245, 6364, 4, 310, 1956, 52, 777, 736, 15, 4, 56027, 24, 436]\n",
      "Max lengths of texts:  87013\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Text: \",post_train[88])\n",
    "print(\"Canonicalized Text: \", x_train[88])\n",
    "print(\"Max lengths of texts: \", max([len(x) for x in x_train+x_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_mbti(string):\n",
    "    label_bin = []\n",
    "    if string[0]==\"e\":\n",
    "        label_bin.append(0)\n",
    "    else:\n",
    "        label_bin.append(1)\n",
    "    if string[1]==\"n\":\n",
    "        label_bin.append(0)\n",
    "    else:\n",
    "        label_bin.append(1)\n",
    "    if string[2]==\"f\":\n",
    "        label_bin.append(0)\n",
    "    else:\n",
    "        label_bin.append(1)\n",
    "    if string[3]==\"j\":\n",
    "        label_bin.append(0)\n",
    "    else:\n",
    "        label_bin.append(1)\n",
    "        \n",
    "    assert len(label_bin) == 4,\"Not a valid MBTI type\"\n",
    "    return label_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infj\n",
      "[1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(label_train[0])\n",
    "print(binary_mbti(label_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 0, 0], [1, 1, 0, 0], [1, 0, 1, 0], [1, 0, 0, 0], [1, 1, 1, 1]]\n",
      "['infj' 'isfj' 'intj' 'infj' 'istp']\n"
     ]
    }
   ],
   "source": [
    "y_train_id = list(map(lambda x: binary_mbti(x), label_train))\n",
    "y_test_id = list(map(lambda x: binary_mbti(x), label_test))\n",
    "\n",
    "print(y_train_id[0:5])\n",
    "print(label_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the NBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_np_array(example_ids, max_len=100, pad_id=0):\n",
    "\n",
    "    arr = np.full([len(example_ids), max_len], pad_id, dtype=np.int32)\n",
    "    ns = np.zeros([len(example_ids)], dtype=np.int32)\n",
    "    for i, ids in enumerate(example_ids):\n",
    "        cpy_len = min(len(ids), max_len)\n",
    "        arr[i,:cpy_len] = ids[:cpy_len]\n",
    "        ns[i] = cpy_len\n",
    "    return arr, ns\n",
    "\n",
    "def tokenize_post(post_string):\n",
    "    return vocab_mbti.words_to_ids(post_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_padded_array(post_ids, targets, max_len=100, pad_id=0,\n",
    "                    root_only=False, df_idxs=None):\n",
    "\n",
    "    x, ns = pad_np_array(post_ids, max_len=max_len, pad_id=pad_id)\n",
    "    return x, ns, np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_1 = []\n",
    "for i in range(len(y_train_id)):\n",
    "    y_train_1.append(y_train_id[i][0])\n",
    "\n",
    "y_test_1 = []\n",
    "for i in range(len(y_test_id)):\n",
    "    y_test_1.append(y_test_id[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_ns, train_y = as_padded_array(x_train, y_train_1)\n",
    "test_x, test_ns, test_y = as_padded_array(x_test, y_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "len(y_train_1)\n",
    "print(len(y_test_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (68,981 words) written to '/tmp/tf_bow_sst_20190713-1909/metadata.tsv'\n",
      "Projector config written to /tmp/tf_bow_sst_20190713-1909/projector_config.pbtxt\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tf_bow_sst_20190713-1909', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x12d451828>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "To view training (once it starts), run:\n",
      "\n",
      "    tensorboard --logdir='/tmp/tf_bow_sst_20190713-1909' --port 6006\n",
      "\n",
      "Then in your browser, open: http://localhost:6006\n"
     ]
    }
   ],
   "source": [
    "#set up model using tf.estimator\n",
    "\n",
    "import MBTI_BOW_model; reload(MBTI_BOW_model)\n",
    "\n",
    "# Specify model hyperparameters as used by model\n",
    "model_params = dict(V=vocab_mbti.size, embed_dim=50, hidden_dims=[25], num_classes=2,\n",
    "                    encoder_type='bow',\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.01)\n",
    "\n",
    "checkpoint_dir = \"/tmp/tf_bow_sst_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "if os.path.isdir(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "\n",
    "vocab_mbti.write_projector_config(checkpoint_dir, \"Encoder/Embedding_Layer/W_embed\")\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn=MBTI_BOW_model.classifier_model_fn, \n",
    "                               params=model_params,\n",
    "                               model_dir=checkpoint_dir)\n",
    "print(\"\")\n",
    "print(\"To view training (once it starts), run:\\n\")\n",
    "print(\"    tensorboard --logdir='{:s}' --port 6006\".format(checkpoint_dir))\n",
    "print(\"\\nThen in your browser, open: http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tf_bow_sst_20190713-1909/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1737149, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 12 into /tmp/tf_bow_sst_20190713-1909/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.0998626.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20190713-1909/model.ckpt-12\n",
      "INFO:tensorflow:Saving checkpoints for 13 into /tmp/tf_bow_sst_20190713-1909/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.5724244, step = 13\n",
      "INFO:tensorflow:Saving checkpoints for 24 into /tmp/tf_bow_sst_20190713-1909/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.7723193.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20190713-1909/model.ckpt-24\n",
      "INFO:tensorflow:Saving checkpoints for 25 into /tmp/tf_bow_sst_20190713-1909/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.83176965, step = 25\n",
      "INFO:tensorflow:Saving checkpoints for 36 into /tmp/tf_bow_sst_20190713-1909/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.96400505.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20190713-1909/model.ckpt-36\n",
      "INFO:tensorflow:Saving checkpoints for 37 into /tmp/tf_bow_sst_20190713-1909/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.3199158, step = 37\n",
      "INFO:tensorflow:Saving checkpoints for 48 into /tmp/tf_bow_sst_20190713-1909/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.5250438.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20190713-1909/model.ckpt-48\n",
      "INFO:tensorflow:Saving checkpoints for 49 into /tmp/tf_bow_sst_20190713-1909/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.6857979, step = 49\n",
      "INFO:tensorflow:Saving checkpoints for 60 into /tmp/tf_bow_sst_20190713-1909/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.45986754.\n"
     ]
    }
   ],
   "source": [
    "#start training\n",
    "\n",
    "\n",
    "train_params = dict(batch_size=25, total_epochs=10, eval_every=2)\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "\n",
    "train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": train_x, \"ns\": train_ns}, y=train_y,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True, seed=42\n",
    "                 )\n",
    "\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": test_x, \"ns\": test_ns}, y=test_y,\n",
    "                    batch_size=25, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "  \n",
    "    model.train(input_fn=train_input_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2019-07-14-02:11:33\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20190713-1909/model.ckpt-60\n",
      "INFO:tensorflow:Finished evaluation at 2019-07-14-02:11:33\n",
      "INFO:tensorflow:Saving dict for global step 60: accuracy = 0.54285717, cross_entropy_loss = 1.2942256, global_step = 60, loss = 1.7624204\n",
      "Perplexity on test set: 3.65\n",
      "Accuracy on test set: 54.29%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.54285717,\n",
       " 'cross_entropy_loss': 1.2942256,\n",
       " 'loss': 1.7624204,\n",
       " 'global_step': 60}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluation on test data\n",
    "\n",
    "eval_metrics = model.evaluate(input_fn=test_input_fn, name=\"test\")  \n",
    "\n",
    "print (\"Perplexity on test set: {:.03}\".format(math.exp(eval_metrics['cross_entropy_loss'])))\n",
    "print(\"Accuracy on test set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2019-07-14-02:11:47\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20190713-1909/model.ckpt-60\n",
      "INFO:tensorflow:Finished evaluation at 2019-07-14-02:11:47\n",
      "INFO:tensorflow:Saving dict for global step 60: accuracy = 0.9352518, cross_entropy_loss = 0.14293686, global_step = 60, loss = 0.6450743\n",
      "Perplexity on train set: 1.15\n",
      "Accuracy on train set: 93.53%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9352518,\n",
       " 'cross_entropy_loss': 0.14293686,\n",
       " 'loss': 0.6450743,\n",
       " 'global_step': 60}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluation on training data\n",
    "\n",
    "eval_metrics = model.evaluate(input_fn=train_input_fn, name=\"train\")  \n",
    "\n",
    "print (\"Perplexity on train set: {:.03}\".format(math.exp(eval_metrics['cross_entropy_loss'])))\n",
    "print(\"Accuracy on train set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
