{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBTI Parallel Classification Model with Neural BOW (I/E Axis)\n",
    "\n",
    "First, load libraries and useful functions from class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from w266_common import patched_numpy_io\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Utils and Helper libraries\n",
    "# import nltk\n",
    "from w266_common import utils, vocabulary\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Specifications for Binary Classification NBOW for MBTI\n",
    "\n",
    "In this baseline, the task is to predict the first MBTI axis (I vs. E) given a text string. We will model after the A2 assignment, with Architecture and Parameters defined below.\n",
    "\n",
    "### Pre-Processing:\n",
    "* Minimial pre-processing, only separating punctuation from text and lower-case all text\n",
    "* Assigning words to numerical indices based on a fixed Vocab size, defined by word frequency in training set\n",
    "* Pulled out first axis of all target labels, assigned to binary (E = 0, I = 1)\n",
    "\n",
    "### Architecture:\n",
    "* Encoder: Bag of Words \n",
    "* Decoder: Softmax\n",
    "* Classification: Binary (2 MBTI types - I or E)\n",
    "\n",
    "### Parameters\n",
    "* Batch Size: 25 \n",
    "* Text length: 100\n",
    "* Vocabulary size (V): ~328K - removed stopwords\n",
    "* Embedding Size: 50\n",
    "* Hidden Dimensions: 25\n",
    "\n",
    "### Training:\n",
    "* Epochs = 10 \n",
    "* 80% train, 20% test\n",
    "* Loss: Sparse Softmax Cross Entropy \n",
    "* Optimizers: Adagrad Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus & Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/heatherkoo/Documents/MIDS/W210 Capstone/personality/Heather/code'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>tweets</th>\n",
       "      <th>prof_image_url</th>\n",
       "      <th>MBTI</th>\n",
       "      <th>orig_tweets</th>\n",
       "      <th>id_</th>\n",
       "      <th>retweets</th>\n",
       "      <th>fav</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BillTooke</td>\n",
       "      <td>[\"Yes. The College as the voting block for the...</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/8070634314...</td>\n",
       "      <td>ENFP</td>\n",
       "      <td>\"I have no shame and little guile when it come...</td>\n",
       "      <td>1123269839026642944</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dougie0216</td>\n",
       "      <td>['All Hail De Gendt! #TDF2019', 'He has a powe...</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1126179691...</td>\n",
       "      <td>INFP</td>\n",
       "      <td>\"@princessfemme Wow  another ENFP  what are th...</td>\n",
       "      <td>1123265408419733505</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kimbetech</td>\n",
       "      <td>['Get on LIVE now. 10:30pm EST https:// share....</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1101342873...</td>\n",
       "      <td>ENFP</td>\n",
       "      <td>\"I am an ENFP . We do not take kindly to being...</td>\n",
       "      <td>988128789577240576</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>honeyBklein</td>\n",
       "      <td>[\"I agree. It's a spiritual battle. In light o...</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1130313304...</td>\n",
       "      <td>ENFJ</td>\n",
       "      <td>\"@ABeardedPoet actually  I am an INFP  not INF...</td>\n",
       "      <td>1123206556181573632</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tanishatray4</td>\n",
       "      <td>['Are you left handed or right handed? — Left ...</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/9792931110...</td>\n",
       "      <td>ENFP</td>\n",
       "      <td>\"Ya Absolutely  that makes a lot of sense .The...</td>\n",
       "      <td>1123130637664309248</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_screen_name                                             tweets  \\\n",
       "0        BillTooke  [\"Yes. The College as the voting block for the...   \n",
       "1       dougie0216  ['All Hail De Gendt! #TDF2019', 'He has a powe...   \n",
       "2        kimbetech  ['Get on LIVE now. 10:30pm EST https:// share....   \n",
       "3      honeyBklein  [\"I agree. It's a spiritual battle. In light o...   \n",
       "4     tanishatray4  ['Are you left handed or right handed? — Left ...   \n",
       "\n",
       "                                      prof_image_url  MBTI  \\\n",
       "0  http://pbs.twimg.com/profile_images/8070634314...  ENFP   \n",
       "1  http://pbs.twimg.com/profile_images/1126179691...  INFP   \n",
       "2  http://pbs.twimg.com/profile_images/1101342873...  ENFP   \n",
       "3  http://pbs.twimg.com/profile_images/1130313304...  ENFJ   \n",
       "4  http://pbs.twimg.com/profile_images/9792931110...  ENFP   \n",
       "\n",
       "                                         orig_tweets                  id_  \\\n",
       "0  \"I have no shame and little guile when it come...  1123269839026642944   \n",
       "1  \"@princessfemme Wow  another ENFP  what are th...  1123265408419733505   \n",
       "2  \"I am an ENFP . We do not take kindly to being...   988128789577240576   \n",
       "3  \"@ABeardedPoet actually  I am an INFP  not INF...  1123206556181573632   \n",
       "4  \"Ya Absolutely  that makes a lot of sense .The...  1123130637664309248   \n",
       "\n",
       "   retweets  fav  \n",
       "0         0    1  \n",
       "1         0    1  \n",
       "2         0    1  \n",
       "3         0    2  \n",
       "4         0    0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "df = pd.read_csv('../personalities_large_no_duplicates_C.csv',index_col = 0)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ENFP',\n",
       " 'INFP',\n",
       " 'ENFJ',\n",
       " 'ENTP',\n",
       " 'ESFJ',\n",
       " 'INFJ',\n",
       " 'INTJ',\n",
       " 'INTP',\n",
       " 'ESTJ',\n",
       " 'ISTJ',\n",
       " 'ENTJ',\n",
       " 'ISFJ',\n",
       " 'ESFP',\n",
       " 'ISTP',\n",
       " 'ESTP',\n",
       " 'ISFP',\n",
       " 'enfp',\n",
       " 'infp',\n",
       " 'enfj',\n",
       " 'entp',\n",
       " 'esfj',\n",
       " 'infj',\n",
       " 'intj',\n",
       " 'intp',\n",
       " 'estj',\n",
       " 'istj',\n",
       " 'entj',\n",
       " 'isfj',\n",
       " 'esfp',\n",
       " 'istp',\n",
       " 'estp',\n",
       " 'isfp']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbti_types = df.MBTI.unique().tolist()\n",
    "\n",
    "mbti_types_low = df['MBTI'].str.lower().unique().tolist()\n",
    "mbti_types = mbti_types + mbti_types_low\n",
    "mbti_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove mbti types from tweets\n",
    "for x in mbti_types:\n",
    "    df['tweets'] = df['tweets'].str.replace(x,'mbti')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove nan values\n",
    "df = df.dropna(subset=['tweets'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize and clean sentence [\"Hello world.\"] into list of words [\"hello\",\"world\"]\n",
    "def clean_tokenize(sentence):\n",
    "    ignore_words = ['a', 'the', 'user', 'i','is']\n",
    "    sentence = re.sub(\"\\'\",\"\",sentence)\n",
    "    words = re.sub(\"[^\\w]|[0-9]\", \" \",  sentence).split() #removes all non-alphanumeric words, removes all numbers\n",
    "    words_cleaned = [w.lower() for w in words if w.lower() not in ignore_words]\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    #words_cleaned = ' '.join(word for word in words_cleaned)\n",
    "    \n",
    "    return words_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_tweets\"] = df[\"tweets\"].apply(clean_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yes',\n",
       " 'college',\n",
       " 'as',\n",
       " 'voting',\n",
       " 'block',\n",
       " 'for',\n",
       " 'pope',\n",
       " 'didnt',\n",
       " 'come',\n",
       " 'around',\n",
       " 'until',\n",
       " 'like',\n",
       " 'th',\n",
       " 'century',\n",
       " 'but',\n",
       " 'cardinals',\n",
       " 'were',\n",
       " 'important',\n",
       " 'romans',\n",
       " 'mean',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'arabia',\n",
       " 'was',\n",
       " 'folk',\n",
       " 'religion',\n",
       " 'mesopotamia',\n",
       " 'was',\n",
       " 'nestorian',\n",
       " 'egypt',\n",
       " 'monophysite',\n",
       " 'to',\n",
       " 'go',\n",
       " 'along',\n",
       " 'with',\n",
       " 'catholicism',\n",
       " 'but',\n",
       " 'arianism',\n",
       " 'hanging',\n",
       " 'around',\n",
       " 'in',\n",
       " 'north',\n",
       " 'africa',\n",
       " 'certainly',\n",
       " 'helped',\n",
       " 'smooth',\n",
       " 'pathway',\n",
       " 'for',\n",
       " 'islam',\n",
       " 'sorry',\n",
       " 'very',\n",
       " 'good',\n",
       " 'ancient',\n",
       " 'church',\n",
       " 'taught',\n",
       " 'islam',\n",
       " 'was',\n",
       " 'christian',\n",
       " 'heresy',\n",
       " 'dante',\n",
       " 'put',\n",
       " 'mohammad',\n",
       " 'in',\n",
       " 'with',\n",
       " 'schismatics',\n",
       " 'richard',\n",
       " 'harriss',\n",
       " 'kid',\n",
       " 'fabulous',\n",
       " 'as',\n",
       " 'stellan',\n",
       " 'of',\n",
       " 'course',\n",
       " 'piecemealed',\n",
       " 'chernobyl',\n",
       " 'through',\n",
       " 'youtube',\n",
       " 'and',\n",
       " 'fuck',\n",
       " 'it',\n",
       " 'magnificent',\n",
       " 'https',\n",
       " 'www',\n",
       " 'ncronline',\n",
       " 'org',\n",
       " 'blogs',\n",
       " 'ncr',\n",
       " 'toda',\n",
       " 'y',\n",
       " 'faith',\n",
       " 'facts',\n",
       " 'mike',\n",
       " 'pence',\n",
       " 'born',\n",
       " 'again',\n",
       " 'evangelical',\n",
       " 'catholic',\n",
       " 'he',\n",
       " 'had',\n",
       " 'whole',\n",
       " 'midwest',\n",
       " 'catholic',\n",
       " 'thing',\n",
       " 'down',\n",
       " 'pat',\n",
       " 'altar',\n",
       " 'boy',\n",
       " 'devout',\n",
       " 'family',\n",
       " 'he',\n",
       " 'still',\n",
       " 'calls',\n",
       " 'himself',\n",
       " 'catholic',\n",
       " 'evangelical',\n",
       " 'or',\n",
       " 'something',\n",
       " 'like',\n",
       " 'that',\n",
       " 'he',\n",
       " 'was',\n",
       " 'actually',\n",
       " 'once',\n",
       " 'catholic',\n",
       " 'bags',\n",
       " 'you',\n",
       " 'got',\n",
       " 'paul',\n",
       " 'rudd',\n",
       " 'vibe',\n",
       " 'going',\n",
       " 'on',\n",
       " 'man',\n",
       " 'minor',\n",
       " 'pedantic',\n",
       " 'point',\n",
       " 'its',\n",
       " 'dr',\n",
       " 'x',\n",
       " 'yz',\n",
       " 'or',\n",
       " 'x',\n",
       " 'yz',\n",
       " 'ph',\n",
       " 'd',\n",
       " 'but',\n",
       " 'combining',\n",
       " 'redundant',\n",
       " 'yay',\n",
       " 'just',\n",
       " 'one',\n",
       " 'beer',\n",
       " 'like',\n",
       " 'one',\n",
       " 'lay',\n",
       " 's',\n",
       " 'potato',\n",
       " 'chip',\n",
       " 'betcha',\n",
       " 'can',\n",
       " 't',\n",
       " 'eat',\n",
       " 'just',\n",
       " 'one',\n",
       " 'ain',\n",
       " 't',\n",
       " 'gonna',\n",
       " 'fuckin',\n",
       " 'happen',\n",
       " 'since',\n",
       " 'youre',\n",
       " 'at',\n",
       " 'frannies',\n",
       " 'other',\n",
       " 'good',\n",
       " 'places',\n",
       " 'out',\n",
       " 'your',\n",
       " 'way',\n",
       " 'fr',\n",
       " 'would',\n",
       " 'be',\n",
       " 'dannys',\n",
       " 'south',\n",
       " 'big',\n",
       " 'tree',\n",
       " 'youll',\n",
       " 'see',\n",
       " 'bills',\n",
       " 'players',\n",
       " 'at',\n",
       " 'big',\n",
       " 'tree',\n",
       " 'canada',\n",
       " 'wonderful',\n",
       " 'has',\n",
       " 'potential',\n",
       " 'to',\n",
       " 'be',\n",
       " 'even',\n",
       " 'better',\n",
       " 'live',\n",
       " 'in',\n",
       " 'buffalo',\n",
       " 'grew',\n",
       " 'uo',\n",
       " 'throughout',\n",
       " 'upstate',\n",
       " 'so',\n",
       " 'canadas',\n",
       " 'presence',\n",
       " 'has',\n",
       " 'always',\n",
       " 'haunted',\n",
       " 'me',\n",
       " 'best',\n",
       " 'mexican',\n",
       " 'in',\n",
       " 'buffalo',\n",
       " 'imo',\n",
       " 'el',\n",
       " 'canelo',\n",
       " 'us',\n",
       " 'brave',\n",
       " 'new',\n",
       " 'world',\n",
       " 'canada',\n",
       " 'delicious',\n",
       " 'would',\n",
       " 'be',\n",
       " 'ok',\n",
       " 'but',\n",
       " 'that',\n",
       " 'also',\n",
       " 'means',\n",
       " 'more',\n",
       " 'meaningless',\n",
       " 'games',\n",
       " 'in',\n",
       " 'lost',\n",
       " 'season',\n",
       " 'but',\n",
       " 'would',\n",
       " 'take',\n",
       " 'my',\n",
       " 'thing',\n",
       " 'starting',\n",
       " 'season',\n",
       " 'earlier',\n",
       " 'start',\n",
       " 'in',\n",
       " 'august',\n",
       " 'end',\n",
       " 'before',\n",
       " 'christmas',\n",
       " 'well',\n",
       " 'wny',\n",
       " 'always',\n",
       " 'looks',\n",
       " 'forward',\n",
       " 'to',\n",
       " 'you',\n",
       " 'being',\n",
       " 'her',\n",
       " 'sir',\n",
       " 'then',\n",
       " 'mid',\n",
       " 's',\n",
       " 'next',\n",
       " 'week',\n",
       " 'with',\n",
       " 'some',\n",
       " 'tropical',\n",
       " 'moisture',\n",
       " 'cc',\n",
       " 'wnyweather',\n",
       " 'kevinbuffalo',\n",
       " 'today',\n",
       " 'perfect',\n",
       " 'sunny',\n",
       " 'with',\n",
       " 'moderate',\n",
       " 'humidity',\n",
       " 'tomorrow',\n",
       " 'and',\n",
       " 'sunny',\n",
       " 'it',\n",
       " 'will',\n",
       " 'get',\n",
       " 'sweltering',\n",
       " 'here',\n",
       " 'next',\n",
       " 'week',\n",
       " 'thoughtfulness',\n",
       " 'pic',\n",
       " 'twitter',\n",
       " 'com',\n",
       " 'qzxuuxeyym',\n",
       " 'cheers',\n",
       " 'might',\n",
       " 'be',\n",
       " 'best',\n",
       " 'sitcom',\n",
       " 'ever',\n",
       " 'also',\n",
       " 'binge',\n",
       " 'watch',\n",
       " 'taxi',\n",
       " 'deus',\n",
       " 'caritas',\n",
       " 'est',\n",
       " 'it',\n",
       " 'looks',\n",
       " 'like',\n",
       " 'middle',\n",
       " 'aged',\n",
       " 'guy',\n",
       " 'named',\n",
       " 'eugene',\n",
       " 'you',\n",
       " 'honestly',\n",
       " 'inspire',\n",
       " 'me',\n",
       " 'sarah',\n",
       " 'youre',\n",
       " 'like',\n",
       " 'modern',\n",
       " 'julian',\n",
       " 'of',\n",
       " 'norwich',\n",
       " 'god',\n",
       " 'bless',\n",
       " 'its',\n",
       " 'so',\n",
       " 'easy',\n",
       " 'being',\n",
       " 'born',\n",
       " 'in',\n",
       " 'nobody',\n",
       " 'expects',\n",
       " 'any',\n",
       " 'of',\n",
       " 'this',\n",
       " 'from',\n",
       " 'me',\n",
       " 'my',\n",
       " 'son',\n",
       " 'will',\n",
       " 'be',\n",
       " 'very',\n",
       " 'soon',\n",
       " 'still',\n",
       " 'call',\n",
       " 'him',\n",
       " 'baby',\n",
       " 'its',\n",
       " 'just',\n",
       " 'affection',\n",
       " 'endearment',\n",
       " 'amen',\n",
       " 'sister',\n",
       " 'next',\n",
       " 'week',\n",
       " 'fascinates',\n",
       " 'me',\n",
       " 'very',\n",
       " 'subtropical',\n",
       " 'to',\n",
       " 'tropical',\n",
       " 'pattern',\n",
       " 'setting',\n",
       " 'up',\n",
       " 'here',\n",
       " 'especially',\n",
       " 'if',\n",
       " 'we',\n",
       " 'get',\n",
       " 'barry',\n",
       " 'remnants',\n",
       " 'hot',\n",
       " 'humid',\n",
       " 'and',\n",
       " 'wet',\n",
       " 'hamster',\n",
       " 'becomes',\n",
       " 'hampster',\n",
       " 'because',\n",
       " 'subjectivity',\n",
       " 'experience',\n",
       " 'desire',\n",
       " 'emotion',\n",
       " 'dictate',\n",
       " 'that',\n",
       " 'reaction',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'religious',\n",
       " 'in',\n",
       " 'negative',\n",
       " 'use',\n",
       " 'of',\n",
       " 'word',\n",
       " 'father',\n",
       " 'one',\n",
       " 'her',\n",
       " 'beliefs',\n",
       " 'were',\n",
       " 'trampled',\n",
       " 'on',\n",
       " 'here',\n",
       " 'ill',\n",
       " 'make',\n",
       " 'old',\n",
       " 'slippery',\n",
       " 'slope',\n",
       " 'argument',\n",
       " 'here',\n",
       " 'when',\n",
       " 'nothing',\n",
       " 'objectively',\n",
       " 'true',\n",
       " 'then',\n",
       " 'its',\n",
       " 'inevitable',\n",
       " 'that',\n",
       " 'these',\n",
       " 'situations',\n",
       " 'occur',\n",
       " 'youll',\n",
       " 'get',\n",
       " 'great',\n",
       " 'guy',\n",
       " 'https',\n",
       " 'youtu',\n",
       " 'be',\n",
       " 'ulvoew',\n",
       " 'l',\n",
       " 'rs',\n",
       " 'my',\n",
       " 'best',\n",
       " 'side',\n",
       " 'above',\n",
       " 'goingblondzo',\n",
       " 'pic',\n",
       " 'twitter',\n",
       " 'com',\n",
       " 's',\n",
       " 'vpkz',\n",
       " 'bj',\n",
       " 'as',\n",
       " 'kids',\n",
       " 'say',\n",
       " 'go',\n",
       " 'eat',\n",
       " 'fam',\n",
       " 'hell',\n",
       " 'yeah',\n",
       " 'michelle',\n",
       " 'https',\n",
       " 'youtu',\n",
       " 'be',\n",
       " 'zbmcbkalvkm',\n",
       " 'goodtweet_man',\n",
       " 'brian',\n",
       " 'johnsons',\n",
       " 'travelogue',\n",
       " 'show',\n",
       " 'was',\n",
       " 'good',\n",
       " 'foucault',\n",
       " 'therealoj',\n",
       " 'living',\n",
       " 'here',\n",
       " 'in',\n",
       " 'buffalo',\n",
       " 'at',\n",
       " 'same',\n",
       " 'time',\n",
       " 'fascinating',\n",
       " 'confluence',\n",
       " 'of',\n",
       " 'energies',\n",
       " 'foucault',\n",
       " 'taught',\n",
       " 'at',\n",
       " 'ubuffalo',\n",
       " 'in',\n",
       " 'early',\n",
       " 's',\n",
       " 'crashed',\n",
       " 'on',\n",
       " 'other',\n",
       " 'professors',\n",
       " 'couches',\n",
       " 'gorgeous',\n",
       " 'good',\n",
       " 'idea',\n",
       " 'my',\n",
       " 'thing',\n",
       " 'start',\n",
       " 'regular',\n",
       " 'season',\n",
       " 'weeks',\n",
       " 'before',\n",
       " 'labor',\n",
       " 'day',\n",
       " 'everybody',\n",
       " 'gets',\n",
       " 'labor',\n",
       " 'day',\n",
       " 'weekend',\n",
       " 'as',\n",
       " 'bye',\n",
       " 'week',\n",
       " 'so',\n",
       " 'college',\n",
       " 'football',\n",
       " 'can',\n",
       " 'open',\n",
       " 'then',\n",
       " 'usual',\n",
       " 'game',\n",
       " 'sked',\n",
       " 'afterward',\n",
       " 'weeks',\n",
       " 'b',\n",
       " 'would',\n",
       " 'be',\n",
       " 'all',\n",
       " 'interconference',\n",
       " 'matchups',\n",
       " 'he',\n",
       " 'was',\n",
       " 'here',\n",
       " 'in',\n",
       " 'buffalo',\n",
       " 'buffalo',\n",
       " 'even',\n",
       " 'worse',\n",
       " 'mike',\n",
       " 'lakeshadow',\n",
       " 'sammy',\n",
       " 'watkins',\n",
       " 'scorched',\n",
       " 'earthed',\n",
       " 'revis',\n",
       " 'island',\n",
       " 'downright',\n",
       " 'chilly',\n",
       " 'in',\n",
       " 'wny',\n",
       " 'today',\n",
       " 'degrees',\n",
       " 'now',\n",
       " 'locked',\n",
       " 'out',\n",
       " 'of',\n",
       " 'my',\n",
       " 'place',\n",
       " 'have',\n",
       " 'so',\n",
       " 'many',\n",
       " 'things',\n",
       " 'to',\n",
       " 'say',\n",
       " 'well',\n",
       " 'nfl',\n",
       " 'probably',\n",
       " 'having',\n",
       " 'signed',\n",
       " 'cba',\n",
       " 'very',\n",
       " 'soon',\n",
       " 'enormous',\n",
       " 'news',\n",
       " 'josephrehrauer',\n",
       " 'wojbomb',\n",
       " 'https',\n",
       " 'twitter',\n",
       " 'com',\n",
       " 'wojespn',\n",
       " 'status',\n",
       " 's',\n",
       " 'alexisscriven',\n",
       " 'https',\n",
       " 'youtu',\n",
       " 'be',\n",
       " 'bmq_x',\n",
       " 'o',\n",
       " 'umg',\n",
       " 'he',\n",
       " 'was',\n",
       " 'an',\n",
       " 'avowed',\n",
       " 'marxist',\n",
       " 'https',\n",
       " 'en',\n",
       " 'wikipedia',\n",
       " 'org',\n",
       " 'wiki',\n",
       " 'the_gospe',\n",
       " 'l_according_to_st',\n",
       " '_matthew_',\n",
       " 'film',\n",
       " 'though',\n",
       " 'its',\n",
       " 'literally',\n",
       " 'religion',\n",
       " 'than',\n",
       " 'about',\n",
       " 'milieu',\n",
       " 'of',\n",
       " 'being',\n",
       " 'religious',\n",
       " 'pasolinis',\n",
       " 'gospel',\n",
       " 'according',\n",
       " 'to',\n",
       " 'st',\n",
       " 'matthew',\n",
       " 'went',\n",
       " 'from',\n",
       " 'completely',\n",
       " 'single',\n",
       " 'and',\n",
       " 'not',\n",
       " 'knowing',\n",
       " 'my',\n",
       " 'wife',\n",
       " 'at',\n",
       " 'all',\n",
       " 'on',\n",
       " 'my',\n",
       " 'th',\n",
       " 'birthday',\n",
       " 'to',\n",
       " 'married',\n",
       " 'to',\n",
       " 'her',\n",
       " 'within',\n",
       " 'three',\n",
       " 'months',\n",
       " 'after',\n",
       " 'my',\n",
       " 'th',\n",
       " 'birthday',\n",
       " 'https',\n",
       " 'en',\n",
       " 'wikipedia',\n",
       " 'org',\n",
       " 'wiki',\n",
       " 'montreal',\n",
       " 'style_smoked_meat',\n",
       " 'https',\n",
       " 'en',\n",
       " 'wikipedia',\n",
       " 'org',\n",
       " 'wiki',\n",
       " 'montreal',\n",
       " 'style_bagel',\n",
       " 'montreal',\n",
       " 'has',\n",
       " 'best',\n",
       " 'bagels',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'had',\n",
       " 'prayers',\n",
       " 'wow',\n",
       " 'hello',\n",
       " 'drums',\n",
       " 'american',\n",
       " 'beauty',\n",
       " 'and',\n",
       " 'ghost',\n",
       " 'world',\n",
       " 'couldnt',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'who',\n",
       " 'would',\n",
       " 'play',\n",
       " 'me',\n",
       " 'no',\n",
       " 'idea',\n",
       " 'thora',\n",
       " 'birch',\n",
       " 'most',\n",
       " 'definitely',\n",
       " 'https',\n",
       " 'www',\n",
       " 'wgrz',\n",
       " 'com',\n",
       " 'day',\n",
       " 'looks',\n",
       " 'steamy',\n",
       " 'love',\n",
       " 'ugcc',\n",
       " 'ruthenian',\n",
       " 'church',\n",
       " 'so',\n",
       " 'much',\n",
       " 'byzantine',\n",
       " 'christianity',\n",
       " 'saved',\n",
       " 'my',\n",
       " 'faith',\n",
       " 'in',\n",
       " 'tough',\n",
       " 'time',\n",
       " 'in',\n",
       " 'late',\n",
       " 's',\n",
       " 'early',\n",
       " 's',\n",
       " 'hope',\n",
       " 'they',\n",
       " 'go',\n",
       " 'in',\n",
       " 'division',\n",
       " 'with',\n",
       " 'sweeps',\n",
       " 'over',\n",
       " 'jets',\n",
       " 'and',\n",
       " 'patriots',\n",
       " 'as',\n",
       " 'much',\n",
       " 'as',\n",
       " 'hate',\n",
       " 'miami',\n",
       " 'dolphins',\n",
       " 'love',\n",
       " 'fitz',\n",
       " 'as',\n",
       " 'much',\n",
       " 'so',\n",
       " 'this',\n",
       " 'an',\n",
       " 'incredibly',\n",
       " 'tough',\n",
       " 'year',\n",
       " 'hope',\n",
       " 'they',\n",
       " 'go',\n",
       " 'circle',\n",
       " 'of',\n",
       " 'life',\n",
       " 'best',\n",
       " 'eltonofficial',\n",
       " 'lion',\n",
       " 'king',\n",
       " 'song',\n",
       " 'come',\n",
       " 'to',\n",
       " 'buffalo',\n",
       " 'we',\n",
       " 'can',\n",
       " 'show',\n",
       " 'you',\n",
       " 'falls',\n",
       " 'https',\n",
       " 'youtu',\n",
       " 'be',\n",
       " '_t',\n",
       " 'qiqo',\n",
       " 'qkg',\n",
       " 'though',\n",
       " 'imbruglia',\n",
       " 'covered',\n",
       " 'it',\n",
       " 'its',\n",
       " 'actually',\n",
       " 'few',\n",
       " 'years',\n",
       " 'older',\n",
       " 'song',\n",
       " 'that',\n",
       " 'came',\n",
       " 'out',\n",
       " 'when',\n",
       " 'was',\n",
       " 'about',\n",
       " 'stuffed',\n",
       " 'bears',\n",
       " 'guess',\n",
       " 'but',\n",
       " 'would',\n",
       " 'say',\n",
       " 'best',\n",
       " 'movies',\n",
       " 'from',\n",
       " 's',\n",
       " 'are',\n",
       " 'better',\n",
       " 'thsn',\n",
       " 'best',\n",
       " 'from',\n",
       " 'today',\n",
       " 'conversation',\n",
       " 'would',\n",
       " 'win',\n",
       " 'best',\n",
       " 'picture',\n",
       " 'this',\n",
       " 'year',\n",
       " 'its',\n",
       " 'super',\n",
       " 'hazy',\n",
       " 'here',\n",
       " 'in',\n",
       " 'buffalo',\n",
       " 'with',\n",
       " 'wildfire',\n",
       " 'smoke',\n",
       " 'this',\n",
       " 'happened',\n",
       " 'only',\n",
       " 'twice',\n",
       " 'surviving',\n",
       " 'with',\n",
       " 'no',\n",
       " 'barrel',\n",
       " 'or',\n",
       " 'restraint',\n",
       " 'in',\n",
       " 'history',\n",
       " 'on',\n",
       " 'anniversary',\n",
       " 'of',\n",
       " 'it',\n",
       " 'happening',\n",
       " 'other',\n",
       " 'time',\n",
       " 'no',\n",
       " 'less',\n",
       " 'oh',\n",
       " 'we',\n",
       " 'hate',\n",
       " 'it',\n",
       " 'too',\n",
       " 'but',\n",
       " 'you',\n",
       " 'get',\n",
       " 'used',\n",
       " 'to',\n",
       " 'it',\n",
       " 'was',\n",
       " 'just',\n",
       " 'commenting',\n",
       " 'to',\n",
       " 'someone',\n",
       " 'at',\n",
       " 'falls',\n",
       " 'sunday',\n",
       " 'about',\n",
       " 'going',\n",
       " 'over',\n",
       " 'surviving',\n",
       " 'https',\n",
       " 'twitter',\n",
       " 'com',\n",
       " 'davemckinley',\n",
       " 'status',\n",
       " 'in',\n",
       " 'western',\n",
       " 'ny',\n",
       " 'flip',\n",
       " 'winter',\n",
       " 'and',\n",
       " 'summer',\n",
       " 'wow',\n",
       " 'you',\n",
       " 'are',\n",
       " 'young',\n",
       " 'early',\n",
       " 'tough',\n",
       " 'to',\n",
       " 'say',\n",
       " 'but',\n",
       " 'challenger',\n",
       " 'disaster',\n",
       " 'was',\n",
       " 'most',\n",
       " 'memorable',\n",
       " 'for',\n",
       " 'me',\n",
       " 'was',\n",
       " 'and',\n",
       " 'half',\n",
       " 'and',\n",
       " 'in',\n",
       " 'st',\n",
       " 'grade',\n",
       " 'boston',\n",
       " 'nyc',\n",
       " 'charleston',\n",
       " 'sc',\n",
       " 'and',\n",
       " 'england',\n",
       " 'all',\n",
       " 'drop',\n",
       " 'their',\n",
       " 'rs',\n",
       " 'all',\n",
       " 'over',\n",
       " 'place',\n",
       " 'newfies',\n",
       " 'and',\n",
       " 'hoi',\n",
       " 'toiders',\n",
       " 'sound',\n",
       " 'very',\n",
       " 'british',\n",
       " 'east',\n",
       " 'coast',\n",
       " 'americans',\n",
       " 'sound',\n",
       " 'quite',\n",
       " 'british',\n",
       " 'with',\n",
       " 'their',\n",
       " 'non',\n",
       " 'rhotic',\n",
       " 'accents',\n",
       " 'of',\n",
       " 'course',\n",
       " 'those',\n",
       " 'who',\n",
       " 'affect',\n",
       " 'transatlantic',\n",
       " 'midatlantic',\n",
       " 'yalie',\n",
       " 'accent',\n",
       " 'sound',\n",
       " 'quite',\n",
       " 'british',\n",
       " 'have',\n",
       " 'an',\n",
       " 'inland',\n",
       " 'north',\n",
       " 'accent',\n",
       " 'very',\n",
       " 'rhotic',\n",
       " 'so',\n",
       " 'dont',\n",
       " 'sound',\n",
       " 'british',\n",
       " 'yes',\n",
       " 'but',\n",
       " 'in',\n",
       " 'this',\n",
       " 'case',\n",
       " 'its',\n",
       " 'from',\n",
       " 'eminem',\n",
       " 'song',\n",
       " 'about',\n",
       " 'obsessive',\n",
       " 'fandom',\n",
       " 'https',\n",
       " 'youtu',\n",
       " 'be',\n",
       " 'ce',\n",
       " 'wxdqdov',\n",
       " 'didnt',\n",
       " 'do',\n",
       " 'it',\n",
       " 'dancers',\n",
       " 'theyre',\n",
       " 'especially',\n",
       " 'neat',\n",
       " 'in',\n",
       " 'autumn',\n",
       " 'over',\n",
       " 'lake',\n",
       " 'ontario',\n",
       " 'with',\n",
       " 'temperature',\n",
       " 'difference',\n",
       " 'one',\n",
       " 'time',\n",
       " 'driving',\n",
       " 'through',\n",
       " 'wilson',\n",
       " 'toronto',\n",
       " 'skyline',\n",
       " 'looked',\n",
       " 'no',\n",
       " 'more',\n",
       " 'than',\n",
       " 'couple',\n",
       " 'miles',\n",
       " 'away',\n",
       " 'could',\n",
       " 'make',\n",
       " 'out',\n",
       " 'buildings',\n",
       " 'very',\n",
       " 'clearly',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"clean_tweets\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize and clean sentence [\"Hello world.\"] into list of words [\"hello\",\"world\"]\n",
    "def clean(sentence):\n",
    "    ignore_words = ['a']\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() #nltk.word_tokenize(sentence)\n",
    "    words_cleaned = [w.lower() for w in words if w not in ignore_words]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words_cleaned = [w for w in words_cleaned if not w in stop_words]\n",
    "    words_string = ''.join(words_cleaned)\n",
    "    return words_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split posts per users into separate sentences\n",
    "#post = []\n",
    "#utype = []\n",
    "#user = []\n",
    "\n",
    "#for index, row in df.iterrows():\n",
    "#    posts = row['posts'].split('|||')\n",
    "#    posts_clean = []\n",
    "##    for sentence in posts:\n",
    "#       posts_clean.append(clean(sentence))\n",
    "#    post.extend(posts_clean)\n",
    "#     post.extend(posts)\n",
    "#    utype.extend([row['type'] for i in range(len(posts))])\n",
    "#    user.extend([index for i in range(len(posts))])\n",
    "    \n",
    "#short_posts = pd.DataFrame({\"user\": user,\"type\": utype,\"post\": post})\n",
    "#print(short_posts.shape)\n",
    "#short_posts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBIT posts []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data: 80% train, 20% test\n",
    "post_train, post_test, label_train, label_test = train_test_split(np.array(df['clean_tweets']), \n",
    "                                                    np.array(df['MBTI']), \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=88)\n",
    "\n",
    "\n",
    "print(\"MBIT posts\", post_train[2])\n",
    "print('')\n",
    "#print(\"MBTI Labels: \",label_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391714"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a vocabulary (V size is defaulted to full text) for train corpus\n",
    "\n",
    "vocab_train = []\n",
    "for i in range(len(post_train)):\n",
    "    for word in post_train[i]:\n",
    "        vocab_train.append(word)\n",
    "\n",
    "\n",
    "vocab_mbti = vocabulary.Vocabulary((w for w in vocab_train))\n",
    "vocab_mbti.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (391,714 words) written to 'vocab.csv'\n"
     ]
    }
   ],
   "source": [
    "vocab_mbti.write_flat_file('vocab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to']\n"
     ]
    }
   ],
   "source": [
    "print (vocab_mbti.ids_to_words([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97, 33, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "print (vocab_mbti.words_to_ids(['got','what','and','the']))\n",
    "#print (vocab_mbti.ids_to_words([202, 147565, 317206, 159348])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_mbti.words_to_ids(post_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and canonicalize train and test sets\n",
    "x_train = []\n",
    "for i in range(len(post_train)):\n",
    "    x_train.append(vocab_mbti.words_to_ids(post_train[i]))\n",
    "\n",
    "x_test = []\n",
    "for i in range(len(post_test)):\n",
    "    x_test.append(vocab_mbti.words_to_ids(post_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  ['whaaaat', 'buffy', 'friends', 'source', 'https', 'www', 'cracked', 'com', 'pictofacts', 'theres', 'reason', 'these', 'movie', 'tv', 'scenes', 'look', 'familiar', 'pic', 'twitter', 'com', 'norfv', 'tn', 'use', 'filters', 'to', 'push', 'contrast', 'to', 'see', 'shapes', 'and', 'planes', 'better', 'pic', 'twitter', 'com', 'z', 'ygsmyiiw', 'and', 'sorry', 'for', 'my', 'part', 'its', 'elegant', 'and', 'simple', 'to', 'have', 'one', 'rule', 'for', 'all', 'genders', 'but', 'could', 'have', 'suggested', 'it', 'better', 'fortunately', 'phones', 'now', 'have', 'voice', 'activated', 'cameras', 'makes', 'getting', 'hands', 'waaaay', 'easier', 'reminder', 'that', 'pro', 'artists', 'use', 'reference', 'it', 'never', 'cheating', 'always', 'lends', 'hand', 'pic', 'twitter', 'com', 'szkjkveur', 'by', 'replier', 'you', 'mean', 'me', 'nah', 'it', 'was', 'wrong', 'how', 'went', 'about', 'suggesting', 'it', 'posted', 'an', 'apology', 'link', 'sould', 'be', 'below', 'somewhere', 'like', 'am', 'gay', 'post', 'stuff', 'just', 'about', 'digital', 'artwork', 'still', 'get', 'slapped', 'with', 'adult', 'content', 'because', 'am', 'openly', 'gay', 'think', 'they', 'meant', 'it', 'in', 'way', 'lesbian', 'content', 'labelled', 'as', 'adult', 'by', 'places', 'like', 'youtube', 'because', 'they', 'think', 'anything', 'homosexual', 'equates', 'to', 'sexual', 'content', 'that', 'minors', 'shouldn', 't', 'be', 'exposed', 'to', 'mean', 'being', 'proud', 'about', 'being', 'in', 'relationship', 'isn', 't', 'sexual', 'in', 'that', 'regard', 'oh', 'yes', 'but', 'just', 'starting', 'point', 'https', 'www', 'taste', 'com', 'au', 'recipes', 'turkis', 'h', 'delight', 'creme', 'brulee', 'b', 'b', 'f', 'ea', 'd', 'e', 'd', 'have', 'some', 'fantasyart', 'commission', 'slots', 'open', 'shortly', 'so', 'its', 'good', 'time', 'to', 'mention', 'also', 'do', 'large', 'pencil', 'and', 'ink', 'originals', 'and', 'illustrations', 'for', 'ttrpgs', 'pic', 'twitter', 'com', 'ztb', 'rlffc', 'you', 'can', 'combine', 'these', 'gamedevs', 'if', 'don', 't', 'see', 'swathe', 'of', 'mass', 'rush', 'area', 'games', 'coming', 'out', 'after', 'this', 'then', 'are', 'you', 'even', 'trying', 'pic', 'twitter', 'com', 'ftjqbo', 'cyw', 'paul', 'taught', 'me', 'more', 'about', 'rigging', 'and', 'prepping', 'characters', 'for', 'rendering', 'on', 'one', 'day', 'whilst', 'was', 'drunk', 'and', 'jetlagged', 'than', 'picked', 'up', 'in', 'years', 'book', 'him', 'as', 'speaker', 'you', 'will', 'not', 'regret', 'it', 'https', 'youtu', 'be', 'sncxe', 'yrhw', 'via', 'youtube', 'seriously', 'this', 'guy', 'https', 'youtu', 'be', 'iqyjxmbza', 'y', 'dm', 'requested', 'link', 'so', 'they', 'could', 'bookmark', 'my', 'optimizing', 'thread', 'https', 'twitter', 'com', 'delaneykingrox', 'status', 'absolutely', 'thall', 'are', 'genji', 'mains', 'like', 'it', 'you', 'know', 'those', 'shattering', 'obsidian', 'knights', 'from', 'snow', 'white', 'and', 'huntsman', 'guy', 'who', 'rigged', 'warcraft', 'cinematics', 'did', 'them', 'paul', 'just', 'an', 'awesome', 'guy', 'to', 'work', 'with', 'and', 'heartily', 'recommend', 'hiring', 'him', 'check', 'out', 'his', 'site', 'http', 'www', 'paulhormis', 'com', 'homewp', 'no', 'am', 'comfortable', 'with', 'choices', 'made', 'blocking', 'people', 'based', 'on', 'their', 'behavior', 'towards', 'me', 'misgendering', 'for', 'example', 'what', 'would', 'you', 'like', 'see', 'make', 'comeback', 'thee', 'and', 'thou', 'are', 'kinda', 'cool', 'though', 'we', 'all', 'are', 'and', 'thats', 'how', 'it', 'probably', 'gonna', 'play', 'out', 'how', 'long', 'did', 'it', 'take', 'you', 'to', 'adapt', 'to', 'they', 'what', 'on', 'lot', 'of', 'people', 'are', 'stumbling', 'over', 'they', 'are', 'as', 'singular', 'in', 'clause', 'if', 'you', 'want', 'to', 'talk', 'grammar', 'completely', 'separate', 'from', 'gender', 'have', 'look', 'at', 'style', 'guides', 'and', 'wiki', 'entries', 'to', 'understand', 'rule', 'we', 'instinctively', 'know', 'and', 'use', 'and', 'yeah', 'they', 'sounds', 'weird', 'but', 'if', 'you', 'think', 'about', 'mechanics', 'of', 'grammar', 'behind', 'that', 'feeling', 'weird', 'our', 'ingrained', 'rules', 'of', 'grammar', 'tapping', 'us', 'on', 'shoulder', 'thanks', 'why', 'am', 'suggesting', 'this', 'that', 'personally', 'feel', 'consistency', 'inclusive', 'feel', 'having', 'an', 'exception', 'for', 'non', 'binary', 'people', 'who', 'use', 'they', 'them', 'pronouns', 'just', 'that', 'an', 'exception', 'feel', 'language', 'important', 'and', 'it', 'evolves', 'where', 'we', 'decide', 'to', 'take', 'it', 'it', 'should', 'be', 'kind', 'think', 'that', 'since', 'singular', 'rd', 'person', 'form', 'of', 'be', 'he', 'she', 'that', 'singular', 'rd', 'person', 'form', 'for', 'they', 'could', 'be', 'they', 'this', 'keeps', 'rule', 'consistent', 'for', 'every', 'gender', 'he', 'she', 'they', 'plural', 'would', 'therefore', 'be', 'consistently', 'they', 'are', 'here', 'reworded', 'version', 'of', 'tweet', 'that', 'harmed', 'people', 'deleted', 'it', 'to', 'stop', 'any', 'further', 'hurt', 'but', 'own', 'mistake', 'and', 'am', 'sorry', 'for', 'any', 'hurt', 'caused', 'following', 'reworded', 'version', 'as', 'it', 'should', 'have', 'been', 'tweeted', 'in', 'first', 'place', 'first', 'three', 'guess', 'how', 'about', 'to', 'anyone', 'who', 'was', 'hurt', 'by', 'that', 'statement', 'you', 'where', 'right', 'in', 'being', 'upset', 'though', 'it', 'was', 'an', 'error', 'it', 'one', 'should', 'not', 'have', 'made', 'and', 'would', 'like', 'to', 'own', 'that', 'it', 'was', 'careless', 'of', 'me', 'and', 'am', 'sorry', 'my', 'actions', 'hurt', 'you', 'better', 'wording', 'yes', 'my', 'tone', 'through', 'thread', 'was', 'based', 'on', 'people', 'misquoting', 'me', 'putting', 'words', 'in', 'my', 'mouth', 'and', 'straw', 'manning', 'also', 'misgendering', 'being', 'called', 'cis', 'and', 'terf', 'which', 'as', 'you', 'can', 'imagine', 'makes', 'me', 'defensive', 'it', 'wasnt', 'intended', 'to', 'police', 'language', 'and', 'understand', 'tone', 'was', 'wrong', 'it', 'was', 'suggestion', 'nevertheless', 'wording', 'caused', 'real', 'hurt', 'to', 'people', 'and', 'for', 'that', 'am', 'deeply', 'sorry', 'https', 'twitter', 'com', 'delaneykingrox', 'status', 'in', 'this', 'example', 'main', 'dude', 'mesh', 'single', 'player', 'mesh', 'but', 'people', 'forget', 'you', 'need', 'st', 'person', 'arms', 'multiplayer', 'skins', 'etc', 'welp', 'am', 'up', 'for', 'it', 'deliciouslybad', 'del', 'gotta', 'keep', 'you', 'on', 'your', 'toes', 'okay', 'fuck', 'off', 'well', 'done', 'miss', 'you', 'seeing', 'you', 'talking', 'shit', 'makes', 'me', 'wanna', 'talk', 'shit', 'with', 'you', 'wording', 'was', 'shit', 'thats', 'on', 'me', 'https', 'twitter', 'com', 'delaneykingrox', 'status', 's', 'https', 'twitter', 'com', 'delaneykingrox', 'status', 's', 'https', 'twitter', 'com', 'delaneykingrox', 'status', 's', 'https', 'twitter', 'com', 'delaneykingrox', 'status', 's', 'yeah', 'prescriptive', 'way', 'this', 'tweet', 'comes', 'across', 'wrong', 'it', 'was', 'intended', 'as', 'suggestion', 'see', 'my', 'apology', 'hopefully', 'did', 'just', 'rechecked', 'apology', 'someone', 'asked', 'for', 'an', 'explaination', 'and', 'answered', 'that', 'seperately', 'so', 'as', 'not', 'to', 'come', 'across', 'as', 'non', 'apology', 'and', 'for', 'that', 'am', 'truely', 'sorry', 'get', 'that', 'day', 'in', 'and', 'day', 'out', 'anyway', 'from', 'actual', 'terfs', 'so', 'am', 'apologising', 'for', 'hurt', 'caused', 'for', 'badly', 'wording', 'my', 'suggestion', 'its', 'real', 'hurt', 'that', 'shouldnt', 'have', 'happened', 'and', 'that', 'on', 'me', 'am', 'not', 'apologising', 'for', 'defending', 'myself', 'against', 'attacks', 'for', 'blocking', 'idiots', 'who', 'misgendered', 'or', 'falsely', 'accused', 'me', 'os', 'all', 'manner', 'of', 'shit', 'so', 'all', 'those', 'attacks', 'didnt', 'help', 'malachite', 'tiger', 'showed', 'me', 'mistake', 'and', 'it', 'was', 'valid', 'one', 'so', 'am', 'at', 'fault', 'for', 'hurting', 'people', 'but', 'through', 'badly', 'stated', 'suggestion', 'not', 'that', 'think', 'my', 'opinion', 'better', 'that', 'am', 'law', 'or', 'am', 'demanding', 'change', 'definately', 'not', 'was', 'getting', 'angry', 'at', 'people', 'being', 'all', 'shades', 'of', 'shitty', 'about', 'stuff', 'they', 'said', 'said', 'which', 'didnt', 'hurt', 'people', 'who', 'thought', 'was', 'demanding', 'they', 'adhere', 'to', 'my', 'idea', 'do', 'you', 'see', 'out', 'of', 'people', 'attacking', 'me', 'most', 'where', 'attributing', 'things', 'had', 'not', 'said', 'which', 'as', 'you', 'may', 'imagine', 'angering', 'and', 'unfair', 'felt', 'was', 'bei', 'ng', 'tarred', 'with', 'wrong', 'brush', 'which', 'was', 'however', 'that', 'was', 'my', 'fault', 'my', 'wording', 'set', 'wrong', 'tone', 'and', 'as', 'result', 'so', 'mistake', 'was', 'there', 'and', 'could', 'see', 'it', 'so', 'can', 'apologise', 'for', 'it', 'it', 'was', 'written', 'in', 'prescriptive', 'tone', 'when', 'actually', 'it', 'was', 'suggestive', 'but', 'written', 'quickly', 'and', 'tersely', 'malachite', 'tiger', 'calmly', 'pointed', 'to', 'issue', 'exact', 'sentence', 'and', 'could', 'see', 'how', 'it', 'was', 'poorly', 'written', 'conveying', 'totally', 'different', 'tone', 'than', 'intended', 'so', 'crux', 'of', 'issue', 'was', 'people', 'where', 'saying', 'said', 'something', 'did', 'not', 'that', 'was', 'expressing', 'values', 'that', 'wasnt', 'what', 'called', 'straw', 'man', 'fallacy', 'thought', 'it', 'was', 'because', 'they', 'where', 'unleashing', 'hurt', 'in', 'wrong', 'direction', 'but', 'not', 'at', 'all', 'find', 'explaining', 'takes', 'away', 'from', 'apologies', 'would', 'you', 'like', 'me', 'to', 'explain', 'https', 'twitter', 'com', 'delaneykingrox', 'status', 's', 'felt', 'was', 'being', 'tarred', 'by', 'wrong', 'brush', 'being', 'pointed', 'back', 'to', 'wording', 'of', 'original', 'statement', 'coming', 'across', 'as', 'prescriptive', 'instead', 'of', 'suggestion', 'was', 'at', 'fault', 'that', 'was', 'my', 'mistake', 'and', 'it', 'hurt', 'people', 'am', 'sorry', 'no', 'leave', 'it', 'will', 'leave', 'mine', 'https', 'twitter', 'com', 'delaneykingrox', 'status', 's', 'genuinely', 'didnt', 'intend', 'it', 'as', 'prescriptive', 'hope', 'you', 'can', 'forgive', 'me', 'right', 'god', 'no', 'not', 'at', 'all', 'get', 'it', 'now', 'https', 'twitter', 'com', 'delaneykingrox', 'status', 's', 'https', 'twitter', 'com', 'delaneykingrox', 'status', 's', 'https', 'twitter', 'com', 'delaneykingrox', 'status', 's', 'gotcha', 'apology', 'posted', 'for', 'hurt', 'caused', 'there', 'was', 'lot', 'of', 'people', 'affected', 'didnt', 'want', 'to', 'miss', 'anyone', 'if', 'it', 'helps', 'it', 'was', 'just', 'one', 'tweet', 'of', 'theirs', 'that', 'showed', 'me', 'mistake', 'an', 'aha', 'moment', 'apologize', 'to', 'you', 'personally', 'am', 'sorry', 'public', 'apology', 'https', 'twitter', 'com', 'delaneykingrox', 'status', 'once', 'again', 'am', 'sorry', 'of', 'inadvertently', 'hurt', 'anyone', 'reading', 'inadvertent', 'hurt', 'still', 'hurt', 'and', 'it', 'helped', 'others', 'attacking', 'me', 'through', 'fallacy', 'became', 'defensive', 'non', 'binary', 'people', 'as', 'have', 'said', 'elsewhere', 'own', 'their', 'pronouns', 'and', 'should', 'police', 'their', 'language', 'my', 'suggestion', 'was', 'consistency', 'could', 'help', 'it', 'should', 'have', 'been', 'worded', 'better', 'thank', 'you', 'malachitetiger', 'for', 'talking', 'through', 'problem', 'to', 'solution', 'am', 'grateful', 'fot', 'time', 'and', 'patience', 'you', 'put', 'in', 'stand', 'by', 'grammar', 'statements', 'about', 'things', 'like', 'second', 'person', 'and', 'third', 'person', 'forms', 'but', 'thats', 'just', 'what', 'written', 'regarding', 'grammar', 'as', 'fite', 'me', 'over', 'grammar', 'as', 'noone', 'gets', 'hurt', 'but', 'statement', 'as', 'an', 'imperitive', 'hurt', 'people', 'opening', 'old', 'wounds', 'for', 'that', 'am', 'sorry', 'know', 'see', 'where', 'anger', 'coming', 'from', 'in', 'regards', 'to', 'feeling', 'that', 'am', 'demanding', 'change', 'am', 'not', 'demanding', 'change', 'and', 'do', 'not', 'think', 'my', 'opinion', 'of', 'greater', 'worth', 'than', 'those', 'affected', 'thanks', 'to', 'time', 'spent', 'by', 'malachite', 'tiger', 'have', 'seen', 'error', 'in', 'my', 'original', 'tweet', 'tone', 'was', 'supposed', 'to', 'be', 'as', 'suggestion', 'not', 'fact', 'hereby', 'genuinely', 'apologise', 'to', 'anyone', 'who', 'felt', 'hurt', 'by', 'that', 'statement', 'am', 'sorry', 'attacks', 'have', 'been', 'getting', 'where', 'not', 'helpful', 'you', 'have', 'been', 'completely', 'helpful', 'and', 'we', 'got', 'down', 'to', 'crux', 'of', 'problem', 'that', 'really', 'would', 'change', 'phasing', 'because', 'would', 'have', 'worked', 'not', 'what', 'hapoening', 'at', 'my', 'end', 'but', 'that', 'really', 'helpful', 'that', 'it', 'comes', 'to', 'that', 'phrasing', 'by', 'which', 'mean', 'intersex', 'stuff', 'you', 'have', 'to', 'understand', 'that', 'am', 'attacked', 'constantly', 'because', 'of', 'what', 'just', 'posted', 'it', 'makes', 'me', 'very', 'quick', 'to', 'brush', 'off', 'and', 'block', 'people', 'where', 'infact', 'was', 'stating', 'that', 'we', 'should', 'make', 'rules', 'consistent', 'to', 'simply', 'include', 'everyone', 'so', 'last', 'five', 'hours', 'of', 'my', 'life', 'has', 'been', 'about', 'phrasing', 'of', 'line', 'that', 'comes', 'across', 'like', 'think', 'am', 'boss', 'of', 'you', 'phrasing', 'not', 'tonal', 'opened', 'up', 'with', 'non', 'binary', 'people', 'shouldn', 't', 'be', 'excluded', 'in', 'case', 'you', 'are', 'wondering', 'where', 'sit', 'on', 'transphobe', 'scale', 'this', 'me', 'https', 'twitter', 'com', 'delaneykingrox', 'status', 's', 'so', 'it', 'comes', 'down', 'to', 'statement', 'that', 'it', 'exclusionary', 'should', 'have', 'been', 'isn', 't', 'it', 'exclusionary', 'so', 'it', 'tonal', 'know', 'https', 'twitter', 'com', 'delaneykingrox', 'status', 's', 'oooookay', 'oh', 'yeah', 'congrats', 'on', 'that', 'even', 'suggesting', 'didnt', 'see', 'this', 'what', 'driving', 'me', 'nuts', 'here', 'how', 'it', 'patronizing', 'to', 'discuss', 'grammar', 'and', 'suggest', 'consistent', 'rd', 'person', 'form', 'of', 'be', 'regardless', 'of', 'gender', 'definitely', 'an', 'issue', 'with', 'people', 'attacking', 'me', 'will', 'learn', 'for', 'mistakes', 'made', 'do', 'it', 'makes', 'me', 'better', 'person', 'will', 'not', 'learn', 'for', 'mistakes', 'attributed', 'to', 'me', 'that', 'did', 'not', 'do', 'but', 'they', 'do', 'in', 'last', 'few', 'hours', 'have', 'been', 'called', 'an', 'asshole', 'terf', 'cis', 'trash', 'dude', 'and', 'had', 'all', 'manner', 'of', 'shit', 'attributed', 'to', 'me', 'saying', 'it', 'could', 'be', 'better', 'my', 'being', 'consistent', 'harmful', 'okay', 'assume', 'am', 'not', 'doing', 'it', 'on', 'purpose', 'and', 'it', 'tone', 'am', 'not', 'demanding', 'anything', 'am', 'talking', 'about', 'rules', 'as', 'they', 'are', 'and', 'where', 'we', 'could', 'take', 'them', 'so', 'its', 'not', 'speaking', 'for', 'its', 'tone', 'thats', 'tone', 'said', 'making', 'an', 'exception', 'to', 'rule', 'just', 'for', 'non', 'binary', 'people', 'who', 'use', 'they', 'exclusionary', 'how', 'one', 'rule', 'for', 'he', 'she', 'e', 'sie', 'zim', 've', 'but', 'then', 'another', 'for', 'they', 'explain', 'speaking', 'over', 'can', 'you', 'see', 'how', 'suggesting', 'we', 'keep', 'consistency', 'can', 'be', 'seen', 'as', 'an', 'inclusive', 'concept', 'though', 'now', 'that', 'statement', 'bang', 'on', 'correct', 'cool', 'bananas', 'never', 'said', 'that', 'how', 'so', 'said', 'should', 'as', 'in', 'ideally', 'hardly', 'insisting', 'not', 'once', 'fair', 'cop', 'you', 'are', 'obviously', 'judge', 'and', 'jury', 'here', 'what', 'on', 'earth', 'are', 'you', 'talking', 'about', 'now', 'you', 'are', 'saying', 'regardless', 'of', 'rules', 'yep', 'where', 'singular', 'third', 'person', 'okay', 'how', 'did', 'do', 'they', 'talk', 'of', 'me', 'like', 'am', 'not', 'surrounded', 'by', 'same', 'hate', 'same', 'cruel', 'systems', 'because', 'they', 'want', 'way', 'to', 'express', 'that', 'and', 'here', 'am', 'away', 'you', 'go', 'they', 'talk', 'of', 'same', 'feelings', 'know', 'well', 'as', 'an', 'intersex', 'person', 'someone', 'who', 'transitioned', 'and', 'faced', 'trauma', 'after', 'trauma', 'know', 'lashed', 'out', 'at', 'wrong', 'people', 'too', 'they', 'speak', 'of', 'being', 'talked', 'over', 'silenced', 'being', 'ordered', 'or', 'forced', 'into', 'rules', 'that', 'they', 'have', 'no', 'say', 'in', 'they', 'express', 'how', 'vulnerable', 'it', 'makes', 'them', 'feel', 'how', 'voiceless', 'to', 'those', 'attacking', 'me', 'grammar', 'has', 'been', 'cruel', 'weapon', 'used', 'against', 'them', 'think', 'that', 'great', 'deal', 'of', 'very', 'valid', 'trauma', 'exists', 'from', 'having', 'to', 'deal', 'with', 'bigots', 'policing', 'pronouns', 'think', 'those', 'attacking', 'me', 'need', 'to', 'vent', 'that', 'anger', 'and', 'hurt', 'see', 'that', 'they', 'are', 'misquoting', 'me', 'attaching', 'false', 'values', 'to', 'my', 'statements', 'to', 'make', 'me', 'bad', 'guy', 'they', 'need', 'now', 'yeah', 'never', 'argued', 'against', 'singular', 'they', 'not', 'that', 'people', 'seem', 'to', 'care', 'also', 'said', 'that', 'stuff', 'too', 'not', 'that', 'it', 'seems', 'to', 'matter', 'that', 'works', 'for', 'me', 'because', 'totally', 'agree', 'with', 'you', 'on', 'all', 'points', 'ciao', 'am', 'not', 'gonna', 'tell', 'anyone', 'what', 'to', 'do', 'either', 'get', 'impression', 'so', 'much', 'negative', 'association', 'linked', 'to', 'pronouns', 'that', 'it', 'spilling', 'over', 'into', 'this', 'discussion', 'hence', 'why', 'so', 'much', 'misquoting', 'and', 'false', 'attribution', 'going', 'on', 'it', 'seems', 'silly', 'to', 'argue', 'about', 'adjusting', 'be', 'when', 'we', 'can', 't', 'all', 'feel', 'safe', 'to', 'use', 'pronouns', 'we', 'want', 'am', 'sorry', 'you', 'have', 'to', 'go', 'through', 'that', 'have', 'few', 'dear', 'trans', 'masc', 'friends', 'who', 'are', 'also', 'terrified', 'of', 'using', 'pronoun', 'they', 'want', 'because', 'they', 'expect', 'backlash', 'always', 'make', 'sure', 'to', 'create', 'safe', 'space', 'around', 'them', 'to', 'ensure', 'they', 'can', 'pronouns', 'are', 'so', 'incredibly', 'powerful', 'but', 'so', 'far', 'have', 'had', 'misquoted', 'misgendered', 'had', 'straw', 'man', 'attacks', 'been', 'called', 'all', 'sorts', 'of', 'names', 'such', 'as', 'terf', 'all', 'over', 'form', 'of', 'be', 'suggesting', 'consistency', 'just', 'that', 'suggestion', 'ultimately', 'non', 'binary', 'people', 'will', 'drive', 'and', 'police', 'language', 'around', 'them', 'tone', 'impossible', 'to', 'read', 'online', 'those', 'attacking', 'me', 'are', 'reading', 'lot', 'into', 'what', 'have', 'discussed', 'adding', 'not', 'only', 'tone', 'but', 'content', 'that', 'isnt', 'there', 'sort', 'of', 'land', 'sliding', 'into', 'place', 'so', 'you', 'think', 'that', 'path', 'you', 'took', 'will', 'be', 'same', 'with', 'singular', 'third', 'person', 'they', 'have', 'another', 'liguist', 'talking', 'about', 'germanic', 'roots', 'its', 'fascinating', 'my', 'suggestion', 'came', 'with', 'little', 'historic', 'background', 'only', 'that', 'in', 'style', 'guides', 'so', 'would', 'love', 'to', 'know', 'all', 'this', 'me', 'either', 'and', 'it', 'isnt', 'battle', 'its', 'intended', 'as', 'suggestion', 'for', 'consistency', 'and', 'inclusion', 'may', 'ask', 'just', 'as', 'an', 'aside', 'if', 'you', 'prefer', 'another', 'pronoun', 'set', 'but', 'put', 'they', 'them', 'on', 'your', 'bio', 'because', 'of', 'that', 'pressure', 'thank', 'you', 'for', 'an', 'actual', 'discussion', 'without', 'assigning', 'false', 'values', 'to', 'my', 'statements', 'this', 'made', 'my', 'day', 'yeah', 'you', 'are', 'right', 'about', 'thin', 'edge', 'of', 'wedge', 'changes', 'as', 'social', 'lubricant', 'arguing', 'what', 'correct', 'vs', 'discussing', 'what', 'we', 'should', 'do', 'that', 'best', 'healthy', 'second', 'person', 'lol', 'yeah', 'english', 'weird', 'and', 'heaven', 'forbid', 'we', 'try', 'to', 'make', 'things', 'simple', 'and', 'inclusive', 'hi', 'am', 'delaney', 'post', 'about', 'intersex', 'biology', 'to', 'counter', 'terf', 'arguements', 'think', 'you', 'have', 'me', 'mixed', 'up', 'with', 'someone', 'else', 'https', 'twitter', 'com', 'delaneykingrox', 'status', 's', 'again', 'never', 'said', 'that', 'or', 'anything', 'like', 'that', 'am', 'an', 'intersex', 'person', 'who', 'identifies', 'as', 'female', 'thats', 'cool', 'thanks', 'for', 'sharing', 'your', 'opinion', 'and', 'not', 'blindly', 'attacking', 'have', 'addressed', 'that', 'still', 'making', 'shit', 'up', 'saying', 'said', 'it', 'then', 'getting', 'angry', 'at', 'me', 'for', 'it', 'majority', 'of', 'what', 'am', 'dealing', 'with', 'sure', 'was', 'just', 'saying', 'it', 'was', 'consistent', 'this', 'suggestion', 'based', 'on', 'current', 'rules', 'as', 'we', 'rework', 'out', 'language', 'to', 'finally', 'rightfully', 'include', 'non', 'binary', 'people', 'you', 'waltzing', 'in', 'and', 'calling', 'me', 'dick', 'though', 'not', 'cool', 'it', 'early', 'days', 'on', 'our', 'language', 'adjusting', 'to', 'rightfully', 'accomdate', 'non', 'binary', 'people', 'talking', 'about', 'it', 'healthy', 'ultimately', 'it', 'evolves', 'based', 'on', 'our', 'usage', 'we', 'can', 'steer', 'that', 'so', 'my', 'suggestion', 'keep', 'rule', 'for', 'singular', 'rd', 'person', 'consistent', 'its', 'stone', 'in', 'pond', 'because', 'again', 'think', 'you', 'are', 'fighting', 'someone', 'else', 'can', 'you', 'map', 'all', 'this', 'to', 'my', 'original', 'suggestion', 'same', 'good', 'night', 'you', 'are', 'right', 'that', 'we', 'already', 'have', 'use', 'they', 'are', 'when', 'we', 'don', 't', 'know', 'gender', 'so', 'by', 'tweaking', 'that', 'rule', 'to', 'be', 'or', 'person', 'has', 'no', 'gender', 'then', 'you', 'hit', 'mark', 'what', 'does', 'everyone', 'else', 'think', 'see', 'your', 'comment', 'works', 'well', 'because', 'you', 'have', 'reframed', 'where', 'gender', 'falls', 'in', 'rules', 'if', 'you', 'do', 'not', 'know', 'gender', 'there', 'no', 'gender', 'then', 'be', 'are', 'what', 'was', 'thinking', 'avoiding', 'this', 'kind', 'of', 'table', 'male', 'female', 'non', 'binary', 'are', 'from', 'functional', 'standpoint', 'should', 'we', 'keep', 'this', 'and', 'they', 'are', 'seperate', 'mean', 'rule', 'are', 'always', 'follows', 'they', 'regardless', 'of', 'it', 's', 'function', 'kinda', 'what', 'people', 'are', 'thinking', 'here', 'nice', 'an', 'actual', 'discussion', 'point', 'thank', 'you', 'thats', 'cool', 'what', 'am', 'doing', 'precisely', 'nah', 'all', 'am', 'arguing', 'that', 'rd', 'person', 'singular', 'of', 'be', 'for', 'he', 'and', 'she', 'so', 'when', 'adding', 'they', 'as', 'singular', 'keeping', 'same', 'rule', 'inclusive', 'otherwise', 'you', 'have', 'to', 'write', 'except', 'for', 'non', 'binary', 'people', 'great', 'do', 'you', 'have', 'moment', 'to', 'explain', 'how', 'in', 'practice', 'isnt', 'am', 'getting', 'mostly', 'straw', 'man', 'attacks', 'trying', 'to', 'lump', 'me', 'in', 'with', 'pronoun', 'police', 'but', 'not', 'discussions', 'as', 'to', 'why', 'keeping', 'grammar', 'rule', 'consistent', 'rd', 'person', 'singular', 'of', 'verb', 'be', 'not', 'this', 'see', 'previous', 'statement', 'on', 'clause', 'singular', 'switched', 'for', 'plural', 'it', 'bound', 'to', 'antecedent', 'in', 'your', 'example', 'though', 'first', 'they', 'isn', 't', 'singular', 'but', 'plural', 'because', 'it', 'someone', 'not', 'specific', 'singular', 'person', 'we', 'are', 'talking', 'they', 'singular', 'third', 'person', 'in', 'addition', 'to', 'misgendering', 'me', 'you', 'are', 'consistently', 'misquoting', 'attributing', 'words', 'and', 'meanings', 'again', 'have', 'no', 'idea', 'who', 'you', 'are', 'fighting', 'there', 'but', 'it', 'aint', 'me', 'you', 'are', 'would', 'be', 'second', 'person', 'singular', 'right', 'words', 'shift', 'meaning', 'and', 'context', 'on', 'ground', 'as', 'you', 'say', 'it', 'still', 'hurt', 'me', 'so', 'some', 'consistency', 'good', 'thing', 'but', 'ask', 'straight', 'guys', 'how', 'many', 'dudes', 'they', 'fucked', 'and', 'suddenly', 'it', 'has', 'gender', 'actually', 'it', 'on', 'point', 'though', 'because', 'meaning', 'for', 'some', 'people', 'in', 'some', 'contexts', 'gendered', 'in', 'others', 'not', 'its', 'really', 'good', 'example', 'of', 'your', 'own', 'arguement', 'about', 'common', 'use', 'and', 'official', 'use', 'dude', 'to', 'me', 'and', 'all', 'while', 'was', 'growing', 'up', 'masculine', 'correct', 'many', 'derived', 'languages', 'use', 'consistent', 'rules', 'that', 'are', 'sneered', 'at', 'in', 'racist', 'classist', 'systems', 'patois', 'use', 'of', 'very', 'consistent', 'you', 'they', 'perfectly', 'good', 'but', 'sneered', 'at', 'not', 'quote', 'from', 'me', 'again', 'straw', 'man', 'dont', 'know', 'who', 'you', 'are', 'fighting', 'but', 'it', 'isnt', 'me', 'mean', 'if', 'didnt', 'say', 'it', 'didnt', 'say', 'it', 'right', 'so', 'why', 'are', 'you', 'upset', 'at', 'me', 'for', 'saying', 'it', 'show', 'me', 'where', 'and', 'feel', 'lot', 'of', 'that', 'anger', 'being', 'misdirected', 'to', 'me', 'for', 'saying', 'they', 'would', 'be', 'consistent', 'for', 'rd', 'person', 'singular', 'and', 'consistent', 'kind', 'and', 'there', 'lot', 'of', 'justified', 'upset', 'about', 'that', 'you', 'should', 'use', 'singular', 'they', 'if', 'it', 'fits', 'you', 'should', 'be', 'angry', 'for', 'people', 'trying', 'to', 'hurt', 'you', 'for', 'your', 'pronouns', 'that', 'sentence', 'means', 'that', 'bigots', 'who', 'argue', 'that', 'singular', 'they', 'wrong', 'are', 'incorrect', 'it', 'has', 'been', 'part', 'of', 'our', 'language', 'for', 'hundreds', 'of', 'years', 'and', 'to', 'use', 'that', 'as', 'reason', 'why', 'non', 'binary', 'folk', 'shouldn', 't', 'use', 'they', 'as', 'their', 'pronouns', 'hateful', 'and', 'wrong', 'non', 'binary', 'people', 'shouldn', 't', 'be', 'excluded', 'from', 'existing', 'rules', 'if', 'you', 'are', 'gonna', 'quote', 'me', 'don', 't', 'try', 'and', 'take', 'it', 'out', 'of', 'context', 'so', 'you', 'think', 'non', 'binary', 'people', 'should', 'be', 'excluded', 'because', 'don', 't', 'and', 'that', 's', 'pretty', 'solid', 'stance', 'to', 'take', 'again', 'never', 'said', 'that', 'and', 'said', 'opposite', 'am', 'not', 'entirely', 'sure', 'who', 'you', 'are', 'arguing', 'with', 'but', 'it', 'isnt', 'me', 'again', 'you', 'are', 'using', 'quotes', 'and', 'putting', 'words', 'in', 'them', 'on', 'my', 'behalf', 'may', 'kindly', 'ask', 'you', 'to', 'only', 'put', 'sentence', 'have', 'said', 'so', 'can', 'discuss', 'them', 'with', 'you', 'so', 'you', 'are', 'happy', 'you', 'are', 'worth', 'my', 'time', 'am', 'just', 'getting', 'tired', 'rather', 'than', 'take', 'my', 'time', 'to', 'run', 'you', 'through', 'verb', 'conjugation', 'which', 'have', 'done', 'lot', 'today', 'may', 'ask', 'that', 'you', 'read', 'up', 'on', 'they', 'am', 'sorry', 'you', 'have', 'anxiety', 'its', 'worst', 'but', 'no', 'that', 'not', 'what', 'am', 'saying', 'it', 'not', 'at', 'all', 'what', 'have', 'said', 'for', 'example', 'can', 'you', 'copy', 'and', 'paste', 'link', 'to', 'those', 'two', 'statements', 'you', 'just', 'made', 'on', 'my', 'behalf', 'well', 'in', 'grammar', 'thats', 'whole', 'different', 'discussion', 'about', 'personal', 'needs', 'why', 'did', 'you', 'use', 'male', 'term', 'for', 'me', 'okay', 'switch', 'your', 'arguement', 'around', 'you', 'called', 'me', 'dude', 'explain', 'why', 'point', 'to', 'where', 'did', 'that', 'post', 'link', 'st', 'am', 'nd', 'you', 'are', 'rd', 'he', 'thats', 'because', 'it', 'second', 'person', 'read', 'it', 'again', 'for', 'fuck', 'sake', 'singular', 'they', 'great', 'selecting', 'your', 'pronouns', 'great', 'people', 'using', 'grammar', 'as', 'an', 'arguement', 'as', 'to', 'why', 'you', 'shouldnt', 'select', 'your', 'own', 'pronouns', 'very', 'bad', 'think', 'anger', 'here', 'coming', 'from', 'people', 'knee', 'jerk', 'reacting', 'to', 'that', 'behaviour', 'soooooo', 'much', 'bigger', 'fish', 'to', 'fry', 'and', 'lot', 'of', 'folk', 'seem', 'more', 'focused', 'on', 'frying', 'people', 'than', 'those', 'fish', 'again', 'you', 'are', 'putting', 'looooot', 'of', 'words', 'in', 'my', 'mouth', 'and', 'then', 'attacking', 'me', 'for', 'it', 'not', 'at', 'all', 'read', 'again']\n",
      "Canonicalized Text:  [20072, 15430, 224, 1767, 9, 48, 6564, 4, 122052, 453, 443, 134, 404, 487, 2385, 149, 2663, 14, 6, 4, 122053, 5392, 268, 8072, 3, 1911, 8028, 3, 89, 9491, 5, 9665, 162, 14, 6, 4, 410, 122054, 5, 256, 15, 12, 286, 56, 11307, 5, 1144, 3, 23, 44, 1833, 15, 32, 9288, 21, 144, 23, 5073, 10, 162, 9531, 3722, 67, 23, 649, 13039, 7496, 232, 222, 858, 19342, 1374, 1416, 13, 1318, 1616, 268, 2397, 10, 120, 5220, 136, 38297, 711, 14, 6, 4, 122055, 68, 122056, 7, 238, 18, 1069, 10, 27, 356, 53, 352, 43, 7132, 10, 982, 58, 5457, 799, 78956, 22, 2111, 1330, 28, 61, 684, 223, 362, 29, 43, 2204, 6771, 102, 57, 9796, 25, 1337, 746, 85, 61, 5809, 684, 70, 34, 832, 10, 11, 122, 2572, 746, 25708, 42, 1337, 68, 1258, 28, 179, 85, 34, 70, 265, 18637, 22999, 3, 1524, 746, 13, 10915, 2166, 30, 22, 4543, 3, 238, 112, 539, 43, 112, 11, 795, 521, 30, 1524, 11, 13, 8155, 119, 123, 21, 29, 794, 333, 9, 48, 1247, 4, 1313, 4782, 122057, 305, 7670, 17995, 60532, 166, 166, 247, 3854, 109, 143, 109, 23, 84, 78957, 4200, 17971, 364, 9157, 19, 56, 63, 62, 3, 1428, 95, 38, 1611, 6435, 5, 6696, 11538, 5, 11264, 15, 49899, 14, 6, 4, 60533, 122058, 7, 39, 10085, 134, 122059, 37, 99, 30, 89, 60534, 8, 2288, 3276, 882, 734, 408, 50, 157, 16, 115, 24, 7, 100, 254, 14, 6, 4, 122060, 49900, 1643, 1671, 18, 66, 43, 38298, 5, 17429, 881, 15, 16851, 20, 44, 91, 4243, 27, 1500, 5, 43068, 127, 2029, 51, 11, 183, 309, 107, 42, 3984, 7, 65, 26, 2383, 10, 9, 186, 22, 122061, 122062, 240, 179, 635, 16, 381, 9, 186, 22, 122063, 175, 868, 9666, 799, 19, 34, 144, 14652, 12, 34646, 615, 9, 6, 4, 18638, 41, 484, 122064, 24, 49901, 24295, 28, 10, 7, 59, 180, 38299, 49902, 13966, 54, 2413, 382, 5, 78958, 381, 64, 12464, 13967, 122065, 106, 73, 1643, 29, 58, 476, 381, 3, 129, 25, 5, 49903, 1417, 4556, 107, 419, 50, 94, 1754, 74, 48, 122066, 4, 122067, 55, 61, 2017, 25, 2315, 184, 4730, 60, 778, 20, 96, 1995, 1401, 18, 25709, 15, 1331, 33, 77, 7, 28, 89, 103, 2576, 8769, 5, 7678, 24, 619, 375, 249, 35, 32, 24, 5, 172, 53, 10, 311, 199, 348, 50, 53, 220, 106, 10, 155, 7, 3, 9400, 3, 34, 33, 20, 197, 8, 60, 24, 17430, 140, 34, 24, 42, 7443, 11, 15016, 37, 7, 87, 3, 326, 4866, 753, 2591, 54, 1627, 23, 149, 31, 972, 11254, 5, 3620, 12717, 3, 388, 1833, 35, 24296, 59, 5, 268, 5, 209, 34, 481, 479, 21, 37, 7, 70, 43, 12224, 8, 4866, 743, 13, 386, 479, 114, 25710, 1480, 8, 4866, 11469, 72, 20, 3935, 182, 86, 61, 7132, 16, 13, 1226, 117, 9028, 7200, 117, 273, 58, 6409, 15, 692, 7444, 60, 64, 268, 34, 73, 6316, 29, 13, 58, 6409, 117, 1070, 520, 5, 10, 21900, 148, 35, 1489, 3, 155, 10, 10, 131, 22, 329, 70, 13, 260, 7443, 1459, 214, 1148, 8, 22, 47, 79, 13, 7443, 1459, 214, 1148, 15, 34, 144, 22, 34, 16, 1215, 1833, 4248, 15, 171, 1627, 47, 79, 34, 19343, 77, 3083, 22, 4483, 34, 24, 104, 60535, 823, 8, 314, 13, 14303, 60, 2897, 10, 3, 229, 161, 1849, 796, 21, 253, 1934, 5, 61, 256, 15, 161, 796, 3006, 1041, 60535, 823, 42, 10, 131, 23, 90, 2381, 11, 130, 370, 130, 517, 367, 53, 43, 3, 274, 64, 27, 796, 68, 13, 1836, 7, 148, 108, 11, 112, 1349, 249, 10, 27, 58, 3609, 10, 44, 131, 26, 23, 184, 5, 77, 28, 3, 253, 13, 10, 27, 15386, 8, 18, 5, 61, 256, 12, 2169, 796, 7, 162, 9127, 123, 12, 3160, 251, 615, 27, 778, 20, 60, 43069, 18, 1115, 449, 11, 12, 1399, 5, 6525, 38300, 95, 25709, 112, 463, 5149, 5, 27389, 187, 42, 7, 39, 612, 232, 18, 7047, 10, 755, 4501, 3, 1338, 1070, 5, 388, 3160, 27, 356, 10, 27, 6049, 13327, 9127, 3006, 235, 796, 3, 60, 5, 15, 13, 61, 2331, 256, 9, 6, 4, 18638, 41, 11, 16, 1331, 889, 535, 15896, 571, 1629, 15896, 21, 60, 712, 7, 101, 118, 214, 2250, 17431, 12225, 666, 3973, 61, 51, 15, 10, 122068, 2620, 581, 233, 7, 20, 36, 5566, 280, 259, 160, 124, 267, 307, 7, 497, 7, 402, 218, 232, 18, 322, 326, 218, 25, 7, 9127, 27, 218, 172, 20, 18, 9, 6, 4, 18638, 41, 17, 9, 6, 4, 18638, 41, 17, 9, 6, 4, 18638, 41, 17, 9, 6, 4, 18638, 41, 17, 209, 43070, 122, 16, 314, 563, 1014, 356, 10, 27, 4501, 42, 6049, 89, 12, 5457, 1072, 106, 29, 78959, 5457, 151, 618, 15, 58, 60536, 5, 4636, 13, 78960, 19, 42, 26, 3, 195, 1014, 42, 692, 5457, 5, 15, 13, 61, 34647, 256, 57, 13, 91, 11, 5, 91, 50, 645, 54, 793, 22883, 19, 61, 38301, 15, 796, 3006, 15, 2437, 9127, 12, 6049, 56, 235, 796, 13, 1630, 23, 527, 5, 13, 20, 18, 61, 26, 38301, 15, 4567, 230, 573, 3650, 15, 4730, 3387, 64, 24297, 52, 15017, 5195, 18, 1799, 32, 5906, 8, 218, 19, 32, 180, 3650, 288, 225, 24298, 6823, 1958, 18, 1934, 5, 10, 27, 1996, 44, 19, 61, 31, 1795, 15, 3824, 60, 21, 251, 2437, 6100, 6049, 26, 13, 70, 12, 949, 162, 13, 61, 849, 52, 61, 6992, 385, 31815, 26, 27, 222, 1114, 31, 60, 112, 32, 5360, 8, 2088, 43, 362, 34, 174, 174, 187, 288, 796, 60, 64, 219, 27, 6992, 34, 21901, 3, 12, 414, 38, 7, 89, 50, 8, 60, 4633, 18, 169, 148, 38302, 133, 92, 26, 174, 187, 42, 7, 245, 612, 49904, 5, 4634, 577, 27, 1846, 428, 78961, 25, 356, 5883, 187, 27, 1095, 13, 27, 12, 1795, 12, 9127, 547, 356, 3160, 5, 42, 2192, 19, 1934, 27, 69, 5, 144, 89, 10, 19, 39, 9406, 15, 10, 10, 27, 1204, 11, 43070, 3160, 49, 176, 10, 27, 38303, 21, 1204, 2020, 5, 78962, 24298, 6823, 16353, 5508, 3, 852, 1686, 2555, 5, 144, 89, 53, 10, 27, 5761, 1204, 60537, 617, 369, 3160, 127, 4501, 19, 17996, 8, 852, 27, 60, 148, 391, 174, 153, 106, 26, 13, 27, 7598, 2509, 13, 755, 33, 463, 6525, 177, 13968, 219, 10, 27, 85, 34, 148, 34648, 796, 11, 356, 2605, 21, 26, 31, 32, 234, 4164, 730, 313, 54, 6271, 77, 7, 28, 18, 3, 1065, 9, 6, 4, 18638, 41, 17, 577, 27, 112, 78961, 68, 356, 5883, 112, 5508, 110, 3, 9127, 8, 994, 1836, 408, 1014, 42, 43070, 457, 8, 6049, 27, 31, 1795, 13, 27, 12, 1934, 5, 10, 796, 60, 61, 256, 55, 502, 10, 65, 502, 528, 9, 6, 4, 18638, 41, 17, 1648, 288, 9935, 10, 42, 43070, 185, 7, 39, 2799, 18, 108, 167, 55, 26, 31, 32, 57, 10, 67, 9, 6, 4, 18638, 41, 17, 9, 6, 4, 18638, 41, 17, 9, 6, 4, 18638, 41, 17, 7355, 5457, 982, 15, 796, 3006, 69, 27, 197, 8, 60, 4262, 288, 87, 3, 307, 274, 37, 10, 1236, 10, 27, 29, 44, 314, 8, 6082, 13, 1958, 18, 1934, 58, 7582, 585, 3053, 3, 7, 1226, 61, 256, 731, 5457, 9, 6, 4, 18638, 41, 393, 181, 61, 256, 8, 20125, 796, 274, 439, 49905, 796, 102, 796, 5, 10, 1478, 418, 4633, 18, 251, 13968, 1684, 7047, 692, 7444, 60, 42, 23, 174, 6007, 253, 96, 6316, 5, 131, 1338, 96, 1070, 12, 6049, 27, 9028, 144, 225, 10, 131, 23, 90, 11540, 162, 93, 7, 122069, 15, 402, 251, 541, 3, 2648, 61, 1194, 38304, 62, 5, 3268, 7, 295, 11, 833, 68, 4866, 6203, 43, 133, 28, 508, 214, 5, 1522, 214, 3394, 21, 172, 29, 33, 1204, 3151, 4866, 42, 18576, 18, 140, 4866, 42, 18639, 514, 796, 21, 1836, 42, 58, 122070, 796, 60, 1943, 239, 8560, 15, 13, 61, 256, 59, 89, 148, 2556, 408, 54, 11, 7798, 3, 386, 13, 61, 6992, 385, 61, 26, 6992, 385, 5, 38, 26, 70, 12, 949, 8, 3159, 639, 127, 180, 4262, 182, 3, 62, 944, 68, 24298, 6823, 23, 357, 3609, 11, 12, 994, 314, 3160, 27, 811, 3, 22, 42, 6049, 26, 491, 29403, 1648, 9406, 3, 274, 64, 577, 796, 68, 13, 1836, 61, 256, 3650, 23, 90, 222, 148, 26, 1924, 7, 23, 90, 753, 1924, 5, 35, 97, 200, 3, 17996, 8, 541, 13, 75, 77, 385, 49906, 85, 77, 23, 1009, 26, 33, 122071, 31, 12, 320, 21, 13, 75, 1924, 13, 10, 563, 3, 13, 19337, 68, 187, 238, 16852, 362, 7, 23, 3, 388, 13, 61, 2703, 1583, 85, 8, 33, 29, 982, 10, 232, 18, 135, 1341, 3, 5883, 160, 5, 1325, 60, 148, 29404, 27, 7965, 13, 35, 131, 103, 1480, 4248, 3, 1216, 2443, 207, 19, 164, 1033, 422, 8, 12, 113, 83, 90, 43, 19337, 8, 646, 13, 563, 1014, 28, 70, 61, 1545, 8, 7, 19337, 26, 31654, 2640, 51, 25, 692, 7444, 60, 2166, 30, 22, 14304, 11, 627, 7, 24, 1343, 148, 1045, 20, 31661, 3502, 16, 18, 9, 6, 4, 18638, 41, 17, 19, 10, 563, 200, 3, 1836, 13, 10, 29405, 131, 23, 90, 521, 30, 10, 29405, 19, 10, 31654, 59, 9, 6, 4, 18638, 41, 17, 122072, 119, 209, 855, 20, 13, 100, 7132, 288, 89, 16, 33, 1458, 18, 3668, 104, 53, 10, 31816, 3, 2841, 4866, 5, 2571, 4248, 1459, 214, 1148, 8, 22, 2108, 8, 1627, 401, 58, 852, 25, 60, 4633, 18, 65, 554, 15, 2613, 184, 38, 10, 232, 18, 162, 214, 65, 26, 554, 15, 2613, 22873, 3, 18, 13, 106, 26, 38, 21, 34, 38, 11, 164, 325, 422, 23, 90, 463, 58, 2519, 27389, 5149, 1470, 535, 5, 92, 32, 5906, 8, 218, 22873, 3, 18, 391, 10, 144, 22, 162, 12, 112, 4248, 6993, 280, 1798, 61, 26, 205, 10, 20, 1552, 5, 10, 3160, 61, 26, 6992, 265, 61, 402, 43, 1480, 42, 34, 24, 5, 148, 35, 144, 155, 73, 19, 56, 26, 1149, 15, 56, 3160, 172, 3160, 174, 324, 58, 6409, 3, 1833, 29, 15, 692, 7444, 60, 64, 268, 34, 29405, 53, 44, 1833, 15, 47, 79, 143, 6994, 38305, 154, 21, 115, 281, 15, 34, 1065, 1149, 140, 39, 7, 89, 53, 7132, 35, 233, 9028, 39, 22, 357, 42, 58, 7200, 1622, 249, 67, 13, 1836, 2582, 20, 1199, 375, 8371, 120, 174, 13, 53, 19, 174, 131, 42, 11, 11759, 3704, 15869, 26, 393, 952, 3997, 7, 24, 1171, 1668, 5, 8621, 104, 33, 20, 907, 24, 7, 402, 43, 67, 7, 24, 391, 2108, 8, 1480, 998, 148, 7443, 1522, 214, 280, 53, 106, 38, 34, 326, 8, 18, 28, 61, 26, 4841, 68, 141, 279, 141, 3957, 3334, 85, 34, 87, 122, 3, 2284, 13, 5, 104, 61, 313, 7, 88, 34, 326, 8, 141, 916, 59, 124, 42, 58, 16852, 214, 151, 64, 25711, 5, 6013, 2632, 157, 2632, 59, 49907, 50, 31, 356, 60, 76, 34, 771, 8, 112, 1739, 140, 15897, 112, 2206, 52, 2404, 138, 1480, 13, 34, 23, 55, 132, 11, 34, 2284, 53, 3887, 10, 232, 73, 117, 53, 43071, 3, 180, 4633, 18, 4866, 83, 90, 3957, 6060, 344, 573, 73, 70, 13, 158, 704, 8, 135, 1996, 2632, 2489, 54, 273, 3, 704, 25, 13969, 13044, 6316, 70, 180, 4633, 18, 101, 3, 8920, 13, 2556, 5, 796, 89, 13, 34, 24, 43069, 18, 34649, 2336, 2509, 3, 12, 6203, 3, 103, 18, 204, 381, 34, 101, 67, 209, 120, 12750, 573, 7443, 34, 26, 13, 60, 805, 3, 339, 95, 174, 13, 362, 76, 26, 13, 10, 492, 3, 644, 13, 696, 15, 18, 85, 617, 485, 25, 7, 20, 32, 1378, 29406, 61, 26, 199, 250, 274, 33, 3, 38, 468, 57, 2839, 19, 82, 1808, 7238, 5336, 3, 6316, 13, 10, 15431, 140, 138, 16, 2304, 4783, 86, 19, 82, 43069, 5, 2336, 78963, 105, 20, 10, 492, 2365, 3, 2832, 43, 11308, 22, 49, 35, 39, 30, 32, 117, 776, 3, 268, 6316, 35, 87, 61, 256, 7, 23, 3, 88, 251, 13, 23, 325, 983, 1773, 20041, 224, 64, 24, 95, 3987, 8, 529, 23000, 34, 87, 85, 34, 1099, 10518, 136, 103, 196, 3, 1181, 776, 732, 276, 73, 3, 5316, 34, 39, 6316, 24, 19, 1745, 1125, 21, 19, 377, 23, 92, 23001, 24297, 92, 6525, 177, 3650, 90, 463, 32, 6453, 8, 1429, 270, 42, 27389, 32, 140, 1148, 8, 22, 7132, 9028, 29, 13, 6049, 4628, 692, 7444, 60, 65, 932, 5, 1338, 1070, 276, 73, 3160, 2121, 3, 202, 703, 180, 4633, 18, 24, 439, 197, 138, 33, 23, 6344, 2929, 26, 98, 3160, 21, 746, 13, 446, 69, 1184, 8, 1784, 14991, 138, 370, 19, 7, 70, 13, 1151, 7, 464, 65, 22, 141, 25, 7443, 1522, 214, 34, 23, 281, 122073, 402, 43, 34650, 5715, 56, 3314, 12, 6049, 475, 25, 215, 7567, 2101, 98, 13, 11, 972, 11254, 19, 77, 46, 3, 59, 32, 16, 18, 468, 5, 10, 446, 2180, 56, 4501, 42, 6049, 15, 9028, 5, 9532, 245, 398, 29, 42, 58, 2487, 37, 7, 1547, 281, 23000, 547, 21, 295, 34, 73, 20, 36, 2462, 85, 8, 13, 2330, 93, 7, 15, 58, 793, 2304, 330, 25712, 2336, 2509, 3, 12, 6203, 16, 184, 12, 91, 209, 7, 24, 108, 43, 4273, 2927, 8, 14305, 1649, 42, 454, 49908, 4223, 33, 1199, 945, 4069, 33, 35, 131, 38, 13, 142, 1233, 508, 214, 116, 209, 847, 479, 5, 2182, 10856, 35, 294, 3, 103, 133, 1144, 5, 7200, 478, 61, 20104, 223, 43, 16852, 9493, 3, 4156, 27389, 49909, 70, 7, 23, 18, 3096, 51, 25, 151, 380, 9, 6, 4, 18638, 41, 17, 181, 120, 174, 13, 52, 265, 28, 13, 61, 58, 16852, 214, 64, 27390, 42, 1351, 172, 375, 182, 15, 859, 36, 949, 5, 26, 13960, 4633, 23, 9407, 13, 102, 324, 218, 51, 391, 174, 10, 115, 222, 1114, 31, 18, 15, 10, 2193, 8, 33, 61, 3034, 25, 196, 27, 29, 391, 10, 27, 4248, 16, 6049, 778, 20, 1030, 1480, 42, 35, 78964, 50, 1070, 3, 400, 15018, 2443, 692, 7444, 60, 7, 78965, 11, 5, 946, 18, 1893, 249, 26, 375, 10, 659, 264, 20, 114, 1070, 11308, 3, 15018, 122074, 692, 7444, 60, 402, 43, 10, 1233, 4628, 10, 21900, 778, 20, 114, 9408, 35, 39, 23002, 13, 19, 12, 6049, 233, 1833, 15, 7443, 1459, 214, 4248, 56, 3656, 11, 9797, 85, 181, 70, 7, 24, 1250, 151, 380, 39, 7, 3699, 32, 16, 3, 12, 994, 6049, 141, 63, 246, 7, 24, 108, 13, 35, 257, 23, 268, 34, 24, 49, 35, 99, 30, 59, 1627, 19, 68, 29407, 13, 1833, 3, 22, 52, 214, 83, 55, 1627, 115, 7, 657, 1137, 33, 190, 207, 380, 70, 89, 36, 1170, 696, 124, 85, 7, 23, 43072, 148, 1627, 3421, 11, 1480, 37, 7, 38, 26, 59, 1627, 69, 55, 1627, 115, 22, 24, 33, 27, 361, 6340, 16, 329, 8, 1549, 1369, 1351, 692, 7444, 24, 54, 8372, 23003, 131, 35, 233, 16, 5, 34, 24, 17997, 238, 1833, 24, 136, 4166, 34, 2108, 8, 10, 17, 2272, 619, 33, 60, 24, 361, 104, 300, 58, 793, 2304, 333, 93, 7, 172, 375, 33, 61, 205, 6234, 1069, 32, 61, 4223, 13, 1459, 214, 7443, 8, 22, 15, 47, 5, 79, 19, 49, 2929, 34, 42, 7443, 1438, 141, 1833, 7200, 1487, 7, 23, 3, 562, 865, 15, 692, 7444, 60, 158, 38, 7, 23, 585, 3, 1065, 53, 11, 1319, 446, 61, 222, 1002, 6525, 177, 3650, 254, 3, 16354, 18, 11, 25, 23000, 1338, 21, 26, 7356, 42, 3, 86, 1438, 4866, 1833, 4248, 1459, 214, 7443, 8, 11545, 22, 26, 16, 89, 2668, 1836, 20, 15016, 7443, 6845, 15, 19343, 10, 4804, 3, 122075, 11, 36, 1331, 249, 130, 34, 521, 30, 7443, 21, 19343, 85, 10, 151, 26, 1496, 7443, 214, 35, 24, 402, 34, 7443, 1522, 214, 11, 3946, 3, 25709, 18, 7, 24, 4483, 43069, 38302, 449, 5, 11742, 181, 23, 55, 414, 64, 7, 24, 1250, 69, 21, 10, 1301, 18, 7, 24, 77, 22, 508, 214, 7443, 108, 449, 2329, 1625, 5, 1512, 20, 1903, 42, 7, 132, 10, 102, 796, 18, 19, 84, 9028, 63, 147, 21, 398, 790, 341, 53, 173, 2644, 34, 1513, 5, 1424, 10, 83, 1627, 176, 10, 20, 333, 249, 85, 1625, 15, 84, 60, 11, 84, 17432, 19344, 11, 418, 26, 56, 75, 63, 1331, 8, 36, 253, 43073, 43, 1240, 268, 5, 812, 268, 535, 3, 18, 5, 32, 221, 27, 1425, 51, 9533, 1199, 173, 16853, 3978, 268, 4248, 1480, 13, 24, 78966, 31, 11, 1191, 23004, 3334, 60538, 268, 8, 135, 4248, 7, 34, 2072, 63, 21, 78966, 31, 26, 890, 54, 18, 181, 6525, 177, 78, 59, 64, 7, 24, 1250, 21, 10, 446, 18, 238, 37, 288, 132, 10, 288, 132, 10, 108, 19, 86, 24, 7, 1349, 31, 18, 15, 391, 10, 237, 18, 148, 5, 117, 197, 8, 13, 2556, 112, 38306, 3, 18, 15, 391, 34, 77, 22, 4248, 15, 1459, 214, 7443, 5, 4248, 329, 5, 69, 197, 8, 9798, 1349, 43, 13, 7, 131, 268, 7443, 34, 37, 10, 3275, 7, 131, 22, 1114, 15, 60, 254, 3, 796, 7, 15, 36, 6316, 13, 2555, 489, 13, 13969, 64, 2832, 13, 7443, 34, 356, 24, 6995, 10, 83, 90, 286, 8, 114, 1070, 15, 4544, 8, 183, 5, 3, 268, 13, 42, 443, 86, 692, 7444, 3263, 2166, 30, 268, 34, 42, 96, 6316, 6040, 5, 356, 692, 7444, 60, 2166, 30, 22, 14304, 54, 4310, 1480, 37, 7, 24, 199, 890, 18, 99, 30, 294, 5, 155, 10, 50, 8, 1512, 19, 7, 70, 692, 7444, 60, 131, 22, 14304, 85, 99, 30, 5, 13, 17, 261, 2177, 7427, 3, 155, 181, 120, 174, 13, 5, 174, 2217, 61, 26, 2463, 196, 64, 7, 24, 4223, 25, 21, 10, 446, 18, 181, 7, 24, 529, 3168, 5, 1115, 449, 11, 73, 20, 12, 6109, 245, 7271, 398, 7, 3, 98, 295, 2555, 23, 174, 19, 39, 2841, 73, 25, 7, 19, 7, 24, 146, 7, 24, 639, 12, 62, 61, 29, 222, 638, 686, 127, 155, 12, 62, 3, 524, 7, 251, 11545, 49910, 187, 23, 267, 197, 126, 245, 398, 13, 7, 202, 51, 20, 34, 61, 256, 7, 23, 878, 56, 680, 21, 55, 13, 26, 33, 61, 391, 10, 26, 31, 32, 33, 23, 174, 15, 1331, 39, 7, 1842, 5, 7588, 799, 3, 180, 210, 6203, 7, 29, 184, 20, 12, 6109, 124, 11, 4866, 172, 308, 369, 2304, 43, 781, 471, 86, 106, 7, 268, 1369, 1359, 15, 18, 280, 1255, 36, 43073, 276, 7, 463, 18, 535, 1065, 86, 333, 3, 148, 106, 13, 223, 799, 118, 61, 978, 7, 24, 1459, 47, 172, 85, 10, 508, 214, 202, 10, 181, 15, 259, 2816, 7443, 34, 158, 25713, 36, 6316, 158, 60, 529, 4866, 42, 58, 43073, 42, 3, 86, 7, 1630, 7354, 36, 253, 6316, 135, 204, 70, 2556, 104, 408, 54, 60, 3966, 5140, 11114, 3, 13, 4139, 5892, 82, 1702, 1964, 3, 7572, 5, 197, 8, 3263, 805, 66, 3518, 20, 16774, 60, 127, 180, 1964, 181, 7, 24, 1115, 78967, 8, 449, 11, 12, 1399, 5, 115, 4633, 18, 15, 10, 26, 31, 32, 202, 181]\n",
      "Max lengths of texts:  8689\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Text: \",post_train[88])\n",
    "print(\"Canonicalized Text: \", x_train[88])\n",
    "print(\"Max lengths of texts: \", max([len(x) for x in x_train+x_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_mbti(string):\n",
    "    label_bin = []\n",
    "    if string[0]==\"E\":\n",
    "        label_bin.append(0)\n",
    "    else:\n",
    "        label_bin.append(1)\n",
    "    if string[1]==\"N\":\n",
    "        label_bin.append(0)\n",
    "    else:\n",
    "        label_bin.append(1)\n",
    "    if string[2]==\"F\":\n",
    "        label_bin.append(0)\n",
    "    else:\n",
    "        label_bin.append(1)\n",
    "    if string[3]==\"J\":\n",
    "        label_bin.append(0)\n",
    "    else:\n",
    "        label_bin.append(1)\n",
    "        \n",
    "    assert len(label_bin) == 4,\"Not a valid MBTI type\"\n",
    "    return label_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESFP\n",
      "[0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(label_train[0])\n",
    "print(binary_mbti(label_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 0, 1], [1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 1, 0], [1, 1, 1, 1]]\n",
      "['ESFP' 'ISTP' 'ISTP' 'ENTJ' 'ISTP']\n"
     ]
    }
   ],
   "source": [
    "y_train_id = list(map(lambda x: binary_mbti(x), label_train))\n",
    "y_test_id = list(map(lambda x: binary_mbti(x), label_test))\n",
    "\n",
    "print(y_train_id[0:5])\n",
    "print(label_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the NBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_np_array(example_ids, max_len=100, pad_id=0):\n",
    "\n",
    "    arr = np.full([len(example_ids), max_len], pad_id, dtype=np.int32)\n",
    "    ns = np.zeros([len(example_ids)], dtype=np.int32)\n",
    "    for i, ids in enumerate(example_ids):\n",
    "        cpy_len = min(len(ids), max_len)\n",
    "        arr[i,:cpy_len] = ids[:cpy_len]\n",
    "        ns[i] = cpy_len\n",
    "    return arr, ns\n",
    "\n",
    "def tokenize_post(post_string):\n",
    "    return vocab_mbti.words_to_ids(post_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_padded_array(post_ids, targets, max_len=100, pad_id=0,\n",
    "                    root_only=False, df_idxs=None):\n",
    "\n",
    "    x, ns = pad_np_array(post_ids, max_len=max_len, pad_id=pad_id)\n",
    "    return x, ns, np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_1 = []\n",
    "for i in range(len(y_train_id)):\n",
    "    y_train_1.append(y_train_id[i][1])\n",
    "\n",
    "y_test_1 = []\n",
    "for i in range(len(y_test_id)):\n",
    "    y_test_1.append(y_test_id[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_ns, train_y = as_padded_array(x_train, y_train_1)\n",
    "test_x, test_ns, test_y = as_padded_array(x_test, y_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2068\n"
     ]
    }
   ],
   "source": [
    "len(y_train_1)\n",
    "print(len(y_test_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (391,714 words) written to '/tmp/tf_bow_sst_20190803-1657/metadata.tsv'\n",
      "Projector config written to /tmp/tf_bow_sst_20190803-1657/projector_config.pbtxt\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tf_bow_sst_20190803-1657', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x15a7975c0>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "To view training (once it starts), run:\n",
      "\n",
      "    tensorboard --logdir='/tmp/tf_bow_sst_20190803-1657' --port 6006\n",
      "\n",
      "Then in your browser, open: http://localhost:6006\n"
     ]
    }
   ],
   "source": [
    "#set up model using tf.estimator\n",
    "\n",
    "import MBTI_BOW_model; reload(MBTI_BOW_model)\n",
    "\n",
    "# Specify model hyperparameters as used by model\n",
    "model_params = dict(V=vocab_mbti.size, embed_dim=50, hidden_dims=[25], num_classes=2,\n",
    "                    encoder_type='bow',\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.01)\n",
    "\n",
    "checkpoint_dir = \"/tmp/tf_bow_sst_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "if os.path.isdir(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "\n",
    "vocab_mbti.write_projector_config(checkpoint_dir, \"Encoder/Embedding_Layer/W_embed\")\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn=MBTI_BOW_model.classifier_model_fn, \n",
    "                               params=model_params,\n",
    "                               model_dir=checkpoint_dir)\n",
    "print(\"\")\n",
    "print(\"To view training (once it starts), run:\\n\")\n",
    "print(\"    tensorboard --logdir='{:s}' --port 6006\".format(checkpoint_dir))\n",
    "print(\"\\nThen in your browser, open: http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tf_bow_sst_20190803-1657/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.175193, step = 1\n",
      "INFO:tensorflow:global_step/sec: 214.806\n",
      "INFO:tensorflow:loss = 1.0870415, step = 101 (0.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.206\n",
      "INFO:tensorflow:loss = 0.9302255, step = 201 (0.442 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.956\n",
      "INFO:tensorflow:loss = 0.8070522, step = 301 (0.435 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.459\n",
      "INFO:tensorflow:loss = 0.7835412, step = 401 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.086\n",
      "INFO:tensorflow:loss = 0.7680011, step = 501 (0.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.603\n",
      "INFO:tensorflow:loss = 1.0118186, step = 601 (0.498 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 662 into /tmp/tf_bow_sst_20190803-1657/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.7286373.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20190803-1657/model.ckpt-662\n",
      "INFO:tensorflow:Saving checkpoints for 663 into /tmp/tf_bow_sst_20190803-1657/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.0104647, step = 663\n",
      "INFO:tensorflow:global_step/sec: 217.004\n",
      "INFO:tensorflow:loss = 0.758318, step = 763 (0.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.076\n",
      "INFO:tensorflow:loss = 0.61055374, step = 863 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 224.165\n",
      "INFO:tensorflow:loss = 0.6450573, step = 963 (0.446 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.696\n",
      "INFO:tensorflow:loss = 0.64289945, step = 1063 (0.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 224.399\n",
      "INFO:tensorflow:loss = 0.50516963, step = 1163 (0.445 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.753\n",
      "INFO:tensorflow:loss = 1.0867898, step = 1263 (0.447 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1324 into /tmp/tf_bow_sst_20190803-1657/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.43100306.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20190803-1657/model.ckpt-1324\n",
      "INFO:tensorflow:Saving checkpoints for 1325 into /tmp/tf_bow_sst_20190803-1657/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.95131683, step = 1325\n",
      "INFO:tensorflow:global_step/sec: 206.211\n",
      "INFO:tensorflow:loss = 0.5001484, step = 1425 (0.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.655\n",
      "INFO:tensorflow:loss = 0.43254817, step = 1525 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.084\n",
      "INFO:tensorflow:loss = 0.37296617, step = 1625 (0.704 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.8\n",
      "INFO:tensorflow:loss = 0.3335076, step = 1725 (0.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.509\n",
      "INFO:tensorflow:loss = 0.35336372, step = 1825 (0.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.674\n",
      "INFO:tensorflow:loss = 0.505574, step = 1925 (0.544 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1986 into /tmp/tf_bow_sst_20190803-1657/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.2150508.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20190803-1657/model.ckpt-1986\n",
      "INFO:tensorflow:Saving checkpoints for 1987 into /tmp/tf_bow_sst_20190803-1657/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.39024788, step = 1987\n",
      "INFO:tensorflow:global_step/sec: 218.46\n",
      "INFO:tensorflow:loss = 0.25836438, step = 2087 (0.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.763\n",
      "INFO:tensorflow:loss = 0.29774418, step = 2187 (0.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 224.122\n",
      "INFO:tensorflow:loss = 0.27499464, step = 2287 (0.446 sec)\n",
      "INFO:tensorflow:global_step/sec: 224.135\n",
      "INFO:tensorflow:loss = 0.2685037, step = 2387 (0.446 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.212\n",
      "INFO:tensorflow:loss = 0.32287425, step = 2487 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 132.158\n",
      "INFO:tensorflow:loss = 0.31904247, step = 2587 (0.757 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2648 into /tmp/tf_bow_sst_20190803-1657/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.16711643.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20190803-1657/model.ckpt-2648\n",
      "INFO:tensorflow:Saving checkpoints for 2649 into /tmp/tf_bow_sst_20190803-1657/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.33934432, step = 2649\n",
      "INFO:tensorflow:global_step/sec: 209.173\n",
      "INFO:tensorflow:loss = 0.20612055, step = 2749 (0.480 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.844\n",
      "INFO:tensorflow:loss = 0.25921315, step = 2849 (0.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.059\n",
      "INFO:tensorflow:loss = 0.21456909, step = 2949 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 175.122\n",
      "INFO:tensorflow:loss = 0.23857331, step = 3049 (0.575 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.915\n",
      "INFO:tensorflow:loss = 0.30797234, step = 3149 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.233\n",
      "INFO:tensorflow:loss = 0.2963382, step = 3249 (0.476 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3310 into /tmp/tf_bow_sst_20190803-1657/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.15723228.\n"
     ]
    }
   ],
   "source": [
    "#start training\n",
    "\n",
    "\n",
    "train_params = dict(batch_size=25, total_epochs=10, eval_every=2)\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "\n",
    "train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": train_x, \"ns\": train_ns}, y=train_y,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True, seed=42\n",
    "                 )\n",
    "\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": test_x, \"ns\": test_ns}, y=test_y,\n",
    "                    batch_size=25, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "  \n",
    "    model.train(input_fn=train_input_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2019-08-03-23:58:31\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20190803-1657/model.ckpt-3310\n",
      "INFO:tensorflow:Finished evaluation at 2019-08-03-23:58:32\n",
      "INFO:tensorflow:Saving dict for global step 3310: accuracy = 0.6000967, cross_entropy_loss = 0.98044854, global_step = 3310, loss = 1.0477558\n",
      "Perplexity on test set: 2.67\n",
      "Accuracy on test set: 60.01%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6000967,\n",
       " 'cross_entropy_loss': 0.98044854,\n",
       " 'loss': 1.0477558,\n",
       " 'global_step': 3310}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluation on test data\n",
    "\n",
    "eval_metrics = model.evaluate(input_fn=test_input_fn, name=\"test\")  \n",
    "\n",
    "print (\"Perplexity on test set: {:.03}\".format(math.exp(eval_metrics['cross_entropy_loss'])))\n",
    "print(\"Accuracy on test set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2019-08-04-00:00:11\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20190803-1657/model.ckpt-3310\n",
      "INFO:tensorflow:Finished evaluation at 2019-08-04-00:00:13\n",
      "INFO:tensorflow:Saving dict for global step 3310: accuracy = 0.879792, cross_entropy_loss = 0.21594322, global_step = 3310, loss = 0.28545442\n",
      "Perplexity on train set: 1.24\n",
      "Accuracy on train set: 87.98%\n"
     ]
    }
   ],
   "source": [
    "#Evaluation on training data\n",
    "\n",
    "eval_metrics = model.evaluate(input_fn=train_input_fn, name=\"train\")  \n",
    "\n",
    "print (\"Perplexity on train set: {:.03}\".format(math.exp(eval_metrics['cross_entropy_loss'])))\n",
    "print(\"Accuracy on train set: {:.02%}\".format(eval_metrics['accuracy']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model.predict(input_fn=test_input_fn)\n",
    "y_pred = []\n",
    "for i, p in enumerate(pred_y):\n",
    "    probs = list(p['proba'])\n",
    "    y_pred.append(probs.index(max(probs)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_label=[]\n",
    "for i in y_pred:\n",
    "    if i == 0:\n",
    "        y_pred_label.append('e')\n",
    "    else:\n",
    "        y_pred_label.append('i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "counter=collections.Counter(y_pred_label)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "987/(987+931)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
