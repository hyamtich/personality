{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBTI Parallel Classification Model with Neural BOW (I/E Axis)\n",
    "\n",
    "First, load libraries and useful functions from class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from w266_common import patched_numpy_io\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Utils and Helper libraries\n",
    "# import nltk\n",
    "from w266_common import utils, vocabulary\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Specifications for Binary Classification NBOW for MBTI\n",
    "\n",
    "In this baseline, the task is to predict the first MBTI axis (I vs. E) given a text string. We will model after the A2 assignment, with Architecture and Parameters defined below.\n",
    "\n",
    "### Pre-Processing:\n",
    "* Minimial pre-processing, only separating punctuation from text and lower-case all text\n",
    "* Assigning words to numerical indices based on a fixed Vocab size, defined by word frequency in training set\n",
    "* Pulled out first axis of all target labels, assigned to binary (E = 0, I = 1)\n",
    "\n",
    "### Architecture:\n",
    "* Encoder: Bag of Words \n",
    "* Decoder: Softmax\n",
    "* Classification: Binary (2 MBTI types - I or E)\n",
    "\n",
    "### Parameters\n",
    "* Batch Size: 25 \n",
    "* Text length: 100\n",
    "* Vocabulary size (V): ~328K - removed stopwords\n",
    "* Embedding Size: 50\n",
    "* Hidden Dimensions: 25\n",
    "\n",
    "### Training:\n",
    "* Epochs = 10 \n",
    "* 80% train, 20% test\n",
    "* Loss: Sparse Softmax Cross Entropy \n",
    "* Optimizers: Adagrad Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus & Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>tweets</th>\n",
       "      <th>prof_image_url</th>\n",
       "      <th>MBTI</th>\n",
       "      <th>orig_tweets</th>\n",
       "      <th>id_</th>\n",
       "      <th>retweets</th>\n",
       "      <th>fav</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BillTooke</td>\n",
       "      <td>[\"Yes. The College as the voting block for the...</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/8070634314...</td>\n",
       "      <td>ENFP</td>\n",
       "      <td>\"I have no shame and little guile when it come...</td>\n",
       "      <td>1123269839026642944</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dougie0216</td>\n",
       "      <td>['All Hail De Gendt! #TDF2019', 'He has a powe...</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1126179691...</td>\n",
       "      <td>INFP</td>\n",
       "      <td>\"@princessfemme Wow  another ENFP  what are th...</td>\n",
       "      <td>1123265408419733505</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kimbetech</td>\n",
       "      <td>['Get on LIVE now. 10:30pm EST https:// share....</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1101342873...</td>\n",
       "      <td>ENFP</td>\n",
       "      <td>\"I am an ENFP . We do not take kindly to being...</td>\n",
       "      <td>988128789577240576</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>honeyBklein</td>\n",
       "      <td>[\"I agree. It's a spiritual battle. In light o...</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1130313304...</td>\n",
       "      <td>ENFJ</td>\n",
       "      <td>\"@ABeardedPoet actually  I am an INFP  not INF...</td>\n",
       "      <td>1123206556181573632</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tanishatray4</td>\n",
       "      <td>['Are you left handed or right handed? — Left ...</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/9792931110...</td>\n",
       "      <td>ENFP</td>\n",
       "      <td>\"Ya Absolutely  that makes a lot of sense .The...</td>\n",
       "      <td>1123130637664309248</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_screen_name                                             tweets  \\\n",
       "0        BillTooke  [\"Yes. The College as the voting block for the...   \n",
       "1       dougie0216  ['All Hail De Gendt! #TDF2019', 'He has a powe...   \n",
       "2        kimbetech  ['Get on LIVE now. 10:30pm EST https:// share....   \n",
       "3      honeyBklein  [\"I agree. It's a spiritual battle. In light o...   \n",
       "4     tanishatray4  ['Are you left handed or right handed? — Left ...   \n",
       "\n",
       "                                      prof_image_url  MBTI  \\\n",
       "0  http://pbs.twimg.com/profile_images/8070634314...  ENFP   \n",
       "1  http://pbs.twimg.com/profile_images/1126179691...  INFP   \n",
       "2  http://pbs.twimg.com/profile_images/1101342873...  ENFP   \n",
       "3  http://pbs.twimg.com/profile_images/1130313304...  ENFJ   \n",
       "4  http://pbs.twimg.com/profile_images/9792931110...  ENFP   \n",
       "\n",
       "                                         orig_tweets                  id_  \\\n",
       "0  \"I have no shame and little guile when it come...  1123269839026642944   \n",
       "1  \"@princessfemme Wow  another ENFP  what are th...  1123265408419733505   \n",
       "2  \"I am an ENFP . We do not take kindly to being...   988128789577240576   \n",
       "3  \"@ABeardedPoet actually  I am an INFP  not INF...  1123206556181573632   \n",
       "4  \"Ya Absolutely  that makes a lot of sense .The...  1123130637664309248   \n",
       "\n",
       "   retweets  fav  \n",
       "0         0    1  \n",
       "1         0    1  \n",
       "2         0    1  \n",
       "3         0    2  \n",
       "4         0    0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "df = pd.read_csv('../personalities_large_no_duplicates_C.csv',index_col = 0)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10339, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## dimensions:\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ENFP',\n",
       " 'INFP',\n",
       " 'ENFJ',\n",
       " 'ENTP',\n",
       " 'ESFJ',\n",
       " 'INFJ',\n",
       " 'INTJ',\n",
       " 'INTP',\n",
       " 'ESTJ',\n",
       " 'ISTJ',\n",
       " 'ENTJ',\n",
       " 'ISFJ',\n",
       " 'ESFP',\n",
       " 'ISTP',\n",
       " 'ESTP',\n",
       " 'ISFP',\n",
       " 'enfp',\n",
       " 'infp',\n",
       " 'enfj',\n",
       " 'entp',\n",
       " 'esfj',\n",
       " 'infj',\n",
       " 'intj',\n",
       " 'intp',\n",
       " 'estj',\n",
       " 'istj',\n",
       " 'entj',\n",
       " 'isfj',\n",
       " 'esfp',\n",
       " 'istp',\n",
       " 'estp',\n",
       " 'isfp']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbti_types = df.MBTI.unique().tolist()\n",
    "\n",
    "mbti_types_low = df['MBTI'].str.lower().unique().tolist()\n",
    "mbti_types = mbti_types + mbti_types_low\n",
    "mbti_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Hypothesis 2: political leaning\n",
    "types = set([e.lower() for e in mbti_types])\n",
    "N = len(list(types))\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove mbti types from tweets\n",
    "for x in mbti_types:\n",
    "    df['tweets'] = df['tweets'].str.replace(x,'mbti')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tweets.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove nan values\n",
    "df = df.dropna(subset=['tweets'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10337, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/heatherkoo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/heatherkoo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use nltk stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize and clean sentence [\"Hello world.\"] into list of words [\"hello\",\"world\"]\n",
    "def clean_tokenize(sentence):\n",
    "    #ignore_words = ['a', 'the', 'user', 'i','is',\"'\",'com','pic_twitt','s','https_twitt','https_www','https_twitter','t', 'u','lol']\n",
    "    custom = [\"'\",'com','pic_twitt','s','https_twitt','https_www','https_twitter','t', 'u','lol']\n",
    "    ignore_words = nltk.corpus.stopwords.words(\"english\") + list(string.punctuation) + custom\n",
    "    sentence = re.sub(\"\\'\",\"\",sentence)\n",
    "    words = re.sub(\"[^\\w]|[0-9]\", \" \",  sentence).split() #removes all non-alphanumeric words, removes all numbers\n",
    "    words_cleaned = [w.lower() for w in words if w.lower() not in ignore_words]\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    #words_cleaned = ' '.join(word for word in words_cleaned)\n",
    "    \n",
    "    return words_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_tweets\"] = df[\"tweets\"].apply(clean_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yes',\n",
       " 'college',\n",
       " 'voting',\n",
       " 'block',\n",
       " 'pope',\n",
       " 'didnt',\n",
       " 'come',\n",
       " 'around',\n",
       " 'like',\n",
       " 'th',\n",
       " 'century',\n",
       " 'cardinals',\n",
       " 'important',\n",
       " 'romans',\n",
       " 'mean',\n",
       " 'lot',\n",
       " 'arabia',\n",
       " 'folk',\n",
       " 'religion',\n",
       " 'mesopotamia',\n",
       " 'nestorian',\n",
       " 'egypt',\n",
       " 'monophysite',\n",
       " 'go',\n",
       " 'along',\n",
       " 'catholicism',\n",
       " 'arianism',\n",
       " 'hanging',\n",
       " 'around',\n",
       " 'north',\n",
       " 'africa',\n",
       " 'certainly',\n",
       " 'helped',\n",
       " 'smooth',\n",
       " 'pathway',\n",
       " 'islam',\n",
       " 'sorry',\n",
       " 'good',\n",
       " 'ancient',\n",
       " 'church',\n",
       " 'taught',\n",
       " 'islam',\n",
       " 'christian',\n",
       " 'heresy',\n",
       " 'dante',\n",
       " 'put',\n",
       " 'mohammad',\n",
       " 'schismatics',\n",
       " 'richard',\n",
       " 'harriss',\n",
       " 'kid',\n",
       " 'fabulous',\n",
       " 'stellan',\n",
       " 'course',\n",
       " 'piecemealed',\n",
       " 'chernobyl',\n",
       " 'youtube',\n",
       " 'fuck',\n",
       " 'magnificent',\n",
       " 'https',\n",
       " 'www',\n",
       " 'ncronline',\n",
       " 'org',\n",
       " 'blogs',\n",
       " 'ncr',\n",
       " 'toda',\n",
       " 'faith',\n",
       " 'facts',\n",
       " 'mike',\n",
       " 'pence',\n",
       " 'born',\n",
       " 'evangelical',\n",
       " 'catholic',\n",
       " 'whole',\n",
       " 'midwest',\n",
       " 'catholic',\n",
       " 'thing',\n",
       " 'pat',\n",
       " 'altar',\n",
       " 'boy',\n",
       " 'devout',\n",
       " 'family',\n",
       " 'still',\n",
       " 'calls',\n",
       " 'catholic',\n",
       " 'evangelical',\n",
       " 'something',\n",
       " 'like',\n",
       " 'actually',\n",
       " 'catholic',\n",
       " 'bags',\n",
       " 'got',\n",
       " 'paul',\n",
       " 'rudd',\n",
       " 'vibe',\n",
       " 'going',\n",
       " 'man',\n",
       " 'minor',\n",
       " 'pedantic',\n",
       " 'point',\n",
       " 'dr',\n",
       " 'x',\n",
       " 'yz',\n",
       " 'x',\n",
       " 'yz',\n",
       " 'ph',\n",
       " 'combining',\n",
       " 'redundant',\n",
       " 'yay',\n",
       " 'one',\n",
       " 'beer',\n",
       " 'like',\n",
       " 'one',\n",
       " 'lay',\n",
       " 'potato',\n",
       " 'chip',\n",
       " 'betcha',\n",
       " 'eat',\n",
       " 'one',\n",
       " 'gonna',\n",
       " 'fuckin',\n",
       " 'happen',\n",
       " 'since',\n",
       " 'youre',\n",
       " 'frannies',\n",
       " 'good',\n",
       " 'places',\n",
       " 'way',\n",
       " 'fr',\n",
       " 'would',\n",
       " 'dannys',\n",
       " 'south',\n",
       " 'big',\n",
       " 'tree',\n",
       " 'youll',\n",
       " 'see',\n",
       " 'bills',\n",
       " 'players',\n",
       " 'big',\n",
       " 'tree',\n",
       " 'canada',\n",
       " 'wonderful',\n",
       " 'potential',\n",
       " 'even',\n",
       " 'better',\n",
       " 'live',\n",
       " 'buffalo',\n",
       " 'grew',\n",
       " 'uo',\n",
       " 'throughout',\n",
       " 'upstate',\n",
       " 'canadas',\n",
       " 'presence',\n",
       " 'always',\n",
       " 'haunted',\n",
       " 'best',\n",
       " 'mexican',\n",
       " 'buffalo',\n",
       " 'imo',\n",
       " 'el',\n",
       " 'canelo',\n",
       " 'us',\n",
       " 'brave',\n",
       " 'new',\n",
       " 'world',\n",
       " 'canada',\n",
       " 'delicious',\n",
       " 'would',\n",
       " 'ok',\n",
       " 'also',\n",
       " 'means',\n",
       " 'meaningless',\n",
       " 'games',\n",
       " 'lost',\n",
       " 'season',\n",
       " 'would',\n",
       " 'take',\n",
       " 'thing',\n",
       " 'starting',\n",
       " 'season',\n",
       " 'earlier',\n",
       " 'start',\n",
       " 'august',\n",
       " 'end',\n",
       " 'christmas',\n",
       " 'well',\n",
       " 'wny',\n",
       " 'always',\n",
       " 'looks',\n",
       " 'forward',\n",
       " 'sir',\n",
       " 'mid',\n",
       " 'next',\n",
       " 'week',\n",
       " 'tropical',\n",
       " 'moisture',\n",
       " 'cc',\n",
       " 'wnyweather',\n",
       " 'kevinbuffalo',\n",
       " 'today',\n",
       " 'perfect',\n",
       " 'sunny',\n",
       " 'moderate',\n",
       " 'humidity',\n",
       " 'tomorrow',\n",
       " 'sunny',\n",
       " 'get',\n",
       " 'sweltering',\n",
       " 'next',\n",
       " 'week',\n",
       " 'thoughtfulness',\n",
       " 'pic',\n",
       " 'twitter',\n",
       " 'qzxuuxeyym',\n",
       " 'cheers',\n",
       " 'might',\n",
       " 'best',\n",
       " 'sitcom',\n",
       " 'ever',\n",
       " 'also',\n",
       " 'binge',\n",
       " 'watch',\n",
       " 'taxi',\n",
       " 'deus',\n",
       " 'caritas',\n",
       " 'est',\n",
       " 'looks',\n",
       " 'like',\n",
       " 'middle',\n",
       " 'aged',\n",
       " 'guy',\n",
       " 'named',\n",
       " 'eugene',\n",
       " 'honestly',\n",
       " 'inspire',\n",
       " 'sarah',\n",
       " 'youre',\n",
       " 'like',\n",
       " 'modern',\n",
       " 'julian',\n",
       " 'norwich',\n",
       " 'god',\n",
       " 'bless',\n",
       " 'easy',\n",
       " 'born',\n",
       " 'nobody',\n",
       " 'expects',\n",
       " 'son',\n",
       " 'soon',\n",
       " 'still',\n",
       " 'call',\n",
       " 'baby',\n",
       " 'affection',\n",
       " 'endearment',\n",
       " 'amen',\n",
       " 'sister',\n",
       " 'next',\n",
       " 'week',\n",
       " 'fascinates',\n",
       " 'subtropical',\n",
       " 'tropical',\n",
       " 'pattern',\n",
       " 'setting',\n",
       " 'especially',\n",
       " 'get',\n",
       " 'barry',\n",
       " 'remnants',\n",
       " 'hot',\n",
       " 'humid',\n",
       " 'wet',\n",
       " 'hamster',\n",
       " 'becomes',\n",
       " 'hampster',\n",
       " 'subjectivity',\n",
       " 'experience',\n",
       " 'desire',\n",
       " 'emotion',\n",
       " 'dictate',\n",
       " 'reaction',\n",
       " 'kind',\n",
       " 'religious',\n",
       " 'negative',\n",
       " 'use',\n",
       " 'word',\n",
       " 'father',\n",
       " 'one',\n",
       " 'beliefs',\n",
       " 'trampled',\n",
       " 'ill',\n",
       " 'make',\n",
       " 'old',\n",
       " 'slippery',\n",
       " 'slope',\n",
       " 'argument',\n",
       " 'nothing',\n",
       " 'objectively',\n",
       " 'true',\n",
       " 'inevitable',\n",
       " 'situations',\n",
       " 'occur',\n",
       " 'youll',\n",
       " 'get',\n",
       " 'great',\n",
       " 'guy',\n",
       " 'https',\n",
       " 'youtu',\n",
       " 'ulvoew',\n",
       " 'l',\n",
       " 'rs',\n",
       " 'best',\n",
       " 'side',\n",
       " 'goingblondzo',\n",
       " 'pic',\n",
       " 'twitter',\n",
       " 'vpkz',\n",
       " 'bj',\n",
       " 'kids',\n",
       " 'say',\n",
       " 'go',\n",
       " 'eat',\n",
       " 'fam',\n",
       " 'hell',\n",
       " 'yeah',\n",
       " 'michelle',\n",
       " 'https',\n",
       " 'youtu',\n",
       " 'zbmcbkalvkm',\n",
       " 'goodtweet_man',\n",
       " 'brian',\n",
       " 'johnsons',\n",
       " 'travelogue',\n",
       " 'show',\n",
       " 'good',\n",
       " 'foucault',\n",
       " 'therealoj',\n",
       " 'living',\n",
       " 'buffalo',\n",
       " 'time',\n",
       " 'fascinating',\n",
       " 'confluence',\n",
       " 'energies',\n",
       " 'foucault',\n",
       " 'taught',\n",
       " 'ubuffalo',\n",
       " 'early',\n",
       " 'crashed',\n",
       " 'professors',\n",
       " 'couches',\n",
       " 'gorgeous',\n",
       " 'good',\n",
       " 'idea',\n",
       " 'thing',\n",
       " 'start',\n",
       " 'regular',\n",
       " 'season',\n",
       " 'weeks',\n",
       " 'labor',\n",
       " 'day',\n",
       " 'everybody',\n",
       " 'gets',\n",
       " 'labor',\n",
       " 'day',\n",
       " 'weekend',\n",
       " 'bye',\n",
       " 'week',\n",
       " 'college',\n",
       " 'football',\n",
       " 'open',\n",
       " 'usual',\n",
       " 'game',\n",
       " 'sked',\n",
       " 'afterward',\n",
       " 'weeks',\n",
       " 'b',\n",
       " 'would',\n",
       " 'interconference',\n",
       " 'matchups',\n",
       " 'buffalo',\n",
       " 'buffalo',\n",
       " 'even',\n",
       " 'worse',\n",
       " 'mike',\n",
       " 'lakeshadow',\n",
       " 'sammy',\n",
       " 'watkins',\n",
       " 'scorched',\n",
       " 'earthed',\n",
       " 'revis',\n",
       " 'island',\n",
       " 'downright',\n",
       " 'chilly',\n",
       " 'wny',\n",
       " 'today',\n",
       " 'degrees',\n",
       " 'locked',\n",
       " 'place',\n",
       " 'many',\n",
       " 'things',\n",
       " 'say',\n",
       " 'well',\n",
       " 'nfl',\n",
       " 'probably',\n",
       " 'signed',\n",
       " 'cba',\n",
       " 'soon',\n",
       " 'enormous',\n",
       " 'news',\n",
       " 'josephrehrauer',\n",
       " 'wojbomb',\n",
       " 'https',\n",
       " 'twitter',\n",
       " 'wojespn',\n",
       " 'status',\n",
       " 'alexisscriven',\n",
       " 'https',\n",
       " 'youtu',\n",
       " 'bmq_x',\n",
       " 'umg',\n",
       " 'avowed',\n",
       " 'marxist',\n",
       " 'https',\n",
       " 'en',\n",
       " 'wikipedia',\n",
       " 'org',\n",
       " 'wiki',\n",
       " 'the_gospe',\n",
       " 'l_according_to_st',\n",
       " '_matthew_',\n",
       " 'film',\n",
       " 'though',\n",
       " 'literally',\n",
       " 'religion',\n",
       " 'milieu',\n",
       " 'religious',\n",
       " 'pasolinis',\n",
       " 'gospel',\n",
       " 'according',\n",
       " 'st',\n",
       " 'matthew',\n",
       " 'went',\n",
       " 'completely',\n",
       " 'single',\n",
       " 'knowing',\n",
       " 'wife',\n",
       " 'th',\n",
       " 'birthday',\n",
       " 'married',\n",
       " 'within',\n",
       " 'three',\n",
       " 'months',\n",
       " 'th',\n",
       " 'birthday',\n",
       " 'https',\n",
       " 'en',\n",
       " 'wikipedia',\n",
       " 'org',\n",
       " 'wiki',\n",
       " 'montreal',\n",
       " 'style_smoked_meat',\n",
       " 'https',\n",
       " 'en',\n",
       " 'wikipedia',\n",
       " 'org',\n",
       " 'wiki',\n",
       " 'montreal',\n",
       " 'style_bagel',\n",
       " 'montreal',\n",
       " 'best',\n",
       " 'bagels',\n",
       " 'ever',\n",
       " 'prayers',\n",
       " 'wow',\n",
       " 'hello',\n",
       " 'drums',\n",
       " 'american',\n",
       " 'beauty',\n",
       " 'ghost',\n",
       " 'world',\n",
       " 'couldnt',\n",
       " 'tell',\n",
       " 'would',\n",
       " 'play',\n",
       " 'idea',\n",
       " 'thora',\n",
       " 'birch',\n",
       " 'definitely',\n",
       " 'https',\n",
       " 'www',\n",
       " 'wgrz',\n",
       " 'day',\n",
       " 'looks',\n",
       " 'steamy',\n",
       " 'love',\n",
       " 'ugcc',\n",
       " 'ruthenian',\n",
       " 'church',\n",
       " 'much',\n",
       " 'byzantine',\n",
       " 'christianity',\n",
       " 'saved',\n",
       " 'faith',\n",
       " 'tough',\n",
       " 'time',\n",
       " 'late',\n",
       " 'early',\n",
       " 'hope',\n",
       " 'go',\n",
       " 'division',\n",
       " 'sweeps',\n",
       " 'jets',\n",
       " 'patriots',\n",
       " 'much',\n",
       " 'hate',\n",
       " 'miami',\n",
       " 'dolphins',\n",
       " 'love',\n",
       " 'fitz',\n",
       " 'much',\n",
       " 'incredibly',\n",
       " 'tough',\n",
       " 'year',\n",
       " 'hope',\n",
       " 'go',\n",
       " 'circle',\n",
       " 'life',\n",
       " 'best',\n",
       " 'eltonofficial',\n",
       " 'lion',\n",
       " 'king',\n",
       " 'song',\n",
       " 'come',\n",
       " 'buffalo',\n",
       " 'show',\n",
       " 'falls',\n",
       " 'https',\n",
       " 'youtu',\n",
       " '_t',\n",
       " 'qiqo',\n",
       " 'qkg',\n",
       " 'though',\n",
       " 'imbruglia',\n",
       " 'covered',\n",
       " 'actually',\n",
       " 'years',\n",
       " 'older',\n",
       " 'song',\n",
       " 'came',\n",
       " 'stuffed',\n",
       " 'bears',\n",
       " 'guess',\n",
       " 'would',\n",
       " 'say',\n",
       " 'best',\n",
       " 'movies',\n",
       " 'better',\n",
       " 'thsn',\n",
       " 'best',\n",
       " 'today',\n",
       " 'conversation',\n",
       " 'would',\n",
       " 'win',\n",
       " 'best',\n",
       " 'picture',\n",
       " 'year',\n",
       " 'super',\n",
       " 'hazy',\n",
       " 'buffalo',\n",
       " 'wildfire',\n",
       " 'smoke',\n",
       " 'happened',\n",
       " 'twice',\n",
       " 'surviving',\n",
       " 'barrel',\n",
       " 'restraint',\n",
       " 'history',\n",
       " 'anniversary',\n",
       " 'happening',\n",
       " 'time',\n",
       " 'less',\n",
       " 'oh',\n",
       " 'hate',\n",
       " 'get',\n",
       " 'used',\n",
       " 'commenting',\n",
       " 'someone',\n",
       " 'falls',\n",
       " 'sunday',\n",
       " 'going',\n",
       " 'surviving',\n",
       " 'https',\n",
       " 'twitter',\n",
       " 'davemckinley',\n",
       " 'status',\n",
       " 'western',\n",
       " 'ny',\n",
       " 'flip',\n",
       " 'winter',\n",
       " 'summer',\n",
       " 'wow',\n",
       " 'young',\n",
       " 'early',\n",
       " 'tough',\n",
       " 'say',\n",
       " 'challenger',\n",
       " 'disaster',\n",
       " 'memorable',\n",
       " 'half',\n",
       " 'st',\n",
       " 'grade',\n",
       " 'boston',\n",
       " 'nyc',\n",
       " 'charleston',\n",
       " 'sc',\n",
       " 'england',\n",
       " 'drop',\n",
       " 'rs',\n",
       " 'place',\n",
       " 'newfies',\n",
       " 'hoi',\n",
       " 'toiders',\n",
       " 'sound',\n",
       " 'british',\n",
       " 'east',\n",
       " 'coast',\n",
       " 'americans',\n",
       " 'sound',\n",
       " 'quite',\n",
       " 'british',\n",
       " 'non',\n",
       " 'rhotic',\n",
       " 'accents',\n",
       " 'course',\n",
       " 'affect',\n",
       " 'transatlantic',\n",
       " 'midatlantic',\n",
       " 'yalie',\n",
       " 'accent',\n",
       " 'sound',\n",
       " 'quite',\n",
       " 'british',\n",
       " 'inland',\n",
       " 'north',\n",
       " 'accent',\n",
       " 'rhotic',\n",
       " 'dont',\n",
       " 'sound',\n",
       " 'british',\n",
       " 'yes',\n",
       " 'case',\n",
       " 'eminem',\n",
       " 'song',\n",
       " 'obsessive',\n",
       " 'fandom',\n",
       " 'https',\n",
       " 'youtu',\n",
       " 'ce',\n",
       " 'wxdqdov',\n",
       " 'didnt',\n",
       " 'dancers',\n",
       " 'theyre',\n",
       " 'especially',\n",
       " 'neat',\n",
       " 'autumn',\n",
       " 'lake',\n",
       " 'ontario',\n",
       " 'temperature',\n",
       " 'difference',\n",
       " 'one',\n",
       " 'time',\n",
       " 'driving',\n",
       " 'wilson',\n",
       " 'toronto',\n",
       " 'skyline',\n",
       " 'looked',\n",
       " 'couple',\n",
       " 'miles',\n",
       " 'away',\n",
       " 'could',\n",
       " 'make',\n",
       " 'buildings',\n",
       " 'clearly',\n",
       " 'skyline',\n",
       " 'massive',\n",
       " 'yum',\n",
       " 'dont',\n",
       " 'think',\n",
       " 'shes',\n",
       " 'catholic',\n",
       " 'get',\n",
       " 'always',\n",
       " 'space',\n",
       " 'church',\n",
       " 'nah',\n",
       " 'prayers',\n",
       " 'vontaze',\n",
       " 'burfict',\n",
       " 'type',\n",
       " 'play',\n",
       " 'im',\n",
       " 'called',\n",
       " 'sacrifice',\n",
       " 'unto',\n",
       " 'death',\n",
       " 'wife',\n",
       " 'husbands',\n",
       " 'love',\n",
       " 'wives',\n",
       " 'christ',\n",
       " 'church',\n",
       " 'although',\n",
       " 'mentioned',\n",
       " 'often',\n",
       " 'folks',\n",
       " 'requires',\n",
       " 'lot',\n",
       " 'dudes',\n",
       " 'takeaway',\n",
       " 'glad',\n",
       " 'catholic',\n",
       " 'even',\n",
       " 'traddiest',\n",
       " 'catholic',\n",
       " 'tradwives',\n",
       " 'enough',\n",
       " 'catholic',\n",
       " 'tradition',\n",
       " 'catholic',\n",
       " 'female',\n",
       " 'saints',\n",
       " 'emulate',\n",
       " 'comport',\n",
       " 'way',\n",
       " 'biblical',\n",
       " 'wayfinder',\n",
       " 'without',\n",
       " 'magisterium',\n",
       " 'go',\n",
       " 'buffalo',\n",
       " 'nice',\n",
       " 'always',\n",
       " 'sparkles',\n",
       " 'summer',\n",
       " 'remember',\n",
       " 'going',\n",
       " 'th',\n",
       " 'anniversary',\n",
       " 'rerelease',\n",
       " 'bambi',\n",
       " 'thank',\n",
       " 'work',\n",
       " 'things',\n",
       " 'like',\n",
       " 'health',\n",
       " 'together',\n",
       " 'partners',\n",
       " 'wife',\n",
       " 'great',\n",
       " 'shape',\n",
       " 'health',\n",
       " 'would',\n",
       " 'never',\n",
       " 'cruel',\n",
       " 'thats',\n",
       " 'spicy',\n",
       " 'meatball',\n",
       " 'icon',\n",
       " 'looks',\n",
       " 'like',\n",
       " 'francis',\n",
       " 'papa',\n",
       " 'roncalli',\n",
       " 'oh',\n",
       " 'theres',\n",
       " 'cult',\n",
       " 'sj',\n",
       " 'nobody',\n",
       " 'beloved',\n",
       " 'progcaths',\n",
       " 'old',\n",
       " 'leftcath',\n",
       " 'grad',\n",
       " 'school',\n",
       " 'prof',\n",
       " 'good',\n",
       " 'guy',\n",
       " 'btw',\n",
       " 'ub',\n",
       " 'gave',\n",
       " 'son',\n",
       " 'roncalli',\n",
       " 'middle',\n",
       " 'name',\n",
       " 'miss',\n",
       " 'football',\n",
       " 'much',\n",
       " 'could',\n",
       " 'cry',\n",
       " 'yes',\n",
       " 'popes',\n",
       " 'saints',\n",
       " 'liberius',\n",
       " 'liberius',\n",
       " 'intents',\n",
       " 'purposes',\n",
       " 'good',\n",
       " 'guy',\n",
       " 'made',\n",
       " 'poor',\n",
       " 'decisions',\n",
       " 'youre',\n",
       " 'stupid',\n",
       " 'went',\n",
       " 'niagara',\n",
       " 'falls',\n",
       " 'today',\n",
       " 'still',\n",
       " 'odd',\n",
       " 'backyard',\n",
       " 'awesone',\n",
       " 'man',\n",
       " 'seeing',\n",
       " 'online',\n",
       " 'jarring',\n",
       " 'timeslip',\n",
       " 'like',\n",
       " 'got',\n",
       " 'pulled',\n",
       " 'backward',\n",
       " 'https',\n",
       " 'twitter',\n",
       " 'waxpancake',\n",
       " 'sta',\n",
       " 'tus',\n",
       " 'ok',\n",
       " 'biscuit',\n",
       " 'weeks',\n",
       " 'week',\n",
       " 'eh',\n",
       " 'youll',\n",
       " 'fine',\n",
       " 'well',\n",
       " 'think',\n",
       " 'nice',\n",
       " 'person',\n",
       " 'purgatory',\n",
       " 'catholicism',\n",
       " 'morbid',\n",
       " 'weird',\n",
       " 'lot',\n",
       " 'wonderful',\n",
       " 'beautiful',\n",
       " 'things',\n",
       " 'pic',\n",
       " 'twitter',\n",
       " 'ip',\n",
       " 'cdbkher',\n",
       " 'long',\n",
       " 'guitar',\n",
       " 'intro',\n",
       " 'went',\n",
       " 'mansion',\n",
       " 'house',\n",
       " 'kid',\n",
       " 'oneida',\n",
       " 'limited',\n",
       " 'biggest',\n",
       " 'enployer',\n",
       " 'area',\n",
       " 'important',\n",
       " 'whole',\n",
       " 'enterprise',\n",
       " 'felt',\n",
       " 'bit',\n",
       " 'embarrassing',\n",
       " 'like',\n",
       " 'lds',\n",
       " 'rejecting',\n",
       " 'old',\n",
       " 'polygamist',\n",
       " 'era',\n",
       " 'everybody',\n",
       " 'oneida',\n",
       " 'catholic',\n",
       " 'anyway',\n",
       " 'chautaqua',\n",
       " 'institute',\n",
       " 'lily',\n",
       " 'dale',\n",
       " 'western',\n",
       " 'ny',\n",
       " 'much',\n",
       " 'th',\n",
       " 'century',\n",
       " 'industrislism',\n",
       " 'romanticism',\n",
       " 'merged',\n",
       " 'theres',\n",
       " 'much',\n",
       " 'burned',\n",
       " 'stuff',\n",
       " 'upstate',\n",
       " 'shakers',\n",
       " 'mormons',\n",
       " 'also',\n",
       " 'general',\n",
       " 'scientific',\n",
       " 'utopianism',\n",
       " 'king',\n",
       " 'camp',\n",
       " 'gillettes',\n",
       " 'moves',\n",
       " 'toward',\n",
       " 'niagara',\n",
       " 'william',\n",
       " 'love',\n",
       " 'built',\n",
       " 'canal',\n",
       " 'set',\n",
       " 'model',\n",
       " 'city',\n",
       " 'utopianism',\n",
       " 'ended',\n",
       " 'ironically',\n",
       " 'love',\n",
       " 'canal',\n",
       " 'oneida',\n",
       " 'start',\n",
       " 'https',\n",
       " 'youtu',\n",
       " 'si',\n",
       " 'nsqurnou',\n",
       " 'bad',\n",
       " 'boys',\n",
       " 'pistons',\n",
       " 'fan',\n",
       " 'dynasty',\n",
       " 'ended',\n",
       " 'lived',\n",
       " 'cali',\n",
       " 'shifted',\n",
       " 'clips',\n",
       " 'fan',\n",
       " 'since',\n",
       " 'danny',\n",
       " 'manning',\n",
       " 'larry',\n",
       " 'brown',\n",
       " 'good',\n",
       " 'like',\n",
       " 'straight',\n",
       " 'pakistani',\n",
       " 'best',\n",
       " 'like',\n",
       " 'though',\n",
       " 'much',\n",
       " 'northern',\n",
       " 'thai',\n",
       " 'like',\n",
       " 'guess',\n",
       " 'lots',\n",
       " 'coconut',\n",
       " 'milk',\n",
       " 'spicier',\n",
       " 'went',\n",
       " 'vegetarian',\n",
       " 'killer',\n",
       " 'last',\n",
       " 'week',\n",
       " 'ever',\n",
       " 'try',\n",
       " 'southern',\n",
       " 'dosas',\n",
       " 'love',\n",
       " 'makhani',\n",
       " 'yummmm',\n",
       " 'good',\n",
       " 'example',\n",
       " 'winning',\n",
       " 'national',\n",
       " 'dialect',\n",
       " 'pop',\n",
       " 'soda',\n",
       " 'divide',\n",
       " 'keeps',\n",
       " 'moving',\n",
       " 'west',\n",
       " 'old',\n",
       " 'folks',\n",
       " 'said',\n",
       " 'pop',\n",
       " 'little',\n",
       " 'cny',\n",
       " 'migrated',\n",
       " 'rochester',\n",
       " 'points',\n",
       " 'west',\n",
       " 'even',\n",
       " 'buffalo',\n",
       " 'soda',\n",
       " 'becoming',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"clean_tweets\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize and clean sentence [\"Hello world.\"] into list of words [\"hello\",\"world\"]\n",
    "def clean(sentence):\n",
    "    ignore_words = ['a']\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() #nltk.word_tokenize(sentence)\n",
    "    words_cleaned = [w.lower() for w in words if w not in ignore_words]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words_cleaned = [w for w in words_cleaned if not w in stop_words]\n",
    "    words_string = ''.join(words_cleaned)\n",
    "    return words_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split posts per users into separate sentences\n",
    "#post = []\n",
    "#utype = []\n",
    "#user = []\n",
    "\n",
    "#for index, row in df.iterrows():\n",
    "#    posts = row['posts'].split('|||')\n",
    "#    posts_clean = []\n",
    "##    for sentence in posts:\n",
    "#       posts_clean.append(clean(sentence))\n",
    "#    post.extend(posts_clean)\n",
    "#     post.extend(posts)\n",
    "#    utype.extend([row['type'] for i in range(len(posts))])\n",
    "#    user.extend([index for i in range(len(posts))])\n",
    "    \n",
    "#short_posts = pd.DataFrame({\"user\": user,\"type\": utype,\"post\": post})\n",
    "#print(short_posts.shape)\n",
    "#short_posts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_tweets'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_screen_name    0\n",
       "tweets              0\n",
       "prof_image_url      0\n",
       "MBTI                0\n",
       "orig_tweets         0\n",
       "id_                 0\n",
       "retweets            0\n",
       "fav                 0\n",
       "clean_tweets        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Remove empty tweets\n",
    "df[df['clean_tweets'] == '[]'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_screen_name    0\n",
       "tweets              0\n",
       "prof_image_url      0\n",
       "MBTI                0\n",
       "orig_tweets         0\n",
       "id_                 0\n",
       "retweets            0\n",
       "fav                 0\n",
       "clean_tweets        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['clean_tweets'] == '[]'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1344"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['clean_tweets'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empty lists came as a result of pre-processing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_ind = []\n",
    "for i in range(df['clean_tweets'].shape[0]):\n",
    "    if len(df['clean_tweets'].iloc[i]) == 0:\n",
    "        empty_ind.append(i)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10337"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_tweets'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2879"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(empty_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[77, 84, 106, 114, 115, 122, 123, 143, 154, 179, 201, 255]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_ind[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_tweets'].iloc[77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_L = df.shape[0]-len(empty_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7458"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df.drop(empty_ind, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7458, 9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRY Stratified sampling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratifiedSplit(X,y,size):\n",
    "    sss = StratifiedShuffleSplit(n_splits = 16, test_size=size, random_state=0)\n",
    "\n",
    "    for train_index, test_index in sss:\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## about calss imbalance:\n",
    "https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#np.array(df['clean_tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remob=ve emmpty []?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data: 80% train, 20% test\n",
    "post_train, post_test, label_train, label_test = train_test_split(np.array(df['clean_tweets']), \n",
    "                                                    np.array(df['MBTI']), \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=88)\n",
    "\n",
    "\n",
    "#print(\"MBTI posts\", post_train[5])\n",
    "print('')\n",
    "#print(\"MBTI Labels: \",label_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "372677"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a vocabulary (V size is defaulted to full text) for train corpus\n",
    "\n",
    "vocab_train = []\n",
    "for i in range(len(post_train)):\n",
    "    for word in post_train[i]:\n",
    "        vocab_train.append(word)\n",
    "\n",
    "\n",
    "vocab_mbti = vocabulary.Vocabulary((w for w in vocab_train))\n",
    "vocab_mbti.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (372,677 words) written to 'vocab.csv'\n"
     ]
    }
   ],
   "source": [
    "vocab_mbti.write_flat_file('vocab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>']\n"
     ]
    }
   ],
   "source": [
    "print (vocab_mbti.ids_to_words([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print (vocab_mbti.words_to_ids(['got','what','and','the']))\n",
    "#print (vocab_mbti.ids_to_words([202, 147565, 317206, 159348])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[88106, 3, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print (vocab_mbti.words_to_ids(['intj','twitter','and','the']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_mbti.words_to_ids(post_train[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#post_train[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and canonicalize train and test sets\n",
    "x_train = []\n",
    "for i in range(len(post_train)):\n",
    "    x_train.append(vocab_mbti.words_to_ids(post_train[i]))\n",
    "\n",
    "x_test = []\n",
    "for i in range(len(post_test)):\n",
    "    x_test.append(vocab_mbti.words_to_ids(post_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  ['lolll', 'looks', 'tall', 'obvs', 'cause', 'stroke', 'baseline', 'stupid', 'yeah', 'prob', 'look', 'pinch', 'wide', 'sans', 'pjs', 'cause', 'formal', 'attire', 'p', 'similarly', 'manic', 'inducing', 'period', 'undiagnosed', 'medical', 'disorder', 'super', 'long', 'shot', 'info', 'maybe', 'get', 'blood', 'work', 'done', 'see', 'everything', 'comes', 'back', 'normal', 'levels', 'one', 'area', 'seeing', 'saved', 'life', 'basically', 'courier', 'bit', 'nice', 'getting', 'designs', 'program', 'useable', 'state', 'developers', 'use', 'course', 'listening', 'loud', 'sounds', 'continued', 'headphone', 'usage', 'cold', 'saturate', 'ears', 'maybe', 'aural', 'break', 'different', 'elevation', 'soft', 'neck', 'back', 'yoga', 'might', 'loosen', 'muscles', 'unkink', 'whatever', 'causing', 'ringing', 'permanent', 'tinnitus', 'right', 'ear', 'work', 'mildly', 'reduces', 'sadly', 'glad', 'worked', 'seth', 'sucks', 'places', 'rural', 'town', 'next', 'city', 'lucky', 'mpbs', 'free', 'upgrade', 'gb', 'soon', 'price', 'change', 'remember', 'bad', 'internet', 'growing', 'blessed', 'good', 'luck', 'anyhow', 'xd', 'pic', 'twitter', 'hkyv', 'fng', 'also', 'literally', 'clue', 'words', 'mean', 'grouped', 'like', 'looks', 'like', 'memeing', 'though', 'http', 'cs', 'williams', 'edu', 'bailey', 'li', 'p', 'df', 'https', 'ikee', 'lib', 'auth', 'gr', 'record', 'files', 'gri', 'pdf', 'https', 'www', 'science', 'gov', 'topicpages', 'q', 'q', 'uadrilateral', 'mesh', 'generation']\n",
      "Canonicalized Text:  [13292, 187, 2537, 9133, 365, 7517, 22011, 567, 110, 3821, 62, 11823, 2909, 6633, 17808, 365, 7559, 13601, 23, 11550, 10880, 15569, 1302, 41266, 1965, 4388, 301, 118, 1015, 1387, 122, 12, 1164, 50, 160, 28, 165, 432, 40, 855, 2649, 9, 784, 380, 1471, 37, 738, 19238, 105, 188, 125, 4974, 1699, 58057, 562, 10341, 163, 397, 538, 1231, 356, 5557, 20027, 8996, 778, 58058, 2102, 122, 58059, 528, 253, 36688, 906, 2342, 40, 2883, 195, 30448, 5657, 118172, 554, 4644, 14697, 6057, 33246, 39, 2610, 50, 10514, 17209, 1992, 335, 888, 8707, 1040, 1124, 6697, 987, 136, 480, 1117, 118173, 216, 6614, 3668, 312, 1381, 267, 212, 107, 749, 1243, 1187, 15, 663, 11089, 664, 5, 3, 118174, 75654, 30, 162, 3247, 327, 138, 28202, 6, 187, 6, 58060, 147, 20, 4271, 4681, 4994, 8052, 3104, 23, 6089, 4, 118175, 12302, 33247, 4055, 1462, 4458, 57454, 5663, 4, 11, 944, 3113, 118176, 192, 192, 118177, 15129, 2190]\n",
      "Max lengths of texts:  6121\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Text: \",post_train[88])\n",
    "print(\"Canonicalized Text: \", x_train[88])\n",
    "print(\"Max lengths of texts: \", max([len(x) for x in x_train+x_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary vs Cluster label assignment: choose one only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary mbti (performs better!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_mbti(string):\n",
    "    \n",
    "    if string[:2]==\"EN\" and string[-1]==\"P\": #ENP\n",
    "        return 0\n",
    "    elif string[:2]==\"EN\" and string[-1]==\"J\": #ENJ \n",
    "        return 1\n",
    "    elif string[:2]==\"IS\" and string[-1]==\"P\": #ISP\n",
    "        return 2\n",
    "    elif string[:2]==\"IS\"and string[-1]==\"J\": #ISJ \n",
    "        return 3\n",
    "    elif string[:2]!=\"IS\" and string[:2]!=\"EN\" and string[-1]==\"P\": #other + P\n",
    "        return 4\n",
    "    elif string[:2]!=\"IS\" and string[:2]!=\"EN\" and string[-1]==\"J\": #other + J\n",
    "        return 5\n",
    "\n",
    "        \n",
    "   # assert len(label_bin) == 4,\"Not a valid MBTI type\"\n",
    "   # return label_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESTP\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(label_train[0])\n",
    "print(binary_mbti(label_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 5, 4, 2]\n",
      "['ESTP' 'ESFJ' 'INFJ' 'INTP' 'ISFP']\n",
      "[1, 5, 0, 0, 5]\n",
      "['ENFJ' 'INTJ' 'ENTP' 'ENFP' 'INFJ']\n"
     ]
    }
   ],
   "source": [
    "## binary\n",
    "y_train_id = list(map(lambda x: binary_mbti(x), label_train))\n",
    "y_test_id = list(map(lambda x: binary_mbti(x), label_test))\n",
    "\n",
    "print(y_train_id[0:5])\n",
    "print(label_train[0:5])\n",
    "print(y_test_id[0:5])\n",
    "print(label_test[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster mbti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_mbti(string):\n",
    "    label_bin = []\n",
    "    if string[0]==\"E\":\n",
    "        label_bin.append(1)\n",
    "    else:\n",
    "        label_bin.append(0)\n",
    "    if string[1]==\"N\":\n",
    "        label_bin.append(1)\n",
    "    else:\n",
    "        label_bin.append(0)\n",
    "    if string[2]==\"F\":\n",
    "        label_bin.append(1)\n",
    "    else:\n",
    "        label_bin.append(0)\n",
    "    if string[3]==\"J\":\n",
    "        label_bin.append(1)\n",
    "    else:\n",
    "        label_bin.append(0)\n",
    "        \n",
    "    assert len(label_bin) == 4,\"Not a valid MBTI type\"\n",
    "    return label_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESTP\n",
      "[1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(label_train[0])\n",
    "print(cluster_mbti(label_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 0, 0], [1, 0, 1, 1], [0, 1, 1, 1], [0, 1, 0, 0], [0, 0, 1, 0]]\n",
      "['ESTP' 'ESFJ' 'INFJ' 'INTP' 'ISFP']\n"
     ]
    }
   ],
   "source": [
    "#cluster\n",
    "y_train_id = list(map(lambda x: cluster_mbti(x), label_train))\n",
    "y_test_id = list(map(lambda x: cluster_mbti(x), label_test))\n",
    "\n",
    "print(y_train_id[0:5])\n",
    "print(label_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the NBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_np_array(example_ids, max_len=100, pad_id=0):\n",
    "\n",
    "    arr = np.full([len(example_ids), max_len], pad_id, dtype=np.int32)\n",
    "    ns = np.zeros([len(example_ids)], dtype=np.int32)\n",
    "    for i, ids in enumerate(example_ids):\n",
    "        cpy_len = min(len(ids), max_len)\n",
    "        arr[i,:cpy_len] = ids[:cpy_len]\n",
    "        ns[i] = cpy_len\n",
    "    return arr, ns\n",
    "\n",
    "def tokenize_post(post_string):\n",
    "    return vocab_mbti.words_to_ids(post_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_padded_array(post_ids, targets, max_len=100, pad_id=0,\n",
    "                    root_only=False, df_idxs=None):\n",
    "\n",
    "    x, ns = pad_np_array(post_ids, max_len=max_len, pad_id=pad_id)\n",
    "    return x, ns, np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_1 = []\n",
    "for i in range(len(y_train_id)):\n",
    "    y_train_1.append(y_train_id[i])\n",
    "\n",
    "y_test_1 = []\n",
    "for i in range(len(y_test_id)):\n",
    "    y_test_1.append(y_test_id[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_ns, train_y = as_padded_array(x_train, y_train_1)\n",
    "test_x, test_ns, test_y = as_padded_array(x_test, y_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1492, 100)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5966, 100)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5966"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 0, 0, 5, 5, 5, 1, 5, 1, 0, 4, 1, 3, 5, 1, 4, 1, 0, 5, 5, 1, 1, 5, 4, 4, 2, 1, 5, 5, 4, 5, 4, 2, 5, 0, 1, 1, 2, 2, 5, 2, 0, 0, 4, 4, 4, 1, 1, 0, 5, 2, 5, 0, 4, 4, 5, 5, 2, 0, 1, 2, 4, 1, 1, 0, 0, 0, 1, 3, 2, 3, 1, 5, 5, 1, 2, 4, 1, 1, 3, 0, 1, 2, 1, 4, 4, 1, 5, 0, 5, 4, 1, 5, 4, 4, 5, 4, 1, 5, 5, 5, 0, 0, 5, 4, 5, 2, 5, 4, 5, 5, 4, 5, 5, 2, 3, 5, 0, 1, 2, 5, 2, 5, 0, 2, 0, 1, 4, 5, 0, 5, 3, 4, 0, 5, 0, 0, 5, 5, 5, 5, 4, 0, 5, 5, 3, 1, 1, 5, 4, 3, 2, 2, 4, 4, 0, 0, 2, 2, 1, 4, 3, 4, 4, 3, 0, 4, 5, 3, 2, 3, 0, 0, 5, 5, 3, 1, 5, 4, 3, 0, 5, 4, 4, 4, 0, 5, 2, 1, 2, 1, 5, 4, 4, 3, 1, 3, 5, 3, 4, 1, 5, 5, 0, 5, 0, 3, 4, 5, 4, 0, 1, 5, 5, 4, 4, 5, 1, 5, 1, 3, 5, 3, 2, 4, 1, 4, 1, 4, 1, 4, 3, 1, 0, 4, 5, 5, 0, 1, 0, 4, 4, 0, 5, 3, 0, 0, 3, 2, 4, 1, 0, 4, 3, 3, 4, 0, 5, 5, 4, 5, 5, 1, 1, 2, 0, 1, 0, 3, 4, 4, 1, 1, 5, 5, 5, 0, 3, 3, 4, 4, 0, 2, 0, 4, 2, 1, 3, 4, 3, 5, 2, 2, 4, 2, 0, 0, 0, 4, 3, 1, 1, 1, 4, 5, 5, 0, 4, 0, 0, 4, 2, 5, 5, 0, 0, 5, 2, 5, 5, 5, 4, 5, 4, 5, 3, 1, 4, 4, 3, 2, 5, 5, 0, 5, 2, 4, 3, 4, 0, 2, 2, 2, 5, 4, 5, 2, 5, 5, 1, 4, 5, 4, 4, 4, 3, 5, 1, 5, 5, 5, 4, 0, 4, 5, 2, 2, 3, 1, 0, 4, 4, 1, 0, 5, 0, 5, 5, 5, 1, 1, 5, 5, 5, 4, 2, 0, 0, 5, 5, 1, 2, 4, 1, 3, 1, 5, 4, 4, 5, 4, 5, 1, 0, 4, 5, 3, 0, 1, 5, 5, 3, 4, 4, 2, 1, 2, 0, 1, 5, 1, 5, 4, 5, 5, 0, 5, 4, 4, 5, 5, 4, 5, 4, 0, 0, 3, 2, 5, 0, 1, 2, 3, 1, 5, 3, 1, 0, 5, 4, 4, 0, 5, 4, 3, 1, 5, 5, 0, 3, 0, 2, 3, 4, 1, 5, 0, 5, 5, 3, 4, 1, 4, 2, 3, 4, 5, 5, 4, 4, 0, 0, 1, 4, 0, 5, 4, 2, 4, 1, 5, 3, 2, 1, 3, 4, 0, 3, 3, 1, 5, 5, 3, 1, 3, 4, 5, 0, 5, 4, 1, 4, 2, 5, 1, 4, 3, 4, 4, 5, 4, 3, 5, 2, 4, 4, 0, 0, 3, 4, 3, 4, 3, 4, 4, 3, 1, 5, 5, 1, 5, 1, 1, 3, 5, 1, 4, 3, 3, 5, 2, 2, 4, 2, 1, 5, 5, 0, 4, 5, 1, 2, 0, 1, 3, 5, 2, 2, 1, 3, 4, 4, 1, 4, 5, 3, 1, 3, 0, 1, 4, 1, 3, 0, 3, 5, 1, 1, 0, 3, 1, 0, 3, 5, 5, 5, 0, 5, 5, 3, 1, 3, 1, 1, 5, 2, 2, 0, 4, 2, 3, 1, 1, 4, 3, 2, 4, 4, 3, 5, 4, 5, 4, 3, 5, 3, 4, 3, 0, 0, 4, 4, 3, 5, 5, 3, 5, 2, 5, 2, 5, 1, 1, 1, 0, 4, 2, 4, 2, 1, 5, 5, 0, 5, 0, 5, 3, 1, 1, 3, 2, 2, 5, 2, 2, 5, 4, 2, 1, 1, 0, 1, 1, 5, 4, 1, 4, 4, 2, 1, 5, 2, 3, 4, 5, 5, 2, 0, 0, 5, 3, 0, 5, 1, 4, 5, 5, 3, 3, 5, 1, 2, 5, 3, 0, 3, 0, 5, 1, 3, 5, 3, 2, 4, 5, 1, 4, 1, 4, 0, 5, 0, 5, 5, 4, 1, 0, 4, 4, 2, 1, 0, 3, 1, 1, 1, 4, 3, 1, 3, 1, 1, 3, 4, 5, 3, 1, 5, 2, 5, 1, 3, 3, 4, 1, 4, 5, 2, 3, 4, 1, 4, 3, 1, 5, 3, 1, 4, 1, 4, 2, 5, 2, 4, 3, 1, 4, 2, 4, 4, 1, 2, 3, 1, 1, 4, 3, 4, 2, 0, 1, 3, 1, 0, 2, 4, 0, 5, 1, 3, 1, 3, 5, 3, 3, 3, 1, 4, 1, 0, 3, 0, 4, 5, 1, 3, 3, 2, 5, 5, 5, 4, 4, 1, 3, 4, 4, 4, 4, 4, 4, 1, 5, 0, 0, 0, 0, 5, 3, 2, 4, 4, 3, 4, 1, 5, 3, 5, 4, 0, 1, 5, 3, 3, 4, 0, 1, 5, 1, 0, 3, 5, 1, 0, 4, 5, 1, 5, 0, 5, 3, 1, 3, 0, 0, 1, 5, 5, 5, 1, 4, 0, 2, 3, 4, 4, 5, 4, 4, 5, 4, 1, 3, 5, 1, 3, 5, 5, 4, 4, 0, 2, 1, 5, 1, 3, 5, 4, 2, 3, 4, 2, 4, 1, 4, 5, 1, 1, 2, 1, 3, 3, 0, 2, 0, 1, 3, 4, 5, 3, 4, 4, 4, 3, 1, 3, 5, 4, 4, 2, 1, 1, 4, 0, 4, 5, 5, 1, 0, 4, 1, 3, 5, 4, 0, 2, 3, 4, 2, 4, 3, 4, 4, 2, 2, 5, 3, 5, 4, 0, 2, 4, 1, 4, 1, 2, 3, 5, 1, 4, 4, 5, 3, 5, 5, 2, 0, 1, 4, 1, 3, 5, 5, 5, 4, 0, 4, 1, 0, 5, 1, 2, 5, 2, 4, 0, 5, 0, 1, 1, 4, 3, 5, 1, 0, 5, 2, 2, 0, 1, 5, 5, 1, 2, 4, 0, 2, 0, 0, 5, 4, 1, 5, 1, 4, 1, 0, 4, 5, 1, 4, 4, 4, 2, 5, 4, 4, 2, 1, 5, 0, 3, 3, 5, 4, 3, 2, 5, 4, 4, 4, 4, 0, 0, 5, 0, 5, 4, 1, 5, 1, 1, 3, 2, 4, 1, 0, 1, 0, 1, 5, 5, 5, 1, 5, 5, 3, 0, 1, 4, 2, 4, 1, 2, 4, 4, 5, 4, 0, 4, 5, 0, 3, 1, 1, 1, 4, 1, 3, 5, 0, 0, 0, 4, 1, 4, 4, 3, 4, 1, 1, 5, 5, 4, 5, 1, 4, 3, 4, 5, 5, 5, 2, 1, 2, 3, 3, 4, 4, 4, 5, 0, 2, 3, 4, 0, 1, 3, 2, 1, 1, 4, 0, 5, 1, 0, 2, 1, 4, 5, 1, 2, 5, 3, 5, 4, 4, 4, 4, 2, 3, 3, 4, 1, 3, 3, 1, 1, 5, 5, 1, 5, 0, 3, 3, 5, 5, 5, 0, 1, 4, 5, 5, 3, 0, 5, 3, 1, 0, 5, 5, 0, 4, 3, 4, 5, 1, 5, 5, 1, 3, 0, 0, 4, 0, 2, 0, 0, 5, 4, 1, 5, 2, 3, 2, 4, 1, 1, 3, 1, 3, 5, 4, 1, 1, 4, 2, 4, 3, 4, 0, 0, 5, 1, 1, 0, 5, 5, 3, 4, 2, 5, 3, 5, 4, 4, 2, 3, 0, 1, 1, 4, 4, 2, 4, 5, 4, 3, 4, 4, 1, 1, 1, 4, 3, 5, 5, 5, 5, 5, 5, 0, 3, 1, 0, 5, 5, 0, 1, 1, 2, 2, 5, 4, 1, 5, 5, 3, 5, 5, 4, 5, 5, 5, 4, 0, 0, 2, 3, 0, 2, 2, 4, 1, 4, 2, 1, 4, 4, 4, 4, 5, 5, 1, 4, 3, 5, 1, 5, 2, 5, 1, 4, 1, 1, 2, 1, 5, 4, 5, 1, 4, 3, 2, 3, 3, 3, 5, 4, 3, 4, 4, 4, 4, 0, 4, 4, 5, 5, 4, 5, 2, 3, 1, 5, 0, 5, 3, 1, 2, 0, 3, 5, 4, 4, 3, 4, 5, 4, 4, 3, 5, 0, 2, 4, 4, 2, 4, 5, 0, 3, 1, 1, 3, 5, 3, 3, 5, 2, 5, 5, 5, 2, 5, 0, 5, 2, 0, 0, 4, 5, 4, 5, 5, 4, 3, 3, 1, 5, 0, 5, 1, 5, 4, 2, 3, 5, 5, 4, 4, 5, 1, 4, 4, 4, 4, 5, 4, 5, 0, 1, 4, 5, 3, 4, 5, 5, 4, 5, 3, 4, 3, 2, 5, 4, 4, 1, 4, 2, 0, 5, 4, 4, 5, 3, 5, 0, 3, 2, 1, 1, 1, 1, 1, 5, 1, 2, 0, 5, 1, 4, 5, 0, 0, 5, 4, 4, 4, 2, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "len(y_train_1)\n",
    "print((y_test_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'MBTI_BOW_model' from '/Users/heatherkoo/Documents/MIDS/W210 Capstone/personality/Heather/code/MBTI_BOW_model.py'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import MBTI_BOW_model; reload(MBTI_BOW_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (372,677 words) written to '/tmp/tf_bow_sst_20190802-0548/metadata.tsv'\n",
      "Projector config written to /tmp/tf_bow_sst_20190802-0548/projector_config.pbtxt\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tf_bow_sst_20190802-0548', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x13f3b9908>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "To view training (once it starts), run:\n",
      "\n",
      "    tensorboard --logdir='/tmp/tf_bow_sst_20190802-0548' --port 6006\n",
      "\n",
      "Then in your browser, open: http://localhost:6006\n"
     ]
    }
   ],
   "source": [
    "#set up model using tf.estimator\n",
    "\n",
    "import MBTI_BOW_model; reload(MBTI_BOW_model)\n",
    "\n",
    "# Specify model hyperparameters as used by model\n",
    "model_params = dict(V=vocab_mbti.size, embed_dim=50, hidden_dims=[25], num_classes=6,\n",
    "                    encoder_type='bow',\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.01)\n",
    "\n",
    "checkpoint_dir = \"/tmp/tf_bow_sst_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "if os.path.isdir(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "\n",
    "vocab_mbti.write_projector_config(checkpoint_dir, \"Encoder/Embedding_Layer/W_embed\")\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn=MBTI_BOW_model.classifier_model_fn, \n",
    "                               params=model_params,\n",
    "                               model_dir=checkpoint_dir)\n",
    "print(\"\")\n",
    "print(\"To view training (once it starts), run:\\n\")\n",
    "print(\"    tensorboard --logdir='{:s}' --port 6006\".format(checkpoint_dir))\n",
    "print(\"\\nThen in your browser, open: http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tf_bow_sst_20190802-0548/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.7754898, step = 1\n",
      "INFO:tensorflow:global_step/sec: 170.467\n",
      "INFO:tensorflow:loss = 2.3798978, step = 101 (0.589 sec)\n",
      "INFO:tensorflow:global_step/sec: 170.046\n",
      "INFO:tensorflow:loss = 2.2799463, step = 201 (0.588 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.757\n",
      "INFO:tensorflow:loss = 1.8924954, step = 301 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.635\n",
      "INFO:tensorflow:loss = 2.0387695, step = 401 (0.471 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 478 into /tmp/tf_bow_sst_20190802-0548/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.0891361.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20190802-0548/model.ckpt-478\n",
      "INFO:tensorflow:Saving checkpoints for 479 into /tmp/tf_bow_sst_20190802-0548/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.0758361, step = 479\n",
      "INFO:tensorflow:global_step/sec: 222.06\n",
      "INFO:tensorflow:loss = 1.6527342, step = 579 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.298\n",
      "INFO:tensorflow:loss = 1.3144922, step = 679 (0.471 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.555\n",
      "INFO:tensorflow:loss = 1.1154441, step = 779 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.183\n",
      "INFO:tensorflow:loss = 0.9587916, step = 879 (0.463 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 956 into /tmp/tf_bow_sst_20190802-0548/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.789669.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20190802-0548/model.ckpt-956\n",
      "INFO:tensorflow:Saving checkpoints for 957 into /tmp/tf_bow_sst_20190802-0548/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.82857335, step = 957\n",
      "INFO:tensorflow:global_step/sec: 199.609\n",
      "INFO:tensorflow:loss = 0.7202477, step = 1057 (0.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.934\n",
      "INFO:tensorflow:loss = 0.59038305, step = 1157 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.948\n",
      "INFO:tensorflow:loss = 0.58669347, step = 1257 (0.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.059\n",
      "INFO:tensorflow:loss = 0.5639186, step = 1357 (0.465 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1434 into /tmp/tf_bow_sst_20190802-0548/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.55088687.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20190802-0548/model.ckpt-1434\n",
      "INFO:tensorflow:Saving checkpoints for 1435 into /tmp/tf_bow_sst_20190802-0548/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.626606, step = 1435\n",
      "INFO:tensorflow:global_step/sec: 221.213\n",
      "INFO:tensorflow:loss = 0.65656686, step = 1535 (0.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.826\n",
      "INFO:tensorflow:loss = 0.4600468, step = 1635 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.453\n",
      "INFO:tensorflow:loss = 0.5107432, step = 1735 (0.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.123\n",
      "INFO:tensorflow:loss = 0.50224787, step = 1835 (0.466 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1912 into /tmp/tf_bow_sst_20190802-0548/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.48348373.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20190802-0548/model.ckpt-1912\n",
      "INFO:tensorflow:Saving checkpoints for 1913 into /tmp/tf_bow_sst_20190802-0548/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.5683376, step = 1913\n",
      "INFO:tensorflow:global_step/sec: 205.726\n",
      "INFO:tensorflow:loss = 0.4747611, step = 2013 (0.488 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.598\n",
      "INFO:tensorflow:loss = 0.4159211, step = 2113 (0.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.468\n",
      "INFO:tensorflow:loss = 0.48536295, step = 2213 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.758\n",
      "INFO:tensorflow:loss = 0.47842592, step = 2313 (0.466 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2390 into /tmp/tf_bow_sst_20190802-0548/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.4616743.\n"
     ]
    }
   ],
   "source": [
    "#start training\n",
    "\n",
    "\n",
    "train_params = dict(batch_size=25, total_epochs=10, eval_every=2)\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "\n",
    "train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": train_x, \"ns\": train_ns}, y=train_y,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True, seed=42\n",
    "                 )\n",
    "\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": test_x, \"ns\": test_ns}, y=test_y,\n",
    "                    batch_size=25, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "  \n",
    "    model.train(input_fn=train_input_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2019-08-02-12:49:29\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20190802-0548/model.ckpt-2390\n",
      "INFO:tensorflow:Finished evaluation at 2019-08-02-12:49:30\n",
      "INFO:tensorflow:Saving dict for global step 2390: accuracy = 0.22520107, cross_entropy_loss = 2.130912, global_step = 2390, loss = 2.3780413\n",
      "Perplexity on test set: 8.42\n",
      "Accuracy on test set: 22.52%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.22520107,\n",
       " 'cross_entropy_loss': 2.130912,\n",
       " 'loss': 2.3780413,\n",
       " 'global_step': 2390}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluation on test data\n",
    "\n",
    "eval_metrics = model.evaluate(input_fn=test_input_fn, name=\"test\")  \n",
    "\n",
    "print (\"Perplexity on test set: {:.03}\".format(math.exp(eval_metrics['cross_entropy_loss'])))\n",
    "print(\"Accuracy on test set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-07-29T08:09:43Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20190729-0108/model.ckpt-3310\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-07-29-08:09:45\n",
      "INFO:tensorflow:Saving dict for global step 3310: accuracy = 0.9203048, cross_entropy_loss = 0.18425459, global_step = 3310, loss = 0.2531177\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3310: /tmp/tf_bow_sst_20190729-0108/model.ckpt-3310\n",
      "Perplexity on train set: 1.2\n",
      "Accuracy on train set: 92.03%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9203048,\n",
       " 'cross_entropy_loss': 0.18425459,\n",
       " 'loss': 0.2531177,\n",
       " 'global_step': 3310}"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluation on training data\n",
    "\n",
    "eval_metrics = model.evaluate(input_fn=train_input_fn, name=\"train\")  \n",
    "\n",
    "print (\"Perplexity on train set: {:.03}\".format(math.exp(eval_metrics['cross_entropy_loss'])))\n",
    "print(\"Accuracy on train set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On data with empty tweet lists!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### binary_mbti(string):\n",
    "Training:\n",
    "{'accuracy': 0.9203048,\n",
    " 'cross_entropy_loss': 0.1842546,\n",
    " 'loss': 0.25311774,\n",
    " 'global_step': 3310}\n",
    "Test:\n",
    "{'accuracy': 0.66779494,\n",
    " 'cross_entropy_loss': 0.90574795,\n",
    " 'loss': 0.97126514,\n",
    " 'global_step': 3310}   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### cluster_mbti(string):\n",
    "Training:\n",
    "{'accuracy': 0.87314063,\n",
    " 'cross_entropy_loss': 0.2112223,\n",
    " 'loss': 0.2764688,\n",
    " 'global_step': 3310}\n",
    "Test:\n",
    "{'accuracy': 0.5628627,\n",
    " 'cross_entropy_loss': 1.050934,\n",
    " 'loss': 1.1135653,\n",
    " 'global_step': 3310}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On data where no empty tweets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### binary_mbti(string):\n",
    "Training:\n",
    "{'accuracy': 0.96178347,\n",
    " 'cross_entropy_loss': 0.09430911,\n",
    " 'loss': 0.16588114,\n",
    " 'global_step': 2390}\n",
    "Test:\n",
    "{'accuracy': 0.64745307,\n",
    " 'cross_entropy_loss': 0.99033755,\n",
    " 'loss': 1.0579644,\n",
    " 'global_step': 2390}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### cluster_mbti(string):\n",
    "Training:\n",
    "{'accuracy': 0.93982565,\n",
    " 'cross_entropy_loss': 0.115194365,\n",
    " 'loss': 0.18973024,\n",
    " 'global_step': 2390}\n",
    "\n",
    "{'accuracy': 0.54825735,\n",
    " 'cross_entropy_loss': 1.1097777,\n",
    " 'loss': 1.1803049,\n",
    " 'global_step': 2390}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
